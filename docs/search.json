[
  {
    "objectID": "03_VL.html#der-plan-für-heute",
    "href": "03_VL.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung 3\n\n\nQuiz\nSelf-Service BI Terminologie\nRecap zur Aufgabe von letzter Woche und gemeinsamer Walk-Through\nVisualisierungstyp Balkendiagramm\nDatentransformation:\n\nWas heißt ETL?\nWelche Transformationschritte durchlaufen operative Daten bis zur Speicherung im DWH?\n\nGemeinsame Transformation eines Datensatzes mit User-Events"
  },
  {
    "objectID": "03_VL.html#self-service-bi",
    "href": "03_VL.html#self-service-bi",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI",
    "text": "Self-Service BI\nTerminologie\n\n\nDashboard:\n\nEine Sammlung von Visualisierungen, die in einem gemeinsamen Kontext dargestellt werden\nEin Dashboard kann mehrere Visualisierungen enthalten, die auf unterschiedlichen Datasets basieren\nDie Visualisierungen können über globale Filter gefiltert werden\n\nChart:\n\nEigenständige Visualisierung eines Datasets, die eigenständig oder in einem Dashboard dargestellt werden kann\nEs gibt eine Vielzahl von Chart-Typen, die in Superset dargestellt werden können\nPreset.io gibt eine Übersicht über die gängigsten Chart-Typen"
  },
  {
    "objectID": "03_VL.html#self-service-bi-1",
    "href": "03_VL.html#self-service-bi-1",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI",
    "text": "Self-Service BI\nTerminologie\n\n\nDimensions:\n\nDimensionen sind die Kategorien, nach denen Daten gruppiert wird\nIdealerweise kategoriale Variablen, die nicht aggregiert werden\nWenn die Daten eine Zeitdimension enthalten (erkennbar am Uhren-Symbol) ist eine spezielle Time Dimension verfügbar\nBei korrekter Pflege der Zeitvariable lässt sich direkt auf Tages-, Wochen-, Monats- und Jahreswerte aggregieren mittels Time Grain\n\nMetrics\n\nQuantitative Variablen, die sich mit Funktionen aggregieren lassen\nBeispielfunktionen sind Summen, Durchschnitte, Min und Max oder Counts\nBesonderheit: Viele BI-Tools erstellen per Default eine Count-Metrik, die Datenpunkte pro Dimension zählt"
  },
  {
    "objectID": "03_VL.html#self-service-bi-2",
    "href": "03_VL.html#self-service-bi-2",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI",
    "text": "Self-Service BI\nTerminologie\n\n\nAggregationsfunktionen\n\nSelf-Service BI Tools werden meist mit Daten von höchster Granularität verknüpft, um die Flexibilität der Analyse zu erhöhen und Aggregationen seitens der Nutzenden zu ermöglichen\nAggregationsfunktionen werden auf Metriken angewendet, um die Daten zu verdichten\nEinfache Beispiele sind Summen, Durchschnitte, Min und Max\nBesonderheit Count und Count Distinct:\n\nCount zählt die Anzahl der Datenpunkte,\nCount Distinct zählt die Anzahl der einzigartigen Werte über die Dimension(en)\n\nSemantisches Wissen und Fragestellung sind für die Wahl der Aggregation entscheidend\n\n\n\n\nBeispieltabelle anschreiben: Kunden A und B, Umsatz 100 und 50, Zufriedenheit (1-100%) 80 und 20"
  },
  {
    "objectID": "03_VL.html#self-service-bi-3",
    "href": "03_VL.html#self-service-bi-3",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI",
    "text": "Self-Service BI\nBesonderheiten Superset\n\n\nDatabase:\n\nBackend-Datenbank, in der die Rohdaten liegen, das Pendant zu einem Data Warehouse\nIn unserem Fall Google BigQuery, aber auch andere Datenbanken wie MySQL, PostgreSQL oder SQLite sind möglich\nAuch der Upload von Excel und CSV Dateien ist möglich\n\nDataset:\n\nEinzelne Tabellen in der Datenbank, die als Grundlage für Analysen und Visualisierungen dienen\nBasieren auf Rohdaten, die für die Visualisierung verarbeitet werden\nPhysische Datasets “leben” auf der Backend-Datenbank, virtuelle Datasets sind direkt in Superset generiert und gespeichert"
  },
  {
    "objectID": "03_VL.html#business-case",
    "href": "03_VL.html#business-case",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nRecap zu den Fragen\n\n\n\n\nFragen zu Tofispy\n\nWächst Tofispy schneller oder langsamer als der Gesamtmarkt?\nWie hoch ist der Marktanteil am Ende von 2024 voraussichtlich?\nWelcher Konkurrent wächst am stärksten?\nSind die Daten dispositiv oder operativ?\nWelcher Reifegrad von Analytics liegt vor?\n\n\n\n\n\n\nTofispy verliert Marktanteile, wächst also langsamer als der Markt\nDer Marktanteil liegt Ende 2024 bei 34%\nYoutubeMusic wächst am stärksten\nDer Datensatz ist dispositiv und der Reifegrad deskriptiv"
  },
  {
    "objectID": "03_VL.html#visualisierung",
    "href": "03_VL.html#visualisierung",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung",
    "text": "Visualisierung\nBalkendiagramm\n\n\n\n\nEinfache Balkendiagramme eignen sich besonders gut zur Visualisierung absoluter Häufigkeiten\nIdealerweise ist die Anzahl der Dimensionen entlang der X-Achse dabei begrenzt\nBei langen Dimensionsnamen ein horizontales Balkendiagramm verwenden, statt die Label auf der X-Achse zu drehen\nZeitachse immer aufsteigend sortieren"
  },
  {
    "objectID": "03_VL.html#visualisierung-1",
    "href": "03_VL.html#visualisierung-1",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung",
    "text": "Visualisierung\nGruppiertes Balkendiagramm\n\n\n\n\nEignen sich für die Darstellung absoluter Häufigkeiten über mehrere Dimensionen\nBei zu großen Unterschieden zwischen Gruppen schwer lesbar\nAuch hier: Bei langen Dimensionsnamen ein horizontales Balkendiagramm verwenden, statt die Label auf der X-Achse zu drehen und Zeitachse immer aufsteigend sortieren"
  },
  {
    "objectID": "03_VL.html#visualisierung-2",
    "href": "03_VL.html#visualisierung-2",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung",
    "text": "Visualisierung\nGestapeltes Balkendiagramm\n\n\n\n\nStatt Darstellung nebeneinander, werden Balken gestapelt\nSinnvoll, wenn die Summe der Beträge, eine sinnvolle Botschaft vermittelt\nAber: Hierbei geht der Eindruck der Anteile der Dimensionen oft verloren, daher nicht ratsam, wenn die Anteile eigentlich von Interesse sind"
  },
  {
    "objectID": "03_VL.html#visualisierung-3",
    "href": "03_VL.html#visualisierung-3",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung",
    "text": "Visualisierung\nRelatives gestapeltes Balkendiagramm\n\n\n\n\nWenn die Anteile der Dimensionen wichtiger sind als die Summe bzw. die Summe keine sinnvolle Interpretation zulässt\nHinweise des einfachen Balkendiagramms gelten weiterhin, ggf. horizontal und Jahreszahlen aufsteigend sortieren"
  },
  {
    "objectID": "03_VL.html#kurzer-überblick-wo-gehts-weiter",
    "href": "03_VL.html#kurzer-überblick-wo-gehts-weiter",
    "title": "Business Intelligence & Data Science",
    "section": "Kurzer Überblick: Wo geht’s weiter?",
    "text": "Kurzer Überblick: Wo geht’s weiter?\nDer ETL Prozess\n\n\n\nBIA Gesamtansatz. Eigene Darstellung in Anlehnung an Baars und Kemper (2021)."
  },
  {
    "objectID": "03_VL.html#etl-prozess",
    "href": "03_VL.html#etl-prozess",
    "title": "Business Intelligence & Data Science",
    "section": "ETL Prozess",
    "text": "ETL Prozess\nWas ist ETL?\n\n\nETL steht für Extract, Transform, Load oder auch Extraktion, Transformation, Laden\nExtraktion beschreibt die Übertragung der Daten aus den operativen Quellsystemen in einen Arbeitsbereich, oft Staging Area genannt\nHier erfolgt die Transformation, die wiederum aus vier Teilschritten besteht:\n\nFilterung\nHarmonisierung\nAggregation\nAnreicherung\n\nAnschließend werden die bereinigten und aufbereiteten Daten in die Zieldatenbank geladen"
  },
  {
    "objectID": "03_VL.html#etl-prozess-1",
    "href": "03_VL.html#etl-prozess-1",
    "title": "Business Intelligence & Data Science",
    "section": "ETL Prozess",
    "text": "ETL Prozess\nTeilschritte der Transformation\n\n\n\n\nTeilprozesse im ETL-Prozess. Eigene Darstellung in Anlehnung an Baars und Kemper (2021)"
  },
  {
    "objectID": "03_VL.html#filterung",
    "href": "03_VL.html#filterung",
    "title": "Business Intelligence & Data Science",
    "section": "Filterung",
    "text": "Filterung\nExtraktion und erste Bereinigung\n\nFilterung umfasst die Extraktion aus operativen Daten und die Bereinigung syntaktischer und semantischer Defekte in den Rohdaten\nDie Extraktion erfolgt vielfältig, z.B. über Flat File Transporte oder API Schnittstellen\nAus Performance-Erwägungen wird die Extraktion mittels geplanter Batch-Jobs oft außerhalb der Betriebszeiten durchgeführt\nBaars und Kemper (2021) ordnen die Extraktion der Filterung zu, da oft nur vorgefilterte Daten übertragen werden, bspw. die letzten 90 Tage oder bestimmte Spalten aus den Rohdaten\nDiese Extrakte werden anschließend bereinigt"
  },
  {
    "objectID": "03_VL.html#filterung-1",
    "href": "03_VL.html#filterung-1",
    "title": "Business Intelligence & Data Science",
    "section": "Filterung",
    "text": "Filterung\nSyntaktische Mängel\n\nSyntaktische Mängel beziehen sich auf Fehler oder Probleme in der Struktur der Daten, die gegen die Syntax- oder Formatregeln verstoßen\nSyntaktische Mängel sind in der Regel einfacher zu erkennen und automatisch zu beheben, da sie auf klaren Regelverstößen basieren\nBeispiele:\n\nFehlende Werte\nWidersprüchliche Datumsformate (2022-04-03 und 04.03.2022)\nLeere Primärschlüssel\nUnzulässige Zeichen wie nicht-numerische Zeichen in numerischen Feldern (z.B. “123a” statt “123”)"
  },
  {
    "objectID": "03_VL.html#filterung-2",
    "href": "03_VL.html#filterung-2",
    "title": "Business Intelligence & Data Science",
    "section": "Filterung",
    "text": "Filterung\nWas ist ein Primärschlüssel?\n\n\n\nEin Primärschlüssel ist ein Attribut oder eine Kombination von Attributen, die eindeutig ein Tupel in einer Tabelle identifizieren\nEin Primärschlüssel darf keine leeren Werte enthalten\nEin Primärschlüssel darf keine Duplikate enthalten\nHäufig wird ein ID-Feld als Primärschlüssel verwendet\nWie muss der Primärschlüssel in der Tabelle rechts aussehen?\n\n\n\n\n\n\n\nVorname\nNachname\n\n\n\n\nEclipse\nEnigma\n\n\nFantastic\nEnigma\n\n\nCrazy\nCommander\n\n\nCrazy\nCameleon\n\n\nHarmony\nHerald\n\n\nOmega\nOracle\n\n\nRadiant\nVoyager"
  },
  {
    "objectID": "03_VL.html#filterung-3",
    "href": "03_VL.html#filterung-3",
    "title": "Business Intelligence & Data Science",
    "section": "Filterung",
    "text": "Filterung\nSemantische Mängel\n\nSemantische Mängel beziehen sich auf Probleme in Bezug auf die Bedeutung und Interpretation der Daten\nDaten sind dann inkonsistent sind oder enthalten widersprüchliche Informationen, selbst wenn sie syntaktisch korrekt sind\nSemantische Mängel erfordern oft ein tieferes Verständnis der Domäne und der Daten, um sie zu identifizieren und zu beheben\nBeispiele:\n\nNegative oder unplausible Werte in Preis-, Alters- oder Umsatzfeldern\nNicht-zulässige Postleitzahlen\nUngültige IBAN"
  },
  {
    "objectID": "03_VL.html#filterung-4",
    "href": "03_VL.html#filterung-4",
    "title": "Business Intelligence & Data Science",
    "section": "Filterung",
    "text": "Filterung\nFinde die Mängel\n\n\n\n\n\nID (Key)\nName\nAlter\nLand\nPLZ\nPremium\nGeburtsdatum\n\n\n\n\n1\nJo (h)N\n25\nDE\n2345\nTRUE\n1999-04-29\n\n\n2\nMary\n30\nDE\n67890\nFALSE\n1994-05-01\n\n\n3\nBob\n40\nDE\n98765\nTRUE\n1984-05-05\n\n\n4\nAlice\n18\nDE\n54321\n1\n2026-04-03\n\n\n\nTom\n19\nDEU\n21345\nFALSE\n2005-04-28\n\n\n\n\n\n\n\n\n\nUserID ist ein Key, leere Werte nicht zugelassen\nJo(h)n ist ein Tippfehler\nLand DEU weicht ab von den anderen Zeilen\nPLZ 2345 ist nicht gültig, da nur 4 Stellen\nPremium ist ein boolscher Wert, sollte also TRUE oder FALSE sein und nicht 1\nDas Geburtsdatum von Alice liegt in der Zukunft\n\n\n\n\nUserID ist ein Key, NA ist nicht zugelassen\nJo(h)n ist ein Tippfehler\nLand DEU ist nicht der ISO-Code für Deutschland\nPLZ 2345 ist nicht gültig, da nur 4 Stellen\nPremium ist ein boolscher Wert, sollte also TRUE oder FALSE sein\nDas Geburtsdatum von Alice liegt in der Zukunft"
  },
  {
    "objectID": "03_VL.html#harmonisierung",
    "href": "03_VL.html#harmonisierung",
    "title": "Business Intelligence & Data Science",
    "section": "Harmonisierung",
    "text": "Harmonisierung\nHarmonisierung\n\nHarmonisierung ist die zweite Schicht der Datentransformation\nNach Abschluss der Filterungs- und der Harmonisierungsschicht liegt im DWH ein bereinigter und konsistenter Datenbestand auf der festgelegten Granularitätsebene vor\nDieser ist bereits direkt für Komponenten der Informationsgenerierung nutzbar\nAuch hier wird zwischen syntaktischer und semantischer Harmonisierung unterschieden"
  },
  {
    "objectID": "03_VL.html#harmonisierung-2",
    "href": "03_VL.html#harmonisierung-2",
    "title": "Business Intelligence & Data Science",
    "section": "Harmonisierung",
    "text": "Harmonisierung\nSyntaktische Harmonisierung\n\nDie operativen Quelldatenbestände weisen meist eine hohe Heterogenität auf und müssen mit Hilfe von umfangreichen Transformationsregeln syntaktisch harmonisiert werden.\nHäufige Gründe sind:\n\n\nSchlüsseldisharmonien\n\nAufgrund verschiedener Primärschlüssel ist die Zusammenführung nicht möglich\nBeispiel: Kundennummer in System A weist eine führende 0 auf, in System B nicht\nOft durch Zuordnungstabellen oder die Generierung neuer Primärschlüssel gelöst"
  },
  {
    "objectID": "03_VL.html#harmonisierung-3",
    "href": "03_VL.html#harmonisierung-3",
    "title": "Business Intelligence & Data Science",
    "section": "Harmonisierung",
    "text": "Harmonisierung\nSyntaktische Harmonisierung\n\nAbweichende Kodierung\n\nDie Systeme weisen identische Attributnamen auf, haben jedoch unterschiedliche Wertebereiche\nBeispiel: Geschlecht in System A: “männlich”, “weiblich”, “divers” und in System B: “m”, “w”, “d”\n\nSynonyme\n\nAttribute haben verschiedene Namen, aber dieselbe Bedeutung\nKundennummer in System A und Customer ID in System B\n\nHomonyme\n\nAttribute haben dieselben Namen, aber verschiedene Bedeutungen\n“Business Partner” als Lieferant in A und als Kunde in B"
  },
  {
    "objectID": "03_VL.html#harmonisierung-4",
    "href": "03_VL.html#harmonisierung-4",
    "title": "Business Intelligence & Data Science",
    "section": "Harmonisierung",
    "text": "Harmonisierung\nSemantische Harmonisierung\n\n\nDie semantische Harmonisierung bezieht sich auf die Vereinheitlichung der fachlichen Bedeutung von Daten\n\n\nAbgleichung fachlicher Kennzahlen\n\nGewährleistet inhaltlich konsistente entscheidungsorientierte Daten\nBeispiele sind Währungen oder Maßeinheiten oder die Periodenzuordnung betriebswirtschaftlicher Kennzahlen\nErfordert hohe Fachkompetenz und ist nicht automatisierbar\n\nGranularität\n\nDie Überführung der operativen Daten in eine gewünschte Granularität erfordert weitere Transforationsregeln\nEine Übersicht tagesaktueller Bestellungen erfordert bspw. die Zusammenfassung aller Transaktionen eines Tages"
  },
  {
    "objectID": "03_VL.html#aggregation",
    "href": "03_VL.html#aggregation",
    "title": "Business Intelligence & Data Science",
    "section": "Aggregation",
    "text": "Aggregation\nVerdichtung und Hierarchisierung\n\nDie Aggregation dient der Erweiterung der gefilterten und harmonisierten Daten um Verdichtungsstrukturen\nIdealerweise mittels mehrfach verwendbarer, zentraler Dimensionshierarchien, die übergreifende Analysen unterstützen\nDie Entwicklung dieser Dimensionshierarchietabellen setzt die Antizipation potenzieller Auswertungen voraus\nDiese Tabellen müssen zudem gepflegt und mit Gültigkeitszeiträumen versehen werden\nDie Erstellung und Pflege von Hierarchietabellen und die Speicherung aggregierter Tabellen ermöglicht erste Anwendungen mit den Daten\nDie physische Speicherung von aggregierten Daten anstelle granularster Ebenen erfolgt hierbei häufig aus Performancegründen"
  },
  {
    "objectID": "03_VL.html#anreicherung",
    "href": "03_VL.html#anreicherung",
    "title": "Business Intelligence & Data Science",
    "section": "Anreicherung",
    "text": "Anreicherung\nKennzahlberechnung\n\nIn der Anreicherungsschicht werden fachliche Kennzahlen berechnet und in die Datenbasis integriert.\nHier können sowohl Werte auf Basis der harmonisierten Daten der gewünschten Granularität (zweite Schicht) als auch auf Basis der dritten Schicht (bereits aggregierte Tabellen) berechnet werden.\nBeispiele sind monatliche Deckungsbeiträge auf Produktebene oder jährliche Deckungsbeiträge auf Filialebene."
  },
  {
    "objectID": "03_VL.html#anreicherung-1",
    "href": "03_VL.html#anreicherung-1",
    "title": "Business Intelligence & Data Science",
    "section": "Anreicherung",
    "text": "Anreicherung\nKennzahlberechnung\n\nDiese Vorgehensweise hat mehrere Vorteile:\n\nKalkulierbare Reaktionszeiten bei späteren Abfragen aufgrund der Vorausberechnung von Kennzahlen.\nGarantierte Konsistenz der kalkulierten Werte, da sie anwendungsübergreifend einmalig gebildet und persistiert werden.\nEtablierung eines abgestimmten betriebswirtschaftlichen Definitionsraumes"
  },
  {
    "objectID": "03_VL.html#business-case-1",
    "href": "03_VL.html#business-case-1",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nZurück zu Tofispy\n\nTofispy hat uns einen Extrakt aus dem System für Neuregistrierung zur Verfügung gestellt\nDer Datensatz ist bei Superset im Dataset Controlling unter “user_events” hinterlegt, hierbei handelt es sich um den Rohdatensatz, den wir erst einmal transformieren müssen\nErneuter Startpunkt ist eine explorative Analyse, um die Daten zu verstehen\nIn Superset:\n\nOben auf + Chart\nDataset “user_events” auswählen\nTable als Visualisierung auswählen"
  },
  {
    "objectID": "03_VL.html#business-case-2",
    "href": "03_VL.html#business-case-2",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nTable Visualisierung in Superset"
  },
  {
    "objectID": "03_VL.html#business-case-3",
    "href": "03_VL.html#business-case-3",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nTable Visualisierung in Superset"
  },
  {
    "objectID": "03_VL.html#business-case-4",
    "href": "03_VL.html#business-case-4",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nTransformationschritte\n\nDie Daten enthalten mehrere Event-Types und Timestamps\nWelche der Events sind interessant, um User-Wachstum zu verstehen?\nWelche Aggregation des Zeitstempels ist sinnvoll?\nWelche Kennzahlen wollen wir aus den Daten ableiten?"
  },
  {
    "objectID": "03_VL.html#quellen",
    "href": "03_VL.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nBaars, Henning, und Hans-Georg Kemper. 2021. Business Intelligence & Analytics: Grundlagen und praktische Anwendungen: Ansätze der IT-basierten Entscheidungsunterstützung. 4., überarbeitete und erweiterte Auflage. Lehrbuch. Wiesbaden [Heidelberg]: Springer Vieweg."
  },
  {
    "objectID": "03_VL_D.html#der-plan-für-heute",
    "href": "03_VL_D.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung 3\n\n\nQuiz\nRecap zur Aufgabe von letzter Woche\nVisualisierung von Häufigkeiten: Balkendiagramm\nDatentransformation:\n\nWas heißt ETL?\nWelche Transformationschritte durchlaufen operative Daten bis zur Speicherung im DWH?\n\nGemeinsame Transformation eines Datensatzes mit User-Events"
  },
  {
    "objectID": "03_VL_D.html#business-case",
    "href": "03_VL_D.html#business-case",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nRecap zu den Fragen\n\n\n\n\nFragen zu Tofispy\n\nWächst Tofispy schneller oder langsamer als der Gesamtmarkt?\nWie hoch ist der Marktanteil am Ende von 2024 voraussichtlich?\nWelcher Konkurrent wächst am stärksten?\nSind die Daten dispositiv oder operativ?\nWelcher Reifegrad von Analytics liegt vor?\n\n\n\n\n\n\nTofispy verliert Marktanteile, wächst also langsamer als der Markt\nDer Marktanteil liegt Ende 2024 bei 34%\nYoutubeMusic wächst am stärksten\nDer Datensatz ist dispositiv und der Reifegrad deskriptiv"
  },
  {
    "objectID": "03_VL_D.html#visualisierung",
    "href": "03_VL_D.html#visualisierung",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung",
    "text": "Visualisierung\nBalkendiagramm\n\n\n\n\nEinfache Balkendiagramme eignen sich besonders gut zur Visualisierung absoluter Häufigkeiten\nIdealerweise ist die Anzahl der Dimensionen entlang der X-Achse dabei begrenzt\nBei langen Dimensionsnamen ein horizontales Balkendiagramm verwenden, statt die Label auf der X-Achse zu drehen\nZeitachse immer aufsteigend sortieren"
  },
  {
    "objectID": "03_VL_D.html#visualisierung-1",
    "href": "03_VL_D.html#visualisierung-1",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung",
    "text": "Visualisierung\nGruppiertes Balkendiagramm\n\n\n\n\nEignen sich für die Darstellung absoluter Häufigkeiten über mehrere Dimensionen\nBei zu großen Unterschieden zwischen Gruppen schwer lesbar\nAuch hier: Bei langen Dimensionsnamen ein horizontales Balkendiagramm verwenden, statt die Label auf der X-Achse zu drehen und Zeitachse immer aufsteigend sortieren"
  },
  {
    "objectID": "03_VL_D.html#visualisierung-2",
    "href": "03_VL_D.html#visualisierung-2",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung",
    "text": "Visualisierung\nGestapeltes Balkendiagramm\n\n\n\n\nStatt Darstellung nebeneinander, werden Balken gestapelt\nSinnvoll, wenn die Summe der Beträge, eine sinnvolle Botschaft vermittelt\nAber: Hierbei geht der Eindruck der Anteile der Dimensionen oft verloren, daher nicht ratsam, wenn die Anteile eigentlich von Interesse sind"
  },
  {
    "objectID": "03_VL_D.html#visualisierung-3",
    "href": "03_VL_D.html#visualisierung-3",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung",
    "text": "Visualisierung\nRelatives gestapeltes Balkendiagramm\n\n\n\n\nWenn die Anteile der Dimensionen wichtiger sind als die Summe bzw. die Summe keine sinnvolle Interpretation zulässt\nHinweise des einfachen Balkendiagramms gelten weiterhin, ggf. horizontal und Jahreszahlen aufsteigend sortieren\nBei mehr als 5 Dimensionen oft unübersichtlich"
  },
  {
    "objectID": "03_VL_D.html#kurzer-überblick-wo-gehts-weiter",
    "href": "03_VL_D.html#kurzer-überblick-wo-gehts-weiter",
    "title": "Business Intelligence & Data Science",
    "section": "Kurzer Überblick: Wo geht’s weiter?",
    "text": "Kurzer Überblick: Wo geht’s weiter?\nDer ETL Prozess\n\n\n\nBIA Gesamtansatz. Eigene Darstellung in Anlehnung an Baars und Kemper (2021)."
  },
  {
    "objectID": "03_VL_D.html#etl-prozess",
    "href": "03_VL_D.html#etl-prozess",
    "title": "Business Intelligence & Data Science",
    "section": "ETL Prozess",
    "text": "ETL Prozess\nWas ist ETL?\n\n\nETL steht für Extract, Transform, Load oder auch Extraktion, Transformation, Laden\nExtraktion beschreibt die Übertragung der Daten aus den operativen Quellsystemen in einen Arbeitsbereich, oft Staging Area genannt\nHier erfolgt die Transformation, die wiederum aus vier Teilschritten besteht:\n\nFilterung\nHarmonisierung\nAggregation\nAnreicherung\n\nAnschließend werden die bereinigten und aufbereiteten Daten in die Zieldatenbank geladen"
  },
  {
    "objectID": "03_VL_D.html#etl-prozess-1",
    "href": "03_VL_D.html#etl-prozess-1",
    "title": "Business Intelligence & Data Science",
    "section": "ETL Prozess",
    "text": "ETL Prozess\nTeilschritte der Transformation\n\n\n\n\nTeilprozesse im ETL-Prozess. Eigene Darstellung in Anlehnung an Baars und Kemper (2021)"
  },
  {
    "objectID": "03_VL_D.html#filterung",
    "href": "03_VL_D.html#filterung",
    "title": "Business Intelligence & Data Science",
    "section": "Filterung",
    "text": "Filterung\nExtraktion und erste Bereinigung\n\nFilterung umfasst die Extraktion aus operativen Daten und die Bereinigung syntaktischer und semantischer Defekte in den Rohdaten\nDie Extraktion erfolgt vielfältig, z.B. über Flat File Transporte oder API Schnittstellen\nAus Performance-Erwägungen wird die Extraktion mittels geplanter Batch-Jobs oft außerhalb der Betriebszeiten durchgeführt\nBaars und Kemper (2021) ordnen die Extraktion der Filterung zu, da oft nur vorgefilterte Daten übertragen werden, bspw. die letzten 90 Tage oder bestimmte Spalten aus den Rohdaten\nDiese Extrakte werden anschließend bereinigt"
  },
  {
    "objectID": "03_VL_D.html#filterung-1",
    "href": "03_VL_D.html#filterung-1",
    "title": "Business Intelligence & Data Science",
    "section": "Filterung",
    "text": "Filterung\nSyntaktische Mängel\n\nSyntaktische Mängel beziehen sich auf Fehler oder Probleme in der Struktur der Daten, die gegen die Syntax- oder Formatregeln verstoßen\nIn der Regel einfacher zu erkennen und automatisch zu beheben, da sie auf klaren Regelverstößen basieren\nBeispiele:\n\nFehlende Werte\nWidersprüchliche Datumsformate (2022-04-03 und 04.03.2022)\nLeere Primärschlüssel\nUnzulässige Zeichen wie nicht-numerische Zeichen in numerischen Feldern (z.B. “123a” statt “123”)"
  },
  {
    "objectID": "03_VL_D.html#filterung-2",
    "href": "03_VL_D.html#filterung-2",
    "title": "Business Intelligence & Data Science",
    "section": "Filterung",
    "text": "Filterung\nWas ist ein Primärschlüssel?\n\n\n\nEin Primärschlüssel ist ein Attribut oder eine Kombination von Attributen, die eindeutig ein Tupel in einer Tabelle identifizieren\nEin Primärschlüssel darf keine leeren Werte enthalten\nEin Primärschlüssel darf keine Duplikate enthalten\nHäufig wird ein ID-Feld als Primärschlüssel verwendet\nWie muss der Primärschlüssel in der Tabelle rechts aussehen?\n\n\n\n\n\n\n\nVorname\nNachname\n\n\n\n\nEclipse\nEnigma\n\n\nFantastic\nEnigma\n\n\nCrazy\nCommander\n\n\nCrazy\nCameleon\n\n\nHarmony\nHerald\n\n\nOmega\nOracle\n\n\nRadiant\nVoyager"
  },
  {
    "objectID": "03_VL_D.html#filterung-3",
    "href": "03_VL_D.html#filterung-3",
    "title": "Business Intelligence & Data Science",
    "section": "Filterung",
    "text": "Filterung\nSemantische Mängel\n\nSemantische Mängel beziehen sich auf Probleme in Bezug auf die Bedeutung und Interpretation der Daten\nDaten sind dann inkonsistent sind oder enthalten widersprüchliche Informationen, selbst wenn sie syntaktisch korrekt sind\nSemantische Mängel erfordern oft ein tieferes Verständnis der Domäne und der Daten, um sie zu identifizieren und zu beheben\nBeispiele:\n\nNegative oder unplausible Werte in Preis-, Alters- oder Umsatzfeldern\nNicht-zulässige Postleitzahlen\nUngültige IBAN"
  },
  {
    "objectID": "03_VL_D.html#filterung-4",
    "href": "03_VL_D.html#filterung-4",
    "title": "Business Intelligence & Data Science",
    "section": "Filterung",
    "text": "Filterung\nFinde die Mängel\n\n\n\n\n\nID (Key)\nName\nAlter\nLand\nPLZ\nPremium\nGeburtsdatum\n\n\n\n\n1\nJo (h)N\n25\nDE\n2345\nTRUE\n1999-04-29\n\n\n2\nMary\n30\nDE\n67890\nFALSE\n1994-05-01\n\n\n3\nBob\n40\nDE\n98765\nTRUE\n1984-05-05\n\n\n4\nAlice\n18\nDE\n54321\n1\n2026-04-03\n\n\n\nTom\n19\nDEU\n21345\nFALSE\n2005-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUserID ist ein Key, NA ist nicht zugelassen\nJo(h)n ist ein Tippfehler\nLand DEU ist nicht der ISO-Code für Deutschland\nPLZ 2345 ist nicht gültig, da nur 4 Stellen\nPremium ist ein boolscher Wert, sollte also TRUE oder FALSE sein\nDas Geburtsdatum von Alice liegt in der Zukunft"
  },
  {
    "objectID": "03_VL_D.html#harmonisierung",
    "href": "03_VL_D.html#harmonisierung",
    "title": "Business Intelligence & Data Science",
    "section": "Harmonisierung",
    "text": "Harmonisierung\nHarmonisierung\n\nHarmonisierung ist die zweite Schicht der Datentransformation\nNach Abschluss der Filterungs- und der Harmonisierungsschicht liegt im DWH ein bereinigter und konsistenter Datenbestand auf der festgelegten Granularitätsebene vor\nDieser ist bereits direkt für Komponenten der Informationsgenerierung nutzbar\nAuch hier wird zwischen syntaktischer und semantischer Harmonisierung unterschieden"
  },
  {
    "objectID": "03_VL_D.html#harmonisierung-2",
    "href": "03_VL_D.html#harmonisierung-2",
    "title": "Business Intelligence & Data Science",
    "section": "Harmonisierung",
    "text": "Harmonisierung\nSyntaktische Harmonisierung\n\nDie operativen Quelldatenbestände weisen meist eine hohe Heterogenität auf und müssen mit Hilfe von umfangreichen Transformationsregeln syntaktisch harmonisiert werden.\nHäufige Gründe sind:\n\n\nSchlüsseldisharmonien\n\nAufgrund verschiedener Primärschlüssel ist die Zusammenführung nicht möglich\nBeispiel: Kundennummer in System A weist eine führende 0 auf, in System B nicht\nOft durch Zuordnungstabellen oder die Generierung neuer Primärschlüssel gelöst"
  },
  {
    "objectID": "03_VL_D.html#harmonisierung-3",
    "href": "03_VL_D.html#harmonisierung-3",
    "title": "Business Intelligence & Data Science",
    "section": "Harmonisierung",
    "text": "Harmonisierung\nSyntaktische Harmonisierung\n\nAbweichende Kodierung\n\nDie Systeme weisen identische Attributnamen auf, haben jedoch unterschiedliche Wertebereiche\nBeispiel: Geschlecht in System A: “männlich”, “weiblich”, “divers” und in System B: “m”, “w”, “d”\n\nSynonyme\n\nAttribute haben verschiedene Namen, aber dieselbe Bedeutung\nKundennummer in System A und Customer ID in System B\n\nHomonyme\n\nAttribute haben dieselben Namen, aber verschiedene Bedeutungen\n“Business Partner” als Lieferant in A und als Kunde in B"
  },
  {
    "objectID": "03_VL_D.html#harmonisierung-4",
    "href": "03_VL_D.html#harmonisierung-4",
    "title": "Business Intelligence & Data Science",
    "section": "Harmonisierung",
    "text": "Harmonisierung\nSemantische Harmonisierung\n\n\nDie semantische Harmonisierung bezieht sich auf die Vereinheitlichung der fachlichen Bedeutung von Daten\n\n\nAbgleichung fachlicher Kennzahlen\n\nGewährleistet inhaltlich konsistente entscheidungsorientierte Daten\nBeispiele sind Währungen oder Maßeinheiten oder die Periodenzuordnung betriebswirtschaftlicher Kennzahlen\nErfordert hohe Fachkompetenz und ist nicht automatisierbar\n\nGranularität\n\nDie Überführung der operativen Daten in eine gewünschte Granularität erfordert weitere Transforationsregeln\nEine Übersicht tagesaktueller Bestellungen erfordert bspw. die Zusammenfassung aller Transaktionen eines Tages"
  },
  {
    "objectID": "03_VL_D.html#aggregation",
    "href": "03_VL_D.html#aggregation",
    "title": "Business Intelligence & Data Science",
    "section": "Aggregation",
    "text": "Aggregation\nVerdichtung und Hierarchisierung\n\nDie Aggregation dient der Erweiterung der gefilterten und harmonisierten Daten um Verdichtungsstrukturen\nIdealerweise mittels mehrfach verwendbarer, zentraler Dimensionshierarchien, die übergreifende Analysen unterstützen\nDie Entwicklung dieser Dimensionshierarchietabellen setzt die Antizipation potenzieller Auswertungen voraus\nDiese Tabellen müssen zudem gepflegt und mit Gültigkeitszeiträumen versehen werden\nDie Erstellung und Pflege von Hierarchietabellen und die Speicherung aggregierter Tabellen ermöglicht erste Anwendungen mit den Daten\nDie physische Speicherung von aggregierten Daten anstelle granularster Ebenen erfolgt hierbei häufig aus Performancegründen"
  },
  {
    "objectID": "03_VL_D.html#anreicherung",
    "href": "03_VL_D.html#anreicherung",
    "title": "Business Intelligence & Data Science",
    "section": "Anreicherung",
    "text": "Anreicherung\nKennzahlberechnung\n\nIn der Anreicherungsschicht werden fachliche Kennzahlen berechnet und in die Datenbasis integriert.\nHier können sowohl Werte auf Basis der harmonisierten Daten der gewünschten Granularität (zweite Schicht) als auch auf Basis der dritten Schicht (bereits aggregierte Tabellen) berechnet werden.\nBeispiele sind monatliche Deckungsbeiträge auf Produktebene oder jährliche Deckungsbeiträge auf Filialebene."
  },
  {
    "objectID": "03_VL_D.html#anreicherung-1",
    "href": "03_VL_D.html#anreicherung-1",
    "title": "Business Intelligence & Data Science",
    "section": "Anreicherung",
    "text": "Anreicherung\nKennzahlberechnung\n\nDiese Vorgehensweise hat mehrere Vorteile:\n\nKalkulierbare Reaktionszeiten bei späteren Abfragen aufgrund der Vorausberechnung von Kennzahlen.\nGarantierte Konsistenz der kalkulierten Werte, da sie anwendungsübergreifend einmalig gebildet und persistiert werden.\nEtablierung eines abgestimmten betriebswirtschaftlichen Definitionsraumes"
  },
  {
    "objectID": "03_VL_D.html#business-case-1",
    "href": "03_VL_D.html#business-case-1",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nZurück zu Tofispy\n\nTofispy hat uns einen Extrakt aus dem System für Neuregistrierung zur Verfügung gestellt\nDer Datensatz ist bei Superset im Dataset Controlling unter “user_events” hinterlegt, hierbei handelt es sich um den Rohdatensatz, den wir erst einmal transformieren müssen\nErneuter Startpunkt ist eine explorative Analyse, um die Daten zu verstehen\nIn Superset:\n\nOben auf + Chart\nDataset “user_events” auswählen\nTable als Visualisierung auswählen"
  },
  {
    "objectID": "03_VL_D.html#business-case-2",
    "href": "03_VL_D.html#business-case-2",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nTable Visualisierung in Superset"
  },
  {
    "objectID": "03_VL_D.html#business-case-3",
    "href": "03_VL_D.html#business-case-3",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nTable Visualisierung in Superset"
  },
  {
    "objectID": "03_VL_D.html#business-case-4",
    "href": "03_VL_D.html#business-case-4",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nTransformationschritte\n\nDie Daten enthalten mehrere Event-Types und Timestamps\nWelche der Events sind interessant, um User-Wachstum zu verstehen?\nWelche Aggregation des Zeitstempels ist sinnvoll?\nWelche Kennzahlen wollen wir aus den Daten ableiten?"
  },
  {
    "objectID": "03_VL_D.html#quellen",
    "href": "03_VL_D.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nBaars, Henning, und Hans-Georg Kemper. 2021. Business Intelligence & Analytics: Grundlagen und praktische Anwendungen: Ansätze der IT-basierten Entscheidungsunterstützung. 4., überarbeitete und erweiterte Auflage. Lehrbuch. Wiesbaden [Heidelberg]: Springer Vieweg."
  },
  {
    "objectID": "02_VL.html#der-plan-für-heute",
    "href": "02_VL.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung 2\n\nDatenbereitstellung:\n\nData Warehouse\nData Mart\nArchitekturkonzepte\n\nBusiness Case: Der Musikstreaming Anbieter Tofispy braucht unsere Hilfe\nVerknüpfung eines ersten Data Marts mit Superset oder Datenanalyse mit Excel"
  },
  {
    "objectID": "02_VL.html#überblick-zum-bia-gesamteinsatz",
    "href": "02_VL.html#überblick-zum-bia-gesamteinsatz",
    "title": "Business Intelligence & Data Science",
    "section": "Überblick zum BIA Gesamteinsatz",
    "text": "Überblick zum BIA Gesamteinsatz\nDatenbereitstellung\n\n\n\nBIA Gesamtansatz. Eigene Darstellung in Anlehnung an Baars und Kemper (2021)."
  },
  {
    "objectID": "02_VL.html#data-warehouse",
    "href": "02_VL.html#data-warehouse",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nBegriff\n\n\n\n\n\n\nDefinition\n\n\nEin Data Warehouse (DWH) ist ein Datenhaltungssystem dispositiver Daten, das von den operativen Datenbeständen getrennt, themenorientiert aufbereitet und logisch zentralisiert ist. Ein DWH integriert unternehmensweit Datenbestände aus verschiedenen operativen internen Systemen (z.B. Kernbanksystemen und Enterprise-Ressource-Planning-Systemen) sowie externen Systemen (z.B. Börseninformationssystemen und Systeme für externe Ratings) und dient idealtypisch als unternehmensweite, einheitliche und konsistente Datenbasis für alle Arten von Systemen der Entscheidungsunterstützung (siehe Kemper und Sun 2023).\n\n\n\n\n\nDie entscheidenden Punkte sind hier:\n\nTrennung von operativen und dispositiven Daten\nIntegration von Datenbeständen aus verschiedenen und oftmals sehr heterogenen Quellen\nEinheitliche und konsistente Datenbasis für das gesamte Unternehmen"
  },
  {
    "objectID": "02_VL.html#data-warehouse-1",
    "href": "02_VL.html#data-warehouse-1",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nEigenschaften\n\n\n\nThemenorientierung:\n\nDispositive Daten des DWH sind explizit an den Interessenslagen der Entscheidenden ausgerichtet\nDie operativen bzw. externen Daten werden vor der Speicherung im DWH aufbereitet, harmonisiert und ggf. voraggregiert\nThemen sind Produkthierarchien, vordefinierte Zeiträume wie Quartale oder betriebswirtschaftliche Kennzahlen wie DB1\n\nIntegration:\n\nDaten aus den unterschiedlichen operativen und externen Systemen werden im DWH integriert\nZusammenführung zu einer inhaltlich widerspruchfreien Datenquelle, sogenannter “single point of truth”"
  },
  {
    "objectID": "02_VL.html#data-warehouse-2",
    "href": "02_VL.html#data-warehouse-2",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nEigenschaften\n\n\nZeitraumbezug:\n\nOperative Systeme sind transaktionsorientiert und bilden einen bestimmten Zeitpunkt ab\nDaten im DWH werden üblicherweise auf Zeiträume aggregiert, bspw. ein Monat oder ein Jahr\n\nNicht-Volatilität:\n\nDaten im DWH werden dauerhaft abgelegt und für die Analyse zur Verfügung gestellt\nDWH-Daten werden somit in der Regel nicht mehr geändert, überschrieben oder entfernt"
  },
  {
    "objectID": "02_VL.html#data-warehouse-3",
    "href": "02_VL.html#data-warehouse-3",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nKomponenten des Data Warehouse\n\n\n\nData Mart\n\nSeparater Datenpool für einen bestimmten Anwendungsbereich spezifischer Abteilungen\nNur ein Ausschnitt aus dem gesamten Datenpool, häufig aus Performance-Erwägungen\nHäufig mit Reporting- und OLAP assoziiert, zunehmend aber auch für Analysen\n\n\nCore Data Warehouse\n\nRückrat der meisten Architekturkonzepte und oft als Basisdatenbank bezeichnet\nBefüllung über ETL-Prozesse aus operativen Quellsystemen\nMeist auf relationalen Datenhaltungssystemen basierend mit großen Datenvolumina (TB Bereich)\nApplikationsneutral modelliert"
  },
  {
    "objectID": "02_VL.html#data-warehouse-4",
    "href": "02_VL.html#data-warehouse-4",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nFunktionen des CDWH\n\nSammel- und Integrationsfunktion:\n\n\n\nAufnahme aller wichtigen Daten für die Analyse in Form eines zentralen Datenlagers\n\n\n\nDistributionsfunktion:\n\n\n\nVersorgung aller nachgeschalteten Data Marts mit Daten\n\n\n\nQualitätssicherungsfunktion:\n\n\n\nDatentransformation sichert die syntaktische und semantische Stimmigkeit der dispositiven Datenbasis"
  },
  {
    "objectID": "02_VL.html#data-warehouse-5",
    "href": "02_VL.html#data-warehouse-5",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nData Mart vs. CDWH\n\n\n\nCharakteristika von Data Mart und Core Data Warehouse im Vergleich. In Anlehnung an Baars und Kemper (2021)\n\n\n\n\n\n\n\n\nData Mart\nCore Data Warehouse\n\n\n\n\nZiel\nEntscheidungsunterstützung für ausgewählte Bereiche, spezifisch auf Analyseanforderungen zugeschnitten\nEntscheidungsunterstützung für alle Bereiche in einem Unternehmen\n\n\nAusrichtung\nBereichsspezifisch oder Abteilungsbezogen\nZentral und unternehmensweit\n\n\nGranularität\nHöhere Aggregationen\nFeinste verfügbare Granularität\n\n\nVerfügbarkeit für Endanwendende\nIn der Regel möglich\nHäufig nicht erlaubt da zentral durch IT betrieben und als Quellsystem für Marts genutzt\n\n\nFlexibilität der Analysen\nTendenziell gering und auf Anwendungsbereich beschränkt\nSehr flexibel\n\n\nVolumina\nGering bis moderat\nModerat bis umfangreich"
  },
  {
    "objectID": "02_VL.html#dwh-architekturen",
    "href": "02_VL.html#dwh-architekturen",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nUnabhängige Data Marts\n\n\n\n\n\n\n\n\nUnabhängige Data Marts. Eigene Darstellung in Anlehnung an Hahne (2016)\n\n\n\n\n\n\nAuch Stove-Pipe Ansatz\nBedienen sich direkt aus den operativen und externen Systemen\nBereiten die enthaltenen Daten für relevante Anwendungsfelder auf\nDaten werden isoliert bezogen und fließen direkt in Datensilos auf Basis bereichsspezifischer Fragestellungen\nVerschiedene Marts können unterschiedliche externe Datenquellen zusammenführen\n\n\n\n\n\n\nStove-Pipe Ansatz heißt so viel wie Ofenrohr, also eine direkte Verbindung zwischen Quelle und Ziel"
  },
  {
    "objectID": "02_VL.html#dwh-architekturen-1",
    "href": "02_VL.html#dwh-architekturen-1",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nUnabhängige Data Marts\n\n\n\nVorteile:\n\nSchnelle und bereichsspezifische Informationsbereitstellung\nSinnvoll bei fehlender Governance Strategie\nErfüllung maßgeschneideter bereichsspezifischer Fragestellungen\n\n\nNachteile:\n\nHäufig historisch gewachsene Strukturen und damit geringe Governance\nMehrfache Aufbereitung der Quelldaten\nGefahr von Inkonsistenzen bei der Kennzahlenberechnung zwischen Marts\nMangelnde Möglichkeit bereichsübergreifender Analysen"
  },
  {
    "objectID": "02_VL.html#dwh-architekturen-2",
    "href": "02_VL.html#dwh-architekturen-2",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nAbgestimmte Data Marts\n\n\n\n\n\n\n\nAbhängige Data Marts. Eigene Darstellung in Anlehnung an Hahne (2016)\n\n\n\n\n\n\nKonzeptionell abgestimmte Datenmodelle um die Integrität des Datenmaterials zu gewährleisten\nDas abgestimmte Datenmodell dient der syntaktischen und semantischen Vereinheitlichung\nInhaltliche und zeitliche Übereinstimmung der Datenextraktionen ist entscheidend für die Konsistenz der Data Marts"
  },
  {
    "objectID": "02_VL.html#dwh-architekturen-3",
    "href": "02_VL.html#dwh-architekturen-3",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nAbgestimmte Data Marts\n\n\n\nVorteile:\n\nIntegrität des Datenmodells wird gewährleistet\nMöglichkeit bereichsübergreifender Analysen bei hoher Flexibilität innerhalb der Bereiche\nEntscheidungsunterstützung für alle Bereiche in einem Unternehmen\n\n\nNachteile:\n\nDurch hohen Bereichsbezug oft unterschiedliche Granularität oder Aufbereitung, damit nur bedingte Integration zwischen Marts\nMöglicherweise höherer Abstimmungsbedarf zwischen Abteilungen bei der Kennzahldefinition\nInformationsverlust bei übergreifenden Analysen"
  },
  {
    "objectID": "02_VL.html#dwh-architekturen-4",
    "href": "02_VL.html#dwh-architekturen-4",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nCore Data Warehouse\n\n\n\n\n\n\n\nCore Data Warehouse. Eigene Darstellung in Anlehnung an Hahne (2016)\n\n\n\n\n\n\nDas CDWH wird direkt aus den operativen Quellsystemen befüllt und basiert auf einer relationalen Datenbank\nDieser Ansatz wird oft als Monolith bezeichnet\nDie Daten decken unterschiedliche Auswertungszwecke ab und sind weniger anwendungsbezogen als Datensilos"
  },
  {
    "objectID": "02_VL.html#dwh-architekturen-5",
    "href": "02_VL.html#dwh-architekturen-5",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nCore Data Warehouse\n\n\n\nVorteile:\n\nHoher Grad an Mehrfachverwendbarkeit der Daten\nHoher Detailgrad möglich\nBei kleineren Anwendungsfällen oft ausreichend\n\n\nNachteile:\n\nBerechtigungsmanagement und Performance stoßen bei komplexen Anwendungsfällen schnell an Grenzen\nBei größeren Einheiten mit eigenen Geschäftsprozessen und stark abweichenden Hierarchiestrukturen sehr komplex, hier bietet sich der Einsatz mehrerer CDWH an"
  },
  {
    "objectID": "02_VL.html#dwh-architekturen-6",
    "href": "02_VL.html#dwh-architekturen-6",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nCore Data Warehouse mit abhängigen Data Marts\n\n\n\n\nAuch Hub-and-Spoke Ansatz genannt\nCDWH wird nicht direkt für Analysen herangezogen, sondern dient der Befüllung von Marts\nMarts sind dann anwendungsbezogen, weisen aber ein einheitliches Datenmodell auf\nCDWH als Hub erfüllt Aufgaben der Integration, Qualitätssicherung und Datenverteilung an die Marts\n\n\n\n\n\n\n\n\nCore Data Warehouse mit abhängigen Data Marts. Eigene Darstellung in Anlehnung an Hahne (2016)"
  },
  {
    "objectID": "02_VL.html#dwh-architekturen-7",
    "href": "02_VL.html#dwh-architekturen-7",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nCore Data Warehouse mit abhängigen Data Marts\n\n\n\nVorteile:\n\nEinmaliger und einheitlicher Transformationsprozess ohne redundante Transformationslogik\nGeringere Anzahl an Extraktionsprozessen\nReduzierte Anzahl direkter Schnittstellen zwischen Marts und operativen Daten\n\n\nNachteile:\n\nNach wie vor eher traditionelle BI-Sicht"
  },
  {
    "objectID": "02_VL.html#dwh-architekturen-8",
    "href": "02_VL.html#dwh-architekturen-8",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nArchitekturen-Mix\n\n\n\n\n\nArchitekturen-Mix. Eigene Darstellung in Anlehnung an Hahne (2016)"
  },
  {
    "objectID": "02_VL.html#business-case",
    "href": "02_VL.html#business-case",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nDas Wachstum bei Tofispy stagniert\n\n\n\nTofispy ist ein Musikstreaming Anbieter aus Deutschland\nDas Unternehmen kämpft mit stagnierendem Wachstum\nWir – die Unternehmensberatung LeinbizConsult – wurden beauftragt, die Gründe für das stagnierende Wachstum zu identifizieren und Handlungsempfehlungen zu entwickeln\nDie Geschäftsführung hat uns erste Daten zur Verfügung gestellt, um die aktuelle Situation zu evaluieren und Ursachenforschung zu betreiben"
  },
  {
    "objectID": "02_VL.html#business-case-1",
    "href": "02_VL.html#business-case-1",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nDatensatz\n\nÜberblick über den Streamingmarkt:\n\nDataset “market_share”\nAnzahl der Nutzer pro Plattform in Mio\nDaten von 2016 bis 2024\nDie Daten für 2024 sind eine Hochrechnung für das laufende Jahr"
  },
  {
    "objectID": "02_VL.html#business-case-2",
    "href": "02_VL.html#business-case-2",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nAufgabe\n\n\nArbeit in 2er Teams mit mindestens einem Laptop oder Tablet (nicht ideal für Excel) pro Team\nJedes Team lädt die Daten bei StudIP herunter und diskutiert untereinander, wie die Daten strukturiert sind\nTrial & Error in Superset mit Hilfe der Dokumentation:\n\nDokumentation unter preset.io\nStartpunkt: Creating a Chart (in der preset Doku)\nTable als Ausgangspunkt für explorative Analyse\n\nAnschließend Analyse in Excel + Superset und kritische Gegenüberstellung der beiden Ansätze"
  },
  {
    "objectID": "02_VL.html#business-case-3",
    "href": "02_VL.html#business-case-3",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nFragen\n\n\nWas sind die Vorteile der beiden Ansätze?\nWächst Tofispy schneller oder langsamer als der Gesamtmarkt?\nWie hoch ist der Marktanteil am Ende von 2024 voraussichtlich?\nWelcher Konkurrent wächst am stärksten?\nIst der Datensatz operativ oder dispositiv?\nWelcher Reifegrad von Analytics liegt vor?"
  },
  {
    "objectID": "02_VL.html#hausaufgabe",
    "href": "02_VL.html#hausaufgabe",
    "title": "Business Intelligence & Data Science",
    "section": "Hausaufgabe",
    "text": "Hausaufgabe\nVideo anschauen\n\nLink zum Video"
  },
  {
    "objectID": "02_VL.html#hausaufgabe-1",
    "href": "02_VL.html#hausaufgabe-1",
    "title": "Business Intelligence & Data Science",
    "section": "Hausaufgabe",
    "text": "Hausaufgabe\nFragen zu Superset\n\nWas sind Dimensions und Metrics? Welchen Zweck haben sie?\nWas ist der Unterschied zwischen einem Dataset und einer Database?\nWas ist der Unterschied zwischen Superset und Preset?"
  },
  {
    "objectID": "02_VL.html#quellen",
    "href": "02_VL.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nBaars, Henning, und Hans-Georg Kemper. 2021. Business Intelligence & Analytics: Grundlagen und praktische Anwendungen: Ansätze der IT-basierten Entscheidungsunterstützung. 4., überarbeitete und erweiterte Auflage. Lehrbuch. Wiesbaden [Heidelberg]: Springer Vieweg.\n\n\nHahne, Michael. 2016. „Architekturkonzepte und Modellierungsverfahren für BI-Systeme“. In Analytische Informationssysteme, herausgegeben von Peter Gluchowski und Peter Chamoni, 147–85. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-47763-2_8.\n\n\nKemper, Hans-Georg, und Xuanpu Sun. 2023. „Data Warehouse“. In Gabler Bankenlexikon. https://www.gabler-banklexikon.de/definition/data-warehouse-56847/version-377924."
  },
  {
    "objectID": "05_VL.html#der-plan-für-heute",
    "href": "05_VL.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung\n\n\nQuiz und Recap\nVisualisierung von Anteilen: Kreisdiagramme und Treemaps\nInformationsgenerierung\n\nWelche Formen von Berichten gibt es?\nWas ist OLAP?\nWo genau unterscheiden sich Self-Sevice BI und traditionelle BI?\n\nModellgestützte Analysen\n\nWie läuft ein Data Science Projekt ab?\nBegriffliche Grundlagen Data Science"
  },
  {
    "objectID": "05_VL.html#business-case",
    "href": "05_VL.html#business-case",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nErgebnisse von letzter Woche\n\n\n\n\nIn der letzten Woche haben wir festgestellt, dass der Hauptgrund für das schwindende Wachstum bei Tofispy ein stark steigender Trend bei den Kündigungen ist\nLaut einer Umfrage ist der Hauptgrund dafür eine Unzufriedenheit mit dem empfohlenen Content\nAlle Ergebnisse sind im Dashboard “Tofispy Marktanalyse” zusammengefasst"
  },
  {
    "objectID": "05_VL.html#visualisierung-von-anteilen",
    "href": "05_VL.html#visualisierung-von-anteilen",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Anteilen",
    "text": "Visualisierung von Anteilen\nKreisdiagramme\n\n\n\n\nKreisdiagramme sind eine der häufigsten und umstrittensten Visualisierungen, um Anteile und Proportionen zu visualisieren\nNur bei einer geringen Anzahl von Kategorien (2-5) sinnvoll\nNicht sinnvoll, wenn die Anteile zwischen den Dimensionen sehr ähnlich sind\nAuch bei negativen Werten oder Measures, die sich nicht sinnvoll zu 100% summieren nicht geeignet"
  },
  {
    "objectID": "05_VL.html#visualisierung-von-anteilen-1",
    "href": "05_VL.html#visualisierung-von-anteilen-1",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Anteilen",
    "text": "Visualisierung von Anteilen\nTreemap\n\n\n\n\nBei hierarchischen Datenstrukturen, die in Kategorien und Subkategorien unterteilt sind, eignet sich eine Treemap\nTreemaps nutzen die Fläche der Rechtecke, um die Größe der Kategorien zu visualisieren und geben damit einen Eindruck über die relativen Anteile\nDurch die Nutzung der Fläche statt des Winkels sind Treemaps leichter lesbar\nTreemaps sind auch in der Lage, mehrere Dimensionen gleichzeitig darzustellen\nBei zu vielen Kategorien oder zu ähnlichen Anteilen ist von Treemaps abzuraten"
  },
  {
    "objectID": "05_VL.html#kurzer-überblick-wo-gehts-weiter",
    "href": "05_VL.html#kurzer-überblick-wo-gehts-weiter",
    "title": "Business Intelligence & Data Science",
    "section": "Kurzer Überblick: Wo geht’s weiter?",
    "text": "Kurzer Überblick: Wo geht’s weiter?\nInformationsgenerierung\n\n\n\nBIA Gesamtansatz. Eigene Darstellung in Anlehnung an Baars und Kemper (2021)."
  },
  {
    "objectID": "05_VL.html#berichtsorientierte-analyse",
    "href": "05_VL.html#berichtsorientierte-analyse",
    "title": "Business Intelligence & Data Science",
    "section": "Berichtsorientierte Analyse",
    "text": "Berichtsorientierte Analyse\nReporting\n\nEin Bericht oder Report gibt einen Überblick über betriebswirtschaftliche Sachverhalte eines abgegrenzten Verantwortungsbereichs\nIn der Regel durch Visualisierung von Zusammenhängen in grafischer Form\nBetriebliches Berichtswesen wird in interne und externe Berichterstattung unterteilt:\n\nInternes Berichtswesen: Informationen für das Management, bspw. internes Rechnungswesen\nExternes Berichtswesen: Informationen für externe Stakeholder, bspw. Jahresbericht"
  },
  {
    "objectID": "05_VL.html#berichtsorientierte-analyse-1",
    "href": "05_VL.html#berichtsorientierte-analyse-1",
    "title": "Business Intelligence & Data Science",
    "section": "Berichtsorientierte Analyse",
    "text": "Berichtsorientierte Analyse\nReporting\n\nAktive Berichtskomponenten:\n\nWerden nach einmaliger Spezifikation der Inhalte und Formate regelmäßig erstellt und aktiv versandt, entweder:\n\nPeriodisch: In festen Zeitabständen (jede Woche, jedes Quartal)\nAperiodisch: Bei Überschreitung bestimmter Grenzwerte (z.B. Umsatzgrenze)\n\n\nPassive Berichtskomponenten:\n\nWerden nur auf konkrete Anforderungen der Anwendenden erstellt\nIndividuelle und bedarfsspezifische Berichte\nAuch Ad-hoc Berichtskomponente genannt, oft mit OLAP und Self-Service Tools umgesetzt"
  },
  {
    "objectID": "05_VL.html#berichtsorientierte-analyse-2",
    "href": "05_VL.html#berichtsorientierte-analyse-2",
    "title": "Business Intelligence & Data Science",
    "section": "Berichtsorientierte Analyse",
    "text": "Berichtsorientierte Analyse\nOLAP\n\nOnline Analyitical Processing (OLAP) ermöglicht die Bereitstellung anwendungsfreundlicher und gleichermaßen flexibler Abfragen in multidimensionalen Datenräumen\nForm des Ad-hoc Reportings\nOLAP-Komponenten sind weitgehend mit Pivot Tabellen in Excel oder Google Sheets vergleichbar\nAber: Erweitert um ein zentrales Datenmodell (meist Data Mart basiert) und Rollenverwaltung"
  },
  {
    "objectID": "05_VL.html#olap-1",
    "href": "05_VL.html#olap-1",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nDatenmodell als Würfel\n\n\n\n\nBestehen konzeptionell aus Fakten (Measures), Dimensionen und Hierarchien\nDa oft mehrere Dimensionen vorliegen, spricht man von Cubes\nTheoretisch ist der Zahl an Dimensionen keine Grenzen gesetzt, in der Praxis aber im einstelligen Bereich begrenzt\nBei mehr als 3 Dimensionen spricht man oft von Hypercube\nBei der Erstellung von Reports aus OLAP Cubes spricht man oft von OLAP Operationen\n\n\n\n\n\n\nCube mit den Dimensionen Zeit, Produkt und Kunde. Quelle: Wikipedia"
  },
  {
    "objectID": "05_VL.html#olap-2",
    "href": "05_VL.html#olap-2",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nSlicing\n\nSlicing ist die Entnahme einer einzigen Dimension, indem eine ausgewählte Dimension auf einen Wert reduziert wird\nIm Beispiel wird der dreidimensionale Raum mit den Jahreswerten 2004–2006 auf das Jahr 2004 reduziert und so eine einzelne Scheibe aus dem Cube entnommen\n\n\n\n\n\nOLAP Slicing. Quelle: Wikipedia"
  },
  {
    "objectID": "05_VL.html#olap-3",
    "href": "05_VL.html#olap-3",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nDicing\n\nDicing ist die Einschränkung mehrerer Dimensionen auf ausgewählte Werte, sodass ein neuer, kleinerer Würfel entsteht\nIm Beispiel wird die Anzahl der Produktkategorien von fünf auf drei reduziert\n\n\n\n\n\nOLAP Dicing Quelle: Wikipedia"
  },
  {
    "objectID": "05_VL.html#olap-4",
    "href": "05_VL.html#olap-4",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nPivotierung\n\n\nAuswertung erfolgt meist auf zweidimensionalen Ausschnitten aus dem Cube, beispielsweise Produktkategorie pro Jahr\nGrafisch entspricht eine solche Ansicht einer Seite des Würfels\nDurch Pivotierung wird der Würfel um eine Achse gedreht\nIm Beispiel: Geografie pro Jahr statt Produktkategorie über Geografie\n\n\n\n\n\n\nOLAP Pivotierung/Rotation Quelle: Wikipedia"
  },
  {
    "objectID": "05_VL.html#olap-5",
    "href": "05_VL.html#olap-5",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nRoll-Up, Drill-Down & Drill-Through\n\nBeim Roll-Up werden die Werte einer Hierarchieebene auf die Werte einer übergeordneten Hierarchieebene aggregiert\nBeim Drill-Down wiederum wird ein aggregierter Wert in die einzelnen Bestandteile aufgeschlüsselt, bspw. die Betrachtung von einzelnen Monaten im Jahr 2004\nIn einigen Fällen wird beim Drilling die physikalische Datenquelle gewechselt, da beispielsweise nur eine begrenzte Granularitätsstufe im aktuellen Cube verfügbar ist\nDas geschieht im Normalfall ohne Wissen der Anwendenden, die die Abfrage stellen"
  },
  {
    "objectID": "05_VL.html#olap-6",
    "href": "05_VL.html#olap-6",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nPhysikalische Umsetzung und Anbindungsschnittstellen\n\nDie Datenhaltung erfolgt in OLAP Komponenten weitgehend unabhängig von der Anwendungsansicht meist in Client-Server-Architekturen und die Datenhaltung erfolgt serverseitig\nOLAP-Anwendungen erfordern zudem eine Programmoberfläche und oft erfolgt die Einbindung in Tabellenkalkulationsprogramm wie Excel\nEin bekanntes Beispiel ist SAP Analysis für Excel\nAndere gängige Praxis sind webbasierte Schnittstellen wie Cubeware Cockpit"
  },
  {
    "objectID": "05_VL.html#self-service-bi",
    "href": "05_VL.html#self-service-bi",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI",
    "text": "Self-Service BI\nDas Versprechen von Self-Service BI\n\nSelf Service BI setzt beim Versprechen an, die Anwendenden in die Lage zu versetzen eigenständig und flexibel den Datenbestand nach neuen Verknüpfungen zu untersuchen\nDas kann zudem weitgehend unabhängig von der Unternehmens-IT betrieben und bedient werden\nTraditionelle BI-Lösungen sind oft als zu starr angesehen, wenn es darum geht, neue Reporting Anforderungen rasch umzusetzen und damit innovative Analysen zu testen"
  },
  {
    "objectID": "05_VL.html#self-service-bi-vs.-traditionelle-bi",
    "href": "05_VL.html#self-service-bi-vs.-traditionelle-bi",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI vs. Traditionelle BI",
    "text": "Self-Service BI vs. Traditionelle BI\nBlick auf die alte Welt\n\n\n\nTraditionelle BI, Quelle: Fidelity"
  },
  {
    "objectID": "05_VL.html#self-service-bi-vs.-traditionelle-bi-1",
    "href": "05_VL.html#self-service-bi-vs.-traditionelle-bi-1",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI vs. Traditionelle BI",
    "text": "Self-Service BI vs. Traditionelle BI\nVersprechen der neuen Welt am Beispiel von Looker\n\n\n\nTraditionelle BI, Quelle: Fidelity"
  },
  {
    "objectID": "05_VL.html#der-data-science-prozess",
    "href": "05_VL.html#der-data-science-prozess",
    "title": "Business Intelligence & Data Science",
    "section": "Der Data Science Prozess",
    "text": "Der Data Science Prozess\nAblauf von Data Science Use Cases\n\n\n\n\nData Science Projekte sind in der Praxis aufwendig und erfordern stets die Zusammenarbeit zwischen Fachabteilungen und Data Science Team\nDeshalb wurden feste Prozesse etabliert, bspw. der Cross Reference Industry Standard Process for Data Mining (CRISP-DM)\nCRISP-DM besteht aus 6 Phasen, die iterativ durchlaufen werden\nRückkopplungen zwischen den Phasen sind dabei oft notwendig\n\n\n\n\n\n\nCRISP-DM Prozess. Eigene Darstellung in Anlehnung an Meier (2021)."
  },
  {
    "objectID": "05_VL.html#crisp-dm",
    "href": "05_VL.html#crisp-dm",
    "title": "Business Intelligence & Data Science",
    "section": "CRISP-DM",
    "text": "CRISP-DM\nGeschäftsmodell\n\nVerständnis des Geschäftsmodells\n\nVerständnis des Geschäftsmodells und der Unternehmensziele\nBerücksichtigung von Chancen, Risiken und zeitlichen Aspekten\nDefinition des erwartbaren Nutzens und messbarer Erfolgskriterien"
  },
  {
    "objectID": "05_VL.html#crisp-dm-1",
    "href": "05_VL.html#crisp-dm-1",
    "title": "Business Intelligence & Data Science",
    "section": "CRISP-DM",
    "text": "CRISP-DM\nAnwendungs-/Datendomäne und Datenvorbereitung\n\nVerständnis der Anwendungs- und Datendomäne\n\nAnalyse der Unternehmensprozesse und Datenquellen\nZusammenführung, Beschreibung und Exploration der Zieldaten\nEruierung der Datenqualität\n\nDatenvorbereitung\n\nZusammenführung und Beschreibung polystrukturierter Daten\nBerechnung von Kennzahlen und Durchführung von Datentransformationen"
  },
  {
    "objectID": "05_VL.html#crisp-dm-2",
    "href": "05_VL.html#crisp-dm-2",
    "title": "Business Intelligence & Data Science",
    "section": "CRISP-DM",
    "text": "CRISP-DM\nModellierung, Evaluation und Bereitstellung\n\nModellierung\n\nModellauswahl und -erstellung\nIterativer Prozess zur Weiterentwicklung von Modellen\nMöglicher Einsatz vortrainierter Modelle für die Verfeinerung\n\nEvaluation\n\nBewertung der erstellten Modelle anhand definierter Erfolgskriterien\n\nEinsatz\n\nUmsetzung der Ergebnisse in die Praxis und Integration in die Unternehmensprozesse"
  },
  {
    "objectID": "05_VL.html#begriffliche-grundlagen",
    "href": "05_VL.html#begriffliche-grundlagen",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nEine Gleichung, viele Anwendungen\n\n\nDie Generierung von modellbasierten Erkenntnissen aus Daten erfordert eine Definition des Lernproblems, oft mit einer simplen Gleichung:\n\n\n\\[\ny = f(X) + \\epsilon\n\\]\n\n\n\\(y\\) ist ein \\(N \\times 1\\) Vektor mit einer Ergebnisvariablen\n\\(X\\) ist eine \\(N \\times P\\) Matrix mit Prädiktoren \\(X_1, X_2,..., X_P\\)\n\\(N\\) entspricht der Anzahl von Beobachtungen im vorliegenden Datensatz, \\(P\\) ist die Anzahl der Prädiktoren\nAnstelle von Prädiktoren werden oft die Begriffe erklärende oder unabhängige Variablen oder Features verwendet\n\\(f\\) beschreibt alle systematischen Zusammenhänge zwischen \\(X\\) und \\(y\\), während der Fehlerterm \\(\\epsilon\\) alle Variation in \\(X\\) aufnimmt, die nicht von \\(f\\) erklärbar sind"
  },
  {
    "objectID": "05_VL.html#begriffliche-grundlagen-1",
    "href": "05_VL.html#begriffliche-grundlagen-1",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nVorhersage vs. Inferenz\n\nAngenommen, wir haben eine Funktion \\(f\\) sowie einen Algorithmus gefunden, um \\(f\\) an unsere Variablen \\(y\\) und \\(X\\) anzupassen\nDie Modellschätzung wird auch als Modelltraining oder Modellanpassung bezeichnet und unser trainiertes Modell hat die Form:\n\n\n\\[\n\\hat{y} = \\hat{f}(X)\n\\]\n\n\nDas trainierte Modell hat keinen Fehlerterm \\(\\epsilon\\), da dieser die Variation in den Daten darstellt, die vom Modell nicht erfasst wird und unvorhersehbar ist\nGeschätzte Größen werden mit einem Zirkumflex-Symbol (^) gekennzeichnet"
  },
  {
    "objectID": "05_VL.html#begriffliche-grundlagen-2",
    "href": "05_VL.html#begriffliche-grundlagen-2",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nVorhersage vs. Inferenz\n\nDas Modell kann nun für zwei Zwecke verwendet werden:\n\nVorhersage: Schätzung von \\(y\\) für neue, nicht im Modell enthaltene Daten\nInferenz: Verständnis der Beziehungen und Muster in den Daten, die das Modell gelernt hat\n\nInferenz erfordert ein genaues Verständnis der Zusammenhänge\nBei Vorhersage kümmern wir uns nicht um seine genaue Struktur oder Parameter des Modells, sondern ausschließlich um die generierten Vorhersagen."
  },
  {
    "objectID": "05_VL.html#begriffliche-grundlagen-3",
    "href": "05_VL.html#begriffliche-grundlagen-3",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nKlassifikation vs. Regression\n\nDie Art der Variablen \\(y\\) bestimmt die Art des Modells, das wir verwenden\nVariablen sind entweder kategorisch oder numerisch, oft auch als qualitativ und quantitativ bezeichnet\nWenn die abhängige Variable numerisch ist, ist es möglich, den exakten Wert der Variable vorherzusagen, dann liegt ein Regressionsproblem vor\nWenn die abhängige Variable kategorisch ist, lässt sich lediglich die erwartete Klasse vorhersagen, basierend auf einer Wahrscheinlichkeit, dann liegt ein Klassifikationsproblem vor\nWichtig: Hierbei ist nur die Art der abhängigen Variable entscheidend"
  },
  {
    "objectID": "05_VL.html#begriffliche-grundlagen-4",
    "href": "05_VL.html#begriffliche-grundlagen-4",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nKategoriale Daten\n\nKategoriale Daten umfassen verschiedene Kategorien oder Labels\nBei kategorialen Daten wird wiederum zwischen nominalen und ordinalen Variablen unterschieden\nNominaldaten repräsentieren Kategorien ohne eine inhärente Rangfolge:\n\nFarben (rot, blau, grün),\nFamilienstand (ledig, verheiratet, geschieden, …),\n\nOrdinaldaten weisen eine spezifische Reihenfolge oder Hierarchie auf, aber keine gleichmäßigen Abstände zwischen den Kategorien:\n\nSchärfe von Essen (mild, pikant, scharf),\nExpertise mit Programmiersprachen (keine, wenig, fortgeschritten, professionell)"
  },
  {
    "objectID": "05_VL.html#begriffliche-grundlagen-5",
    "href": "05_VL.html#begriffliche-grundlagen-5",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nNumerische Daten\n\n\n\nNumerische Daten reräsentieren Mengen, Messungen, und allgemein numerische Werte\nLassen sich mit mathematichen Operationen verarbeiten und manipulieren\nNumerische Daten können entweder diskret oder kontinuierlich sein, Beispiele:\n\nAlter,\nTemperatur,\nFinanzwerte ($ oder €)\n\n\n\n\n\n\n\nStetig vs. Diskret, Quelle: Allison Horst"
  },
  {
    "objectID": "05_VL.html#begriffliche-grundlagen-6",
    "href": "05_VL.html#begriffliche-grundlagen-6",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nÜberwachtes vs. Unüberwachtes Lernen\n\nÜberwachte Lernprobleme weisen eine mess- oder beobachtbare abhängige Variable \\(y_i, i = 1,2,...N\\) für jede Beobachtung \\(i\\) auf sowie eine oder mehrere Prädiktoren\nUnüberwachte Lernprobleme hingegen haben keine abhängige Variable, sondern versuchen, Muster in den Daten zu finden\nDie Mehrheit der Data Science Projekte sind überwacht, da sie auf der Vorhersage von \\(y\\) basieren\nAuch wir konzentrieren uns bei der Klassifikation auf überwachte Lernprobleme"
  },
  {
    "objectID": "05_VL.html#quellen",
    "href": "05_VL.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nBaars, Henning, und Hans-Georg Kemper. 2021. Business Intelligence & Analytics: Grundlagen und praktische Anwendungen: Ansätze der IT-basierten Entscheidungsunterstützung. 4., überarbeitete und erweiterte Auflage. Lehrbuch. Wiesbaden [Heidelberg]: Springer Vieweg.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, und Robert Tibshirani. 2021. An Introduction to Statistical Learning: with Applications in R. Second edition. Springer texts in statistics. New York NY: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nMeier, Andreas. 2021. „Rundgang Big Data Analytics – Hard & Soft Data Mining“. In Big Data Analytics, herausgegeben von Sara D’Onofrio und Andreas Meier, 3–23. Wiesbaden: Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-32236-6_1."
  },
  {
    "objectID": "08_VL.html#der-plan-für-heute",
    "href": "08_VL.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung\n\nModellgestützte Analysen\n\nNearest Neighbor Klassifikation\nEntscheidungsbäume"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation",
    "href": "08_VL.html#nearest-neighbor-klassifikation",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nGrundidee\n\nNearest Neighbor Klassifikation oder meist K-Nearest Neighbor (KNN) basiert auf der Idee, dass ähnliche Datenpunkte zu ähnlichen Klassen gehören\nKNN macht keine Annahmen über die Verteilung der Daten und die For der Entscheidungsgrenze\nKNN ist ein sogenanntes “lazy learning” Verfahren, da es keine explizite Modellbildung durchführt\nDie Klassifikation erfolgt durch die Mehrheitsentscheidung der \\(K\\) nächsten Nachbarn in den Testdaten"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-1",
    "href": "08_VL.html#nearest-neighbor-klassifikation-1",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nGrundidee\n\n\nAuch bei KNN wird die Wahrscheinlichkeit ermittelt, mit der Element \\(x_0\\) zur Klasse \\(j\\) gehört\n\n\n\\[\nP(Y=j|X= x_0) = \\frac{1}{K} \\sum_{i \\in \\aleph_0} I(y_i=j)\n\\]\n\n\nDabei repräsentiert \\(\\aleph_0\\) die Menge der \\(K\\) nächsten Nachbarn von \\(x_0\\) und \\(I(y_i=j)\\) ist eine Indikatorfunktion, die 1 zurückgibt, wenn \\(y_i=j\\) und 0 sonst\nDie Zuordnung erfolgt dann durch die Klasse mit der höchsten Wahrscheinlichkeit im Sinne einer Mehrheitsentscheidung\nBeispiel: Wenn \\(K = 3\\), wird die Klasse bestimmt, zu der die Mehrheit der drei nächsten Nachbarn gehört und die Wahrscheinlichkeit entspricht dem Anteil der Nachbarn in dieser Klasse"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-2",
    "href": "08_VL.html#nearest-neighbor-klassifikation-2",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nBestimmung der Nachbarn\n\nDie Bestimmung der nächsten Nachbarn erfolgt durch sogenannte Ähnlichkeits- oder Distanzmaße\nEs gibt eine Vielzahl von Distanzmaßen, die sich in der Berechnung und Interpretation unterscheiden\nDie einfachste Form ist die euklidische Distanz, in allgemeiner Form:\n\n\n\\[\nd(p,q) = \\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2  + ... + (p_n-q_n)^2}\n\\]"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-3",
    "href": "08_VL.html#nearest-neighbor-klassifikation-3",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nBestimmung der Nachbarn\n\nIn zwei Dimensionen vereinfacht sich diese Gleichung zu:\n\n\n\n\\[\nd(p,q) = \\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}\n\\]"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-4",
    "href": "08_VL.html#nearest-neighbor-klassifikation-4",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nBestimmung der Nachbarn\n\n\nIn zwei Dimensionen vereinfacht sich diese Gleichung zu:\n\n\n\n\\[\nd(p,q) = \\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}\n\\]"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-5",
    "href": "08_VL.html#nearest-neighbor-klassifikation-5",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nSchritte und Intuition\n\n\nBestimme die Trainingsdaten durch einen Split, bspw. 70% Trainingsdaten, lege \\(K\\) fest"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-6",
    "href": "08_VL.html#nearest-neighbor-klassifikation-6",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nSchritte und Intuition\n\n\nFür den neuen Datenpunkt Doja Cat, berechne die euklidische Distanz zu allen anderen Punkten im Trainingsdatensatz"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-7",
    "href": "08_VL.html#nearest-neighbor-klassifikation-7",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nSchritte und Intuition\n\n\nFür den neuen Datenpunkt Doja Cat, berechne die euklidische Distanz zu allen anderen Punkten im Trainingsdatensatz"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-8",
    "href": "08_VL.html#nearest-neighbor-klassifikation-8",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nSchritte und Intuition\n\n\nIdentifiziere die \\(K\\) nächsten Nachbarn und ordne den neuen Punkt der Klasse zu, in der die Mehrheit der Nachbarn liegt\n\n\n\n\n\n\n\n\ncategory\ntrack.artist\nenergy\ndanceability\nDistanz zu Doja Cat\nRang\n\n\n\n\nHip-Hop\nKAROL G\n0.7\n0.8\n0.10\n1\n\n\nHip-Hop\nJamal\n0.6\n0.7\n0.10\n2\n\n\nEDM\nDeetron\n0.7\n0.6\n0.22\n3\n\n\nHip-Hop\nFugees\n0.4\n0.6\n0.28\n4\n\n\nEDM\nVendex\n0.8\n0.6\n0.28\n5\n\n\nHip-Hop\nSira\n0.3\n0.9\n0.32\n6\n\n\nEDM\nKaori\n0.9\n0.6\n0.36\n7\n\n\nEDM\nGusted\n1.0\n0.7\n0.41\n8\n\n\nEDM\nKronos\n1.0\n0.6\n0.45\n9\n\n\nEDM\nSpectre\n1.0\n0.5\n0.50\n10\n\n\n\n\n\n\n\n\n\nWie ist die Vorhersage bei und die zugehörige Wahrscheinlichkeit bei:\n\nK = 1\nK = 5\nK = 10"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-9",
    "href": "08_VL.html#nearest-neighbor-klassifikation-9",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nSchritte und Intuition\n\nFür die ausgewählten \\(K\\) erhalten wir das folgende Ergebnis:\n\n\n\n\n\n\n\nVorhersage\nWahrscheinlichkeit\nK\n\n\n\n\nHip-Hop\n1.0\n1\n\n\nHip-Hop\n0.6\n5\n\n\nEDM\n0.6\n10\n\n\n\n\n\n\n\n\n\nAnschließend wird der Prozess für alle anderen neuen Beobachtungen wiederholt"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-10",
    "href": "08_VL.html#nearest-neighbor-klassifikation-10",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nModellevaluation\n\n\n\nDie Modellevaluation erfolgt anschließend analog zur logistischen Regression mit den bisher dargestellten Metriken Accuracy, Precision, Recall und F1-Score\n\\(K\\) ist ein wichtiger Hyperparameter, der sorgfältig gewählt werden sollte"
  },
  {
    "objectID": "08_VL.html#nearest-neighbor-klassifikation-11",
    "href": "08_VL.html#nearest-neighbor-klassifikation-11",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nModellevaluation\n\n\n\nOffenbar maximiert \\(K = 60\\) sämtliche Metriken\nMit \\(K\\) = 60 erhalten wir die Konfusionsmatrix rechts sowie die Metriken unten:\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nmulticlass\n0.85\n\n\nrecall\nmacro\n0.85\n\n\nprecision\nmacro\n0.86\n\n\nf_meas\nmacro\n0.85"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume",
    "href": "08_VL.html#entscheidungsbäume",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nGrundidee\n\nEntscheidungsbäume sind ein weiteres Verfahren des überwachten Lernens, das sich sowohl für Klassifikations- als auch für Regressionsprobleme eignet\nDie Klassifikation mit Entscheidungsbäumen umfasst grob zwei Schritte:\n\n\nAufteilung der erklärenden Variablen \\(X_1\\), \\(X_2\\), …, \\(X_p\\) in \\(K\\) immer kleinere, nicht überlappende Regionen/Untergruppen (Blätter) \\(R_1\\), \\(R_2\\), …, \\(R_J\\) anhand von Entscheidungsregeln (Knoten)\nKlassifikation der Testdaten anhand der Entscheidungsregeln. Alle Beobachtungen, die in Region \\(R_k\\) fallen, werden der Klasse \\(k\\) zugeordnet"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-1",
    "href": "08_VL.html#entscheidungsbäume-1",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\nStart ist der gesamte Datensatz, auch Root-Node oder Wurzelknoten\nGesucht wird die beste Variable und der beste Schwellenwert, um den Datensatz in zwei Gruppen zu teilen, auch Entscheidungsregel oder Split\nZur Ermittlung der besten Entscheidungsregel wird ein Maß für die Homogenität der Regionen verwendet, bspw. der Gini-Index\n\n\n\\[\nG = \\sum_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk})\n\\]\n\n\nwobei \\(\\hat{p}_{mk}\\) der Anteil der Beobachtungen der Klasse \\(k\\) in Region \\(m\\) ist.\nWenn die Region \\(m\\) homogen ist, ist der Gini-Index 0, eine hälftige Aufteilung ergibt bei zwei Klassen den Wert 0.5"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-2",
    "href": "08_VL.html#entscheidungsbäume-2",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\nAnschließend wird mit dem Gini-Index der Informationsgewinn \\(IG\\) berechnet, der sich aus der Differenz des Gini-Index vor und nach dem Split ergibt:\n\n\n\\[\n\\text{IG} = G_{\\text{init}} - G_{\\text{split}}\n\\]\n\n\nwobei \\(G_{\\text{init}}\\) der Gini-Index vor dem Split und \\(G_{\\text{split}}\\) der Gini-Index nach dem Split ist\n\\(G_{\\text{split}}\\) ist die gewichtete Summe der Gini-Indizes der Regionen, die durch den Split entstehen"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-3",
    "href": "08_VL.html#entscheidungsbäume-3",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\n\n\nWir starten mit dem gesamten Trainings-Datensatz für die drei Genres Klassik, EDM und Hip-Hop\nZunächst berechnen wir den Gini-Index für die Root-Node:\n\n\n\n\n\n\n\ncategory\np\n1-p\nGini\n\n\n\n\nEDM\n0.302\n0.698\n0.211\n\n\nHip-Hop\n0.363\n0.637\n0.231\n\n\nKlassik\n0.334\n0.666\n0.223\n\n\n\n\n\n\n\n\n\n\\[\nG_{\\text{init}} = 0.211 + 0.231 + 0.223 = 0.665\n\\]\n\n\nAnschließend wird der beste Split für die Root-Node ermittelt, indem der Informationsgewinn für alle möglichen Splits berechnet wird"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-4",
    "href": "08_VL.html#entscheidungsbäume-4",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\n\n\nFür den Beispiel-Split rechts erhalten wir links von der gestrichelten Linie\n\n\n\n\n\n\n\ncategory\nn\nGini\n\n\n\n\nEDM\n24\n0.016\n\n\nHip-Hop\n196\n0.115\n\n\nKlassik\n1257\n0.127\n\n\nTotal\n1477\n0.258\n\n\n\n\n\n\n\n\n\nund rechts davon:\n\n\n\n\n\n\n\ncategory\nn\nGini\n\n\n\n\nEDM\n1118\n0.250\n\n\nHip-Hop\n1176\n0.250\n\n\nKlassik\n7\n0.003\n\n\nTotal\n2301\n0.503"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-5",
    "href": "08_VL.html#entscheidungsbäume-5",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\n\n\nDer Information Gain beträgt für den Split bei 0.5:\n\n\n\\[\nIG = G_{\\text{init}} - G_{\\text{split}} = \\\\0.665 - \\left( \\frac{1477}{3780 } \\cdot 0.258 + \\frac{2301}{3780 } \\cdot 0.503 \\right) = \\\\0.258\n\\]\n\n\nFür alle Variablen und Splits wird nun der Split mit dem höchsten Informationsgewinn ermittelt"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-6",
    "href": "08_VL.html#entscheidungsbäume-6",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\n\nDer Split bei 0.5 ist allerdings nicht ideal\nDie Grafik rechts illustriert den Informationsgewinn für alle möglichen Splits der Variablen Energy und Danceability\nDer maximale Informationsgewinn wird für den Split bei ca. 0.37 für die Variable Energy erzielt"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-7",
    "href": "08_VL.html#entscheidungsbäume-7",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\n\n\nAnschließend wird dieser Vorgang für die neu entstandenen Regionen wiederholt, bis kein zusätzlicher Informationsgewinn mehr erzielt wird\nDie Regionen werden dann als Blätter bezeichnet und die Klassifikation erfolgt anhand der Mehrheit der Beobachtungen in der Region\nDie Grafik rechts zeigt den finalen Entscheidungsbaum für die Genres Klassik, EDM und Hip-Hop mit den beiden Variablen Energy und Danceability\nDie Hintergrundfarbe repräsentiert die Klassifikation, die Farbe der Punkte die Klasse der Trainingsdaten"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-8",
    "href": "08_VL.html#entscheidungsbäume-8",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\nEine noch intuitivere Darstellung des Entscheidungsbaums ist die Visualisierung der Entscheidungsregeln\nHierbei wird ein Pfad durch den Baum verfolgt und die Entscheidungsregeln für die Klassifikation dargestellt\nDie Grafik rechts zeigt die Entscheidungsregeln für die Genres Klassik, EDM und Hip-Hop illustriert denselben Sachverhalt wie die Partitionsgrafik"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-9",
    "href": "08_VL.html#entscheidungsbäume-9",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\nRoot-Node\n\n\nDer Split umfasst die Anteile der Klassen in der Root-Node\nDer Datensat umfasst 30% EDM Songs, 36% Hip-Hop Songs und 33% Klassik Songs\nDa die Mehrheit der Songs aus dem Genre Hip-Hop stammen, ist die Root-Node Hip-Hop grau"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-10",
    "href": "08_VL.html#entscheidungsbäume-10",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\n\nEntscheidungsregel 1:\n\nUnter den Nodes wird die Entscheidungsregel dargestellt\nIn unserem Fall wird der Datensatz anhand der Variable Energy \\(\\geq\\) 0.37 in zwei Regionen geteilt\nDer Pfad links entspricht der Region, in der die Bedingung erfüllt ist, rechts der Region, in der die Bedingung nicht erfüllt ist\nBeide Wege führen zu einer weiteren Entscheidungsregel"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-11",
    "href": "08_VL.html#entscheidungsbäume-11",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\n\nEntscheidungsregel 2 (links):\n\nHier befinden sich 67% der Daten\nDiese setzen sich zusammen aus:\n\n45% EDM, 53% Hip-Hop und 2% Klassik\n\nDie Entscheidungsregel ist nun Energy \\(\\geq\\) 0.82\nWenn die Bedingung erfüllt ist, wird der Song dem Genre EDM zugeordnet\nWenn die Bedingung nicht erfüllt ist, wird der Song dem Genre Hip-Hop zugeordnet"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-12",
    "href": "08_VL.html#entscheidungsbäume-12",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\n\nEntscheidungsregel 3 (rechts):\n\nHier befinden sich 33% der Daten\nDiese setzen sich zusammen aus:\n\n0% EDM, 2% Hip-Hop und 98% Klassik\n\nDie Entscheidungsregel ist nun Danceability \\(\\geq\\) 0.67\nWenn die Bedingung erfüllt ist, wird der Song dem Genre Hip-Hop zugeordnet\nWenn die Bedingung nicht erfüllt ist, wird der Song dem Genre Klassik zugeordnet"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-13",
    "href": "08_VL.html#entscheidungsbäume-13",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\n\nDie untersten Nodes sind die sogenannten Blätter des Entscheidungsbaums\nJedes Blatt entspricht einer Region, in der die Klassifikation erfolgt\nDie Angabe unterhalb der Klassifikation ergibt die Wahrscheinlichkeit, dass ein Song in das jeweilige Genre fällt\nDie Wahrscheinlichkeit entspricht dabei dem Anteil der Songs in der Region, die den jeweiligen Genres zugeordnet sind\nDie Variable Danceability spielt für die Klassifikation offenbar keine Rolle"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-14",
    "href": "08_VL.html#entscheidungsbäume-14",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nModellevaluation\n\n\n\nDie Modellevaluation erfolgt analog zur logistischen Regression und Nearest Neighbor Klassifikation\nDie Konfusionsmatrix rechts führt zu diesen Metriken auf Basis der Testdaten:\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nmulticlass\n0.83\n\n\nrecall\nmacro\n0.83\n\n\nprecision\nmacro\n0.85\n\n\nf_meas\nmacro\n0.83"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-15",
    "href": "08_VL.html#entscheidungsbäume-15",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nOverfitting\n\n\nZiel beim Machine Learning ist eine Generalisierung auf bisher unbekannte Daten\nDas bisherige Modell hat nach 2 Nodes aufgehört, aber warum?\nDies erfolgt aufgrund sogenannter Stopping Rules oder Hyperparameter\nKomplexitätskosten:\n\nEin zusätzlicher Split verbessert die Klassifikation auf den Trainingsdaten, führt aber zu erhöhter Komplexität des Entscheidungsbaums\nDie Komplexitätskosten begrenzen weitere Splits und sorgen dafür, dass nur dann weiter aufgeteilt wird, wenn ein Minimum an zusätzlicher Verbesserung des Modells gewährleistet wird\n\nMaximale Tiefe:\n\nBegrenzung der Verzweigung verhindert eine granulare Einteilung der Daten\n\n\nMinimum an Beobachtungen pro Blatt:\n\nVerhindert, dass nur wenige Beobachtungen in den finalen Blättern sind"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-16",
    "href": "08_VL.html#entscheidungsbäume-16",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nOverfitting\n\n\n\n\nGenerell gilt: Je komplexer der Entscheidungsbaum, desto besser ist die Anpassung auf den Trainingsdaten\nAber: Je perfekter die Anpassung auf den Trainingsdaten, desto schlechter die Generalisierung auf unbekannte Daten\nDie Grafik rechts zeigt ein extremes Beispiel, bei dem der Entscheidungsbaum mit 5 Nodes ausgebaut wurde"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-17",
    "href": "08_VL.html#entscheidungsbäume-17",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nOverfitting\n\n\n\n\nIm Rahmen des Hyperparameter-Tunings wird die Komplexität des Entscheidungsbaums optimiert\nDie Grafik rechts zeigt die Anpassung des Entscheidungsbaums an die Trainingsdaten und die Generalisierung auf die Testdaten für verschiedene Tiefen des Baums\nAls Metrik dient der F1-Score\nOffenbar steigt der F1-Score auf den Trainingsdaten mit der Tiefe des Baums, während es auf den Testdaten ein Optimum bei 6 Nodes gibt"
  },
  {
    "objectID": "08_VL.html#entscheidungsbäume-18",
    "href": "08_VL.html#entscheidungsbäume-18",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nErweiterungen\n\nEntscheidungsbäume sind sehr intuitiv und einfach zu interpretieren\nEntscheidungsbäume erfordern nur wenig Datenbereinigung und können mit numerischen und kategorialen Variablen umgehen\nAllerdings sind Entscheidungsbäume anfällig für Overfitting\nEntscheidungsbäume sind Grundlage für viele weitere Modelle:\n\nRandom Forests kombinieren mehrere Entscheidungsbäume, um die Vorhersagegenauigkeit zu verbessern und das Overfitting zu reduzieren\nBoosted Trees kombinieren sequentielle Entscheidungsbäume, die auf den Fehlern des vorherigen Baums aufbauen"
  },
  {
    "objectID": "08_VL.html#modellvergleich",
    "href": "08_VL.html#modellvergleich",
    "title": "Business Intelligence & Data Science",
    "section": "Modellvergleich",
    "text": "Modellvergleich\n…and the Winner is?\n\nDie Modelle mit 2 erklärenden Variablen dienen primär der Illustration der Modellintuition und sind daher nicht unbedingt praxistauglich\nWelches Modell performt am besten auf den Testdaten?\n\n\n\n\n\n\n\n\n.metric\nLogistische Regression\n60-Nearest Neighbor\nEntscheidungsbaum 6 Nodes\n\n\n\n\naccuracy\n0.8478\n0.8521\n0.8472\n\n\nrecall\n0.8454\n0.8458\n0.8435\n\n\nprecision\n0.8479\n0.8575\n0.8488\n\n\nf_meas\n0.8464\n0.8481\n0.8453\n\n\n\n\n\n\n\n\n\n\nWenn auch knapp hat das Nearest Neighbor Modell mit \\(K = 60\\) die Nase vorn\n\n\n\nBusiness Intelligence & Data Science, SoSe 2024"
  },
  {
    "objectID": "08_VL_D.html#der-plan-für-heute",
    "href": "08_VL_D.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung\n\nModellgestützte Analysen\n\nNearest Neighbor Klassifikation\nEntscheidungsbäume"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nGrundidee\n\nNearest Neighbor Klassifikation oder meist K-Nearest Neighbor (KNN) basiert auf der Idee, dass ähnliche Datenpunkte zu ähnlichen Klassen gehören\nKNN macht keine Annahmen über die Verteilung der Daten und die For der Entscheidungsgrenze\nKNN ist ein sogenanntes “lazy learning” Verfahren, da es keine explizite Modellbildung durchführt\nDie Klassifikation erfolgt durch die Mehrheitsentscheidung der \\(K\\) nächsten Nachbarn in den Testdaten"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-1",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-1",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nGrundidee\n\n\nAuch bei KNN wird die Wahrscheinlichkeit ermittelt, mit der Element \\(x_0\\) zur Klasse \\(j\\) gehört\n\n\n\\[\nP(Y=j|X= x_0) = \\frac{1}{K} \\sum_{i \\in \\aleph_0} I(y_i=j)\n\\]\n\n\nDabei repräsentiert \\(\\aleph_0\\) die Menge der \\(K\\) nächsten Nachbarn von \\(x_0\\) und \\(I(y_i=j)\\) ist eine Indikatorfunktion, die 1 zurückgibt, wenn \\(y_i=j\\) und 0 sonst\nDie Zuordnung erfolgt dann durch die Klasse mit der höchsten Wahrscheinlichkeit im Sinne einer Mehrheitsentscheidung\nBeispiel: Wenn \\(K = 3\\), wird die Klasse bestimmt, zu der die Mehrheit der drei nächsten Nachbarn gehört und die Wahrscheinlichkeit entspricht dem Anteil der Nachbarn in dieser Klasse"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-2",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-2",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nBestimmung der Nachbarn\n\nDie Bestimmung der nächsten Nachbarn erfolgt durch sogenannte Ähnlichkeits- oder Distanzmaße\nEs gibt eine Vielzahl von Distanzmaßen, die sich in der Berechnung und Interpretation unterscheiden\nDie einfachste Form ist die euklidische Distanz, in allgemeiner Form:\n\n\n\\[\nd(p,q) = \\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2  + ... + (p_n-q_n)^2}\n\\]"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-3",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-3",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nBestimmung der Nachbarn\n\nIn zwei Dimensionen vereinfacht sich diese Gleichung zu:\n\n\n\n\\[\nd(p,q) = \\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}\n\\]"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-4",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-4",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nBestimmung der Nachbarn\n\n\nIn zwei Dimensionen vereinfacht sich diese Gleichung zu:\n\n\n\n\\[\nd(p,q) = \\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}\n\\]"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-5",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-5",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nSchritte und Intuition\n\n\nBestimme die Trainingsdaten durch einen Split, bspw. 70% Trainingsdaten, lege \\(K\\) fest"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-6",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-6",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nSchritte und Intuition\n\n\nFür den neuen Datenpunkt Doja Cat, berechne die euklidische Distanz zu allen anderen Punkten im Trainingsdatensatz"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-7",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-7",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nSchritte und Intuition\n\n\nFür den neuen Datenpunkt Doja Cat, berechne die euklidische Distanz zu allen anderen Punkten im Trainingsdatensatz"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-8",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-8",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nSchritte und Intuition\n\n\nIdentifiziere die \\(K\\) nächsten Nachbarn und ordne den neuen Punkt der Klasse zu, in der die Mehrheit der Nachbarn liegt\n\n\n\n\n\n\n\n\ncategory\ntrack.artist\nenergy\ndanceability\nDistanz zu Doja Cat\nRang\n\n\n\n\nHip-Hop\nKAROL G\n0.7\n0.8\n0.10\n1\n\n\nHip-Hop\nJamal\n0.6\n0.7\n0.10\n2\n\n\nEDM\nDeetron\n0.7\n0.6\n0.22\n3\n\n\nHip-Hop\nFugees\n0.4\n0.6\n0.28\n4\n\n\nEDM\nVendex\n0.8\n0.6\n0.28\n5\n\n\nHip-Hop\nSira\n0.3\n0.9\n0.32\n6\n\n\nEDM\nKaori\n0.9\n0.6\n0.36\n7\n\n\nEDM\nGusted\n1.0\n0.7\n0.41\n8\n\n\nEDM\nKronos\n1.0\n0.6\n0.45\n9\n\n\nEDM\nSpectre\n1.0\n0.5\n0.50\n10\n\n\n\n\n\n\n\n\n\nWie ist die Vorhersage bei und die zugehörige Wahrscheinlichkeit bei:\n\nK = 1\nK = 5\nK = 10"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-9",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-9",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nSchritte und Intuition\n\nFür die ausgewählten \\(K\\) erhalten wir das folgende Ergebnis:\n\n\n\n\n\n\n\nVorhersage\nWahrscheinlichkeit\nK\n\n\n\n\nHip-Hop\n1.0\n1\n\n\nHip-Hop\n0.6\n5\n\n\nEDM\n0.6\n10\n\n\n\n\n\n\n\n\n\nAnschließend wird der Prozess für alle anderen neuen Beobachtungen wiederholt"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-10",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-10",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nModellevaluation\n\n\n\nDie Modellevaluation erfolgt anschließend analog zur logistischen Regression mit den bisher dargestellten Metriken Accuracy, Precision, Recall und F1-Score\n\\(K\\) ist ein wichtiger Hyperparameter, der sorgfältig gewählt werden sollte"
  },
  {
    "objectID": "08_VL_D.html#nearest-neighbor-klassifikation-11",
    "href": "08_VL_D.html#nearest-neighbor-klassifikation-11",
    "title": "Business Intelligence & Data Science",
    "section": "Nearest Neighbor Klassifikation",
    "text": "Nearest Neighbor Klassifikation\nModellevaluation\n\n\n\nOffenbar maximiert \\(K = 60\\) sämtliche Metriken\nMit \\(K\\) = 60 erhalten wir die Konfusionsmatrix rechts sowie die Metriken unten:\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nmulticlass\n0.85\n\n\nrecall\nmacro\n0.85\n\n\nprecision\nmacro\n0.86\n\n\nf_meas\nmacro\n0.85"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume",
    "href": "08_VL_D.html#entscheidungsbäume",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nGrundidee\n\nEntscheidungsbäume sind ein weiteres Verfahren des überwachten Lernens, das sich sowohl für Klassifikations- als auch für Regressionsprobleme eignet\nDie Klassifikation mit Entscheidungsbäumen umfasst grob zwei Schritte:\n\n\nAufteilung der erklärenden Variablen \\(X_1\\), \\(X_2\\), …, \\(X_p\\) in \\(K\\) immer kleinere, nicht überlappende Regionen/Untergruppen (Blätter) \\(R_1\\), \\(R_2\\), …, \\(R_J\\) anhand von Entscheidungsregeln (Knoten)\nKlassifikation der Testdaten anhand der Entscheidungsregeln. Alle Beobachtungen, die in Region \\(R_k\\) fallen, werden der Klasse \\(k\\) zugeordnet"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-1",
    "href": "08_VL_D.html#entscheidungsbäume-1",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\nStart ist der gesamte Datensatz, auch Root-Node oder Wurzelknoten\nGesucht wird die beste Variable und der beste Schwellenwert, um den Datensatz in zwei Gruppen zu teilen, auch Entscheidungsregel oder Split\nZur Ermittlung der besten Entscheidungsregel wird ein Maß für die Homogenität der Regionen verwendet, bspw. der Gini-Index\n\n\n\\[\nG = \\sum_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk})\n\\]\n\n\nwobei \\(\\hat{p}_{mk}\\) der Anteil der Beobachtungen der Klasse \\(k\\) in Region \\(m\\) ist.\nWenn die Region \\(m\\) homogen ist, ist der Gini-Index 0, eine hälftige Aufteilung ergibt bei zwei Klassen den Wert 0.5"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-2",
    "href": "08_VL_D.html#entscheidungsbäume-2",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\nAnschließend wird mit dem Gini-Index der Informationsgewinn \\(IG\\) berechnet, der sich aus der Differenz des Gini-Index vor und nach dem Split ergibt:\n\n\n\\[\n\\text{IG} = G_{\\text{init}} - G_{\\text{split}}\n\\]\n\n\nwobei \\(G_{\\text{init}}\\) der Gini-Index vor dem Split und \\(G_{\\text{split}}\\) der Gini-Index nach dem Split ist\n\\(G_{\\text{split}}\\) ist die gewichtete Summe der Gini-Indizes der Regionen, die durch den Split entstehen"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-3",
    "href": "08_VL_D.html#entscheidungsbäume-3",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\n\n\nWir starten mit dem gesamten Trainings-Datensatz für die drei Genres Klassik, EDM und Hip-Hop\nZunächst berechnen wir den Gini-Index für die Root-Node:\n\n\n\n\n\n\n\ncategory\np\n1-p\nGini\n\n\n\n\nEDM\n0.302\n0.698\n0.211\n\n\nHip-Hop\n0.363\n0.637\n0.231\n\n\nKlassik\n0.334\n0.666\n0.223\n\n\n\n\n\n\n\n\n\n\\[\nG_{\\text{init}} = 0.211 + 0.231 + 0.223 = 0.665\n\\]\n\n\nAnschließend wird der beste Split für die Root-Node ermittelt, indem der Informationsgewinn für alle möglichen Splits berechnet wird"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-4",
    "href": "08_VL_D.html#entscheidungsbäume-4",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\n\n\nFür den Beispiel-Split rechts erhalten wir links von der gestrichelten Linie\n\n\n\n\n\n\n\ncategory\nn\nGini\n\n\n\n\nEDM\n24\n0.016\n\n\nHip-Hop\n196\n0.115\n\n\nKlassik\n1257\n0.127\n\n\nTotal\n1477\n0.258\n\n\n\n\n\n\n\n\n\nund rechts davon:\n\n\n\n\n\n\n\ncategory\nn\nGini\n\n\n\n\nEDM\n1118\n0.250\n\n\nHip-Hop\n1176\n0.250\n\n\nKlassik\n7\n0.003\n\n\nTotal\n2301\n0.503"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-5",
    "href": "08_VL_D.html#entscheidungsbäume-5",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\n\n\nDer Information Gain beträgt für den Split bei 0.5:\n\n\n\\[\nIG = G_{\\text{init}} - G_{\\text{split}} = \\\\0.665 - \\left( \\frac{1477}{3780 } \\cdot 0.258 + \\frac{2301}{3780 } \\cdot 0.503 \\right) = \\\\0.258\n\\]\n\n\nFür alle Variablen und Splits wird nun der Split mit dem höchsten Informationsgewinn ermittelt"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-6",
    "href": "08_VL_D.html#entscheidungsbäume-6",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\n\nDer Split bei 0.5 ist allerdings nicht ideal\nDie Grafik rechts illustriert den Informationsgewinn für alle möglichen Splits der Variablen Energy und Danceability\nDer maximale Informationsgewinn wird für den Split bei ca. 0.37 für die Variable Energy erzielt"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-7",
    "href": "08_VL_D.html#entscheidungsbäume-7",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nSchritte und Intuition\n\n\n\n\nAnschließend wird dieser Vorgang für die neu entstandenen Regionen wiederholt, bis kein zusätzlicher Informationsgewinn mehr erzielt wird\nDie Regionen werden dann als Blätter bezeichnet und die Klassifikation erfolgt anhand der Mehrheit der Beobachtungen in der Region\nDie Grafik rechts zeigt den finalen Entscheidungsbaum für die Genres Klassik, EDM und Hip-Hop mit den beiden Variablen Energy und Danceability\nDie Hintergrundfarbe repräsentiert die Klassifikation, die Farbe der Punkte die Klasse der Trainingsdaten"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-8",
    "href": "08_VL_D.html#entscheidungsbäume-8",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\nEine noch intuitivere Darstellung des Entscheidungsbaums ist die Visualisierung der Entscheidungsregeln\nHierbei wird ein Pfad durch den Baum verfolgt und die Entscheidungsregeln für die Klassifikation dargestellt\nDie Grafik rechts zeigt die Entscheidungsregeln für die Genres Klassik, EDM und Hip-Hop illustriert denselben Sachverhalt wie die Partitionsgrafik"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-9",
    "href": "08_VL_D.html#entscheidungsbäume-9",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\nRoot-Node\n\n\nDer Split umfasst die Anteile der Klassen in der Root-Node\nDer Datensat umfasst 30% EDM Songs, 36% Hip-Hop Songs und 33% Klassik Songs\nDa die Mehrheit der Songs aus dem Genre Hip-Hop stammen, ist die Root-Node Hip-Hop grau"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-10",
    "href": "08_VL_D.html#entscheidungsbäume-10",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\n\nEntscheidungsregel 1:\n\nUnter den Nodes wird die Entscheidungsregel dargestellt\nIn unserem Fall wird der Datensatz anhand der Variable Energy \\(\\geq\\) 0.37 in zwei Regionen geteilt\nDer Pfad links entspricht der Region, in der die Bedingung erfüllt ist, rechts der Region, in der die Bedingung nicht erfüllt ist\nBeide Wege führen zu einer weiteren Entscheidungsregel"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-11",
    "href": "08_VL_D.html#entscheidungsbäume-11",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\n\nEntscheidungsregel 2 (links):\n\nHier befinden sich 67% der Daten\nDiese setzen sich zusammen aus:\n\n45% EDM, 53% Hip-Hop und 2% Klassik\n\nDie Entscheidungsregel ist nun Energy \\(\\geq\\) 0.82\nWenn die Bedingung erfüllt ist, wird der Song dem Genre EDM zugeordnet\nWenn die Bedingung nicht erfüllt ist, wird der Song dem Genre Hip-Hop zugeordnet"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-12",
    "href": "08_VL_D.html#entscheidungsbäume-12",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\n\nEntscheidungsregel 3 (rechts):\n\nHier befinden sich 33% der Daten\nDiese setzen sich zusammen aus:\n\n0% EDM, 2% Hip-Hop und 98% Klassik\n\nDie Entscheidungsregel ist nun Danceability \\(\\geq\\) 0.67\nWenn die Bedingung erfüllt ist, wird der Song dem Genre Hip-Hop zugeordnet\nWenn die Bedingung nicht erfüllt ist, wird der Song dem Genre Klassik zugeordnet"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-13",
    "href": "08_VL_D.html#entscheidungsbäume-13",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nDarstellung der Entscheidungsregeln\n\n\n\n\nDie untersten Nodes sind die sogenannten Blätter des Entscheidungsbaums\nJedes Blatt entspricht einer Region, in der die Klassifikation erfolgt\nDie Angabe unterhalb der Klassifikation ergibt die Wahrscheinlichkeit, dass ein Song in das jeweilige Genre fällt\nDie Wahrscheinlichkeit entspricht dabei dem Anteil der Songs in der Region, die den jeweiligen Genres zugeordnet sind\nDie Variable Danceability spielt für die Klassifikation offenbar keine Rolle"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-14",
    "href": "08_VL_D.html#entscheidungsbäume-14",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nModellevaluation\n\n\n\nDie Modellevaluation erfolgt analog zur logistischen Regression und Nearest Neighbor Klassifikation\nDie Konfusionsmatrix rechts führt zu diesen Metriken auf Basis der Testdaten:\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nmulticlass\n0.83\n\n\nrecall\nmacro\n0.83\n\n\nprecision\nmacro\n0.85\n\n\nf_meas\nmacro\n0.83"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-15",
    "href": "08_VL_D.html#entscheidungsbäume-15",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nOverfitting\n\n\nZiel beim Machine Learning ist eine Generalisierung auf bisher unbekannte Daten\nDas bisherige Modell hat nach 2 Nodes aufgehört, aber warum?\nDies erfolgt aufgrund sogenannter Stopping Rules oder Hyperparameter\nKomplexitätskosten:\n\nEin zusätzlicher Split verbessert die Klassifikation auf den Trainingsdaten, führt aber zu erhöhter Komplexität des Entscheidungsbaums\nDie Komplexitätskosten begrenzen weitere Splits und sorgen dafür, dass nur dann weiter aufgeteilt wird, wenn ein Minimum an zusätzlicher Verbesserung des Modells gewährleistet wird\n\nMaximale Tiefe:\n\nBegrenzung der Verzweigung verhindert eine granulare Einteilung der Daten\n\n\nMinimum an Beobachtungen pro Blatt:\n\nVerhindert, dass nur wenige Beobachtungen in den finalen Blättern sind"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-16",
    "href": "08_VL_D.html#entscheidungsbäume-16",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nOverfitting\n\n\n\n\nGenerell gilt: Je komplexer der Entscheidungsbaum, desto besser ist die Anpassung auf den Trainingsdaten\nAber: Je perfekter die Anpassung auf den Trainingsdaten, desto schlechter die Generalisierung auf unbekannte Daten\nDie Grafik rechts zeigt ein extremes Beispiel, bei dem der Entscheidungsbaum mit 5 Nodes ausgebaut wurde"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-17",
    "href": "08_VL_D.html#entscheidungsbäume-17",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nOverfitting\n\n\n\n\nIm Rahmen des Hyperparameter-Tunings wird die Komplexität des Entscheidungsbaums optimiert\nDie Grafik rechts zeigt die Anpassung des Entscheidungsbaums an die Trainingsdaten und die Generalisierung auf die Testdaten für verschiedene Tiefen des Baums\nAls Metrik dient der F1-Score\nOffenbar steigt der F1-Score auf den Trainingsdaten mit der Tiefe des Baums, während es auf den Testdaten ein Optimum bei 6 Nodes gibt"
  },
  {
    "objectID": "08_VL_D.html#entscheidungsbäume-18",
    "href": "08_VL_D.html#entscheidungsbäume-18",
    "title": "Business Intelligence & Data Science",
    "section": "Entscheidungsbäume",
    "text": "Entscheidungsbäume\nErweiterungen\n\nEntscheidungsbäume sind sehr intuitiv und einfach zu interpretieren\nEntscheidungsbäume erfordern nur wenig Datenbereinigung und können mit numerischen und kategorialen Variablen umgehen\nAllerdings sind Entscheidungsbäume anfällig für Overfitting\nEntscheidungsbäume sind Grundlage für viele weitere Modelle:\n\nRandom Forests kombinieren mehrere Entscheidungsbäume, um die Vorhersagegenauigkeit zu verbessern und das Overfitting zu reduzieren\nBoosted Trees kombinieren sequentielle Entscheidungsbäume, die auf den Fehlern des vorherigen Baums aufbauen"
  },
  {
    "objectID": "08_VL_D.html#modellvergleich",
    "href": "08_VL_D.html#modellvergleich",
    "title": "Business Intelligence & Data Science",
    "section": "Modellvergleich",
    "text": "Modellvergleich\n…and the Winner is?\n\nDie Modelle mit 2 erklärenden Variablen dienen primär der Illustration der Modellintuition und sind daher nicht unbedingt praxistauglich\nWelches Modell performt am besten auf den Testdaten?\n\n\n\n\n\n\n\n\n.metric\nLogistische Regression\n60-Nearest Neighbor\nEntscheidungsbaum 6 Nodes\n\n\n\n\naccuracy\n0.8478\n0.8521\n0.8472\n\n\nrecall\n0.8454\n0.8458\n0.8435\n\n\nprecision\n0.8479\n0.8575\n0.8488\n\n\nf_meas\n0.8464\n0.8481\n0.8453\n\n\n\n\n\n\n\n\n\n\nWenn auch knapp hat das Nearest Neighbor Modell mit \\(K = 60\\) die Nase vorn\n\n\n\nBusiness Intelligence & Data Science, SoSe 2024"
  },
  {
    "objectID": "01_VL_D.html#der-plan-für-heute",
    "href": "01_VL_D.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung 1\n\n\nVorstellung & kurzes Kennenlern-Quiz\nOrganisatorisches\nEinleitung:\n\nWas sind Business Intelligence & Data Science?\nWas sind operative und dispositive Daten?\nWie werden aus operativen Daten entscheidungsrelevante Informationen?\n\nErster Login bei unserem BI-Tool Superset\nDatenexploration in Superset\n\n\n\n\nWir starten mit einer kurzen Vorstellung, in der ich mich kurz vorstelle und anschließend gibt es ein kurzes Quiz, das einerseits Quizzes als Tool einführt, mit dem wir künftig die Doppelblöcke eröffnet und mir andererseits die Gelegenheit gibt, den Wissenstand abzuklopfn\nDann folgt ein Stresstest für unser BI-Tool Apache Superset, das wir künftig für die Visualisierung von Daten nutzen werden, indem wir uns alle gleichzeitig einloggen und testen, ob das System der Last standhält\nHiernach folgen ein paar organisatorische Hinweise, bevor wir in die inhaltliche Einführung starten:\n\nWas ist das eigentlich, Business Intelligence und Data Science?\nWie werden Informationen aus operativen Daten gewonnen?"
  },
  {
    "objectID": "01_VL_D.html#vorstellung",
    "href": "01_VL_D.html#vorstellung",
    "title": "Business Intelligence & Data Science",
    "section": "Vorstellung",
    "text": "Vorstellung\nKennenlern-Quiz\n\n\nDas Kennenlern-Quiz umfasst Fragen zu Vorkenntnissen rund um BI-Software und R, um ein Gefühl für den Wissensstand zu bekommen.\nAußerdem gibt es wichtige Fachfragen rund um BI und Data Science.\nDie Gewinnerin/Der Gewinner erhält einen überragenden Preis\nAls Plattform dient Quizziz.com, es ist keine Registrierung notwendig, Verwendung über Smartphone und Laptop möglich\nKünftig starten wir einen Doppelblock mit einem Quiz zum vorangeganenen Block mit einem kleinen Preis für den Tagessieg"
  },
  {
    "objectID": "01_VL_D.html#warum-bi-und-data-science",
    "href": "01_VL_D.html#warum-bi-und-data-science",
    "title": "Business Intelligence & Data Science",
    "section": "Warum BI und Data Science?",
    "text": "Warum BI und Data Science?\nEntscheidende Skills für die berufliche Zukunft\n\n\n\nQuelle: McKinsey\n\n\n\n\nDie Grafik zeigt die sogenannten Delta Skills nach McKinsey, das sind jene Fähigkeiten, die einen Unterschied machen werden, um in der Arbeitswelt der Zukunft erfolgreich zu sein.\nDie Kategorien sind: Kognitiv, Interpersonell, Selbstführung und Digital\nUnser FOkus ist die Kategorie Digital, die sich wiederum in die Bereiche Digital Fluency, Software Nutzung und Entwicklung sowie Verständnis digitaler Systeme aufteilt.\nWas ist digital Fluency? Es ist die Fähigkeit, digitale Tools und Technologien zu nutzen und hier Digital Learning und Collaboration besonders wichtig.\nSoftware Nutzung und Entwicklung umfasst insbesondere Programming und Datenanalyse, insbesondere den zweiten Punkt werden wir sehr ausführlich im Methodenteil zu Data Science behandeln.\nZiel ist hier nicht, in wenigen Blöcken eine vollwertige Ausbildung in diesem Bereich zu ersetzen, sondern vielmehr das Big Picture zu vermitteln, um im Unternehmen als guter Sparringspartner für Data Science Themen und Projekte zu fungieren.\nProgramming wird eher nebenbei mit einfließen\nEin entscheidender Punkt ist unten rechts die Data Literacy. Das umfasst die Fähigkeit, Daten zu lesen, zu interpretieren und zu kommunizieren. Das ist ein zentraler Punkt, den wir in dieser Vorlesung adressieren werden. Meines Erachtens der größte Knackpunkt im Unternehmensumfeld, da hier die größte Lücke zwischen den Fachbereichen und der IT besteht."
  },
  {
    "objectID": "01_VL_D.html#warum-bi-und-data-science-1",
    "href": "01_VL_D.html#warum-bi-und-data-science-1",
    "title": "Business Intelligence & Data Science",
    "section": "Warum BI und Data Science?",
    "text": "Warum BI und Data Science?\nTechnologische Kompetenzen\n\n\n\nLink zur Quelle\n\n\n\n\nDie Grafik zeigt die Ergebnisse einer Studie des Stifterverbands, die sich mit den zukünftigen Anforderungen an die Arbeitswelt befasst.\nEs handelt sich hier um eine Umfrage und die Grafik gibt den Anteil der teilnehmenden an, welche die gegeben Aspekte jetzt und in 5 Jahren als wichtig erachten.\nFür uns wichtig sind IT-Architektur und Data Analytics & KI\nDie Vorlesung wird einige Architektur-Komponenten umfassen, nämlich den Aufbau einer BI-Architektur und eine Einbettung ins Unternehmen allgemein\nData Analytics und KI umfasst die Methoden, die wir in der zweiten Hälfte der Vorlesung behandeln werden, wobei der Fokus weniger auf KI liegt und mehr auf den Data Science Methoden, die vielen KI-Technologien zu Grunde liegen"
  },
  {
    "objectID": "01_VL_D.html#kursaufbau",
    "href": "01_VL_D.html#kursaufbau",
    "title": "Business Intelligence & Data Science",
    "section": "Kursaufbau",
    "text": "Kursaufbau\nZeitplan Gruppe D\n\n\n\n\n\nBlock\nGruppe D\nThema\n\n\n\n\n1\n21.03.2024:09:00 – 10:30\nOrganisation, Einleitung\n\n\n2\n21.03.2024:10:45 – 12:15\nDatenbereitstellung: Data Warehousing\n\n\n3\n28.03.2024:13:00 – 14:30\nDatentransformation\n\n\n4\n28.03.2024:14:45 – 16:15\nBig Data und Data Lake\n\n\n5\n02.04.2024:13:00 – 14:30\nInformationsgenerierung: Berichtsorientierte Analysen\n\n\n6\n02.04.2024:14:45 – 16:15\nAdvanced und Predictive Analytics: Grundlagen\n\n\n7\n15.04.2024:13:00 – 14:30\nAdvanced und Predictive Analytics: Klassifikation\n\n\n8\n15.04.2024:14:45 – 16:15\nAdvanced und Predictive Analytics: Klassifikation\n\n\n9\n02.05.2024:13:00 – 14:30\nRestinhalte Klassifikation; Assoziationsanalyse; Probeklausur (30 Minuten)\n\n\n10\n02.05.2024:14:45 – 16:15\nBesprechung Probeklausur; Evaluation; Fragen\n\n\n-\n13.05.2024:13:00 – 14:30\nKlausur (60 Minuten)"
  },
  {
    "objectID": "01_VL_D.html#kursmaterialen",
    "href": "01_VL_D.html#kursmaterialen",
    "title": "Business Intelligence & Data Science",
    "section": "Kursmaterialen",
    "text": "Kursmaterialen\nKurs-Website\n\nAlle Kursinhalte sind auf der Kurs-Website verfügbar:\n\nhttps://sebschroen.github.io/bi_and_ds-lecture_notes/\n\n\n\n\nAktuell passwortgeschützt aufgrund Nutzung Copyright-geschützter Materialien, daher nur Nutzung in diesem Personenkreis\n3 gefundene und gemeldete Typos (E-Mail, StudIP) = 1 Packung Bahlsen-Kekse nach Wahl\nTipp: Bookmark des Links nach Passworteingabe erübrigt künftige Eingabe des Passworts\n\n\n\n\nPasswort anschreiben\nEinfach zusammen durchklicken und kurz Zeit zur Orientierung geben, Trick zum Bookmark zeigen"
  },
  {
    "objectID": "01_VL_D.html#kursmaterialen-1",
    "href": "01_VL_D.html#kursmaterialen-1",
    "title": "Business Intelligence & Data Science",
    "section": "Kursmaterialen",
    "text": "Kursmaterialen\nFolien\n\n\nFolien sind auf der Startseite der Kurs-Website verlinkt\nDarstellung ist für den Browser (Chrome, Safari und Firefox) optimiert, um interaktive Elemente darzustellen\nIm Burger-Menü oben rechts lässt sich zu jeder Zeit auch eine PDF Version zum Ausdruck in Papierform oder für Notizen erstellen:\n\nTools -&gt; PDF Export Mode -&gt; Strg + P (Cmd + P) -&gt; Druck als PDF\n\nDie Folien werden rechtzeitig vor der Vorlesung als PDF auf StudIP hochgeladen\nInteraktive Elemente werden separat verlinkt.\n\n\n\n\nDie Folien sind im Ablaufplan auf der Startseite in aktueller Fassung verlinkt und lassen sich auf mehreren Wegen darstellen.\n\n\nKonzipiert sind die Folien für die Darstellung im Browser, um interaktive Elemente darzustellen.\nIm Menü oben rechts lässt sich zu jeder Zeit auch eine PDF Version zum Ausdrucken in Papierform erstellen oder zur Darstellung auf Endgeräten, auf denen Sie Notizen machen möchten.\nAm Ende jeder Vorlesung wird die finale Fassung der Folien auf StudIP im PDF Format hochgeladen.\nDie HTML Fassung bleibt verfügbar."
  },
  {
    "objectID": "01_VL_D.html#kursmaterialen-2",
    "href": "01_VL_D.html#kursmaterialen-2",
    "title": "Business Intelligence & Data Science",
    "section": "Kursmaterialen",
    "text": "Kursmaterialen\nPDF-Skript\n\nNeben der HTML-Version ist auch ein PDF-Skript verfügbar und kann auf der Startseite heruntergeladen werden\nDie PDF-Version entspricht inhaltlich immer der Website\nDie Darstellung ist für HTML optimiert und kann für Artefakte beim PDF Rendering sorgen, erleichtert aber das Ausdrucken\n\n\n\nDie PDF-Version zum Skript ist auf der Startseite der Website verlinkt und entspricht inhaltlich immer der Website.\nDie Darstellung ist für html optimiert und kann für Artefakte beim PDF Rendering sorgen, erleichtert aber das Ausdrucken"
  },
  {
    "objectID": "01_VL_D.html#kursmaterialien",
    "href": "01_VL_D.html#kursmaterialien",
    "title": "Business Intelligence & Data Science",
    "section": "Kursmaterialien",
    "text": "Kursmaterialien\nErgänzende Literatur\n\n\n\nAlle klausurrelevanten Inhalte lassen sich auf der Kurs-Website finden und nachlesen, zusätzliche Literatur ist nicht notwendig\nDer Aufbau des Kurses richtet sich nach dem Lehrbuch Business Intelligence & Analytics - Grundlagen und praktische Anwendungen, 4. Auflage von Henning Baars und Hans-Georg Kemper\nDas Buch ist über die Bibliothek der Leibniz FH als E-Book verfügbar\nBlock 1 - 5 entstammen größtenteils in Baars und Kemper (2021)\nMethodische Aspekte rund um Predictive Analytics entstammen größtenteils dem frei verfügbaren Introduction to Statistical Learning, 2. Auflage von Gareth James, Daniela Witten, Trevor Hastie und Robert Tibshirani\nDie Quellen zu jeder Vorlesung sind jeweils auf der letzten Folien angegeben."
  },
  {
    "objectID": "01_VL_D.html#business-intelligence",
    "href": "01_VL_D.html#business-intelligence",
    "title": "Business Intelligence & Data Science",
    "section": "Business Intelligence",
    "text": "Business Intelligence\nBegriffsabgrenzung\n\n\n\n\n\n\nDefinition\n\n\nBusiness Intelligence (BI) ist eine Reihe von Architekturen und Technologien, die Rohdaten in sinnvolle und nutzbare, entscheidungsrelevante Informationen umwandeln. Es ermöglicht Anwendenden, informierte Entscheidungen auf der Grundlage von Daten zu treffen, die ein Unternehmen gegenüber seinen Wettbewerbern in Vorteil bringen können (siehe Forrester.com).\n\n\n\n\n\nAbgeleitet vom Intelligence-Begriff in der militärischen Informationsverarbeitung Großbritanniens im 2. Weltkrieg:\n\nDie richtigen Informationen zur richtigen Zeit an die richtigen Personen.\n\nFrühe kommerzielle Ansätze in den 60er Jahren im Zuge der Entwicklung relationaler Datenbanken."
  },
  {
    "objectID": "01_VL_D.html#business-intelligence-1",
    "href": "01_VL_D.html#business-intelligence-1",
    "title": "Business Intelligence & Data Science",
    "section": "Business Intelligence",
    "text": "Business Intelligence\nBegriffsabgrenzung\n\n\nZunächst Fokus auf Management Support Systeme (MSS) und daher eher auf oberste Ebenen zugeschnitten\nDer Begriff Business Intelligence (BI) wurde in den 1990ern geprägt\nHeute wird BI laut Gartner Group charakterisiert durch:\n\nBreite Verfügbarkeit von BI Tools auf allen Ebenen des Unternehmens\nGeschäftsentscheidung auf Basis aktueller Informationen und Daten und nicht auf Intuition\nUmfangreiche Analyse- und Reportingmöglichkeiten mit Self-Service Tools für Fachbereiche\n\n\n\n\n\nPunkt 1 und 3 der Definition von Gartner auf den ersten Blick identisch\nAber: Punkt 3 geht einen Schritt weiter, nämlich dass moderne Self-Service BI Tools mehr könnnen als früher und Fachbereichen ermöglichen, viele der Aufgaben selbst zu übernehmen, die früher durch die IT ausgeführt wurden, wie Drill-Downs, Erstellung eigener Visualisierungen und Dashbboards"
  },
  {
    "objectID": "01_VL_D.html#business-intelligence-2",
    "href": "01_VL_D.html#business-intelligence-2",
    "title": "Business Intelligence & Data Science",
    "section": "Business Intelligence",
    "text": "Business Intelligence\nBetriebliche Dimensionen\n\n\n\nBegriffsdimensionen von BI nach Schieder (2016).\n\n\n\n\nEine Personengruppe innerhalb der Organisation ist mit der Realisierung von BI-Prozessen vertraut.\nDie Generierung geschäftsrelevanter Informationen, Erkenntnisse und Wissen erfordert die Überführung fragmentierter Unternehmens- und Wettbewerbsdaten in handlungsgerichtetes Wissen. Hierbei liegt der Fokus auf dem Geschäftsprozess von der Datenerfassung hin zur Wissenskommunnikation.\nBezeichnet das Ergebnis eines Erkenntnissprozesses, beispielsweise ziel- und zweckorientiertes Wissen in Form von Berichten, Analysen und Prognosen für das Management.\nEine Sammlung von informationstechnischen Werkzeugen, Architekturen, Systemen und Technologien zur Aufbereitung und Bereitstellung reschäftsrelevanter Daten zum Zweck der Informationsgewinnung.\n\nIm Mittelpunkt dieser Vorlesung steht der BI-Prozess, beginnend mit der Datenerfassung und -bereitstellung im unternehmenseigenen Data Warehouse, über die Informationsentdeckung bzw. -generierung mit modellgestützten Analysemethoden, bis hin zur Kommunikation. Jeder Bestandteil des BI-Prozesses wird dabei um wichtige technische Aspekte ergänzt, bspw. Data Warehouse Architekturen, ausgewählten Analysemethoden und Einblicken in moderne BI-Dashboard-Tools. Ziel ist eine ganzheitliche Sicht auf den BI-Prozess im Unternehmenskontext."
  },
  {
    "objectID": "01_VL_D.html#data-science",
    "href": "01_VL_D.html#data-science",
    "title": "Business Intelligence & Data Science",
    "section": "Data Science",
    "text": "Data Science\nBegriffsabgrenzung\n\n\n\n\n\n\nDefinition\n\n\nData Science beschäftigt sich mit einer zweckorientierten Datenanalyse und der systematischen Generierung von Entscheidungshilfen und -grundlagen, um Wettbewerbsvorteile erzielen zu können. Der Schwerpunkt liegt dabei nicht auf den Daten selbst, sondern auf der Art und weise, wie diese verarbeitet und analysiert werden (siehe Gesellschaft für Informatik 2019).\n\n\n\n\n\nData Science ist ein vergleichsweise neues wissenschaftliches Feld, eine Kombination aus Statistik und Informatik, insbesondere Software Engineering\nDa es sich um ein junges Feld handelt sind Definitionen und die damit verbundenen Rollen im stetigen Wandel\n\n\n\nDieser Wandel geht so weit, dass die Rolle “Data Scientist” heute seltener auf Job-Portalen zu finden ist, als noch vor einigen Jahren. Stattdessen werden vermehrt spezialisierte Rollen wie “Data Engineer”, “Data Analyst” oder “Machine Learning Engineer” ausgeschrieben."
  },
  {
    "objectID": "01_VL_D.html#data-science-1",
    "href": "01_VL_D.html#data-science-1",
    "title": "Business Intelligence & Data Science",
    "section": "Data Science",
    "text": "Data Science\nSchwerpunkte\n\nAufgrund der potentiellen Breite des Felds erfolgt oft eine genauere Aufteilung in vier Kernbereiche:\n\n\n\nData Engineering: Methoden und Prozesse für die Speicherung, Haltung und Replikation von Daten\nData Analytics: Datenanalyse mit statistischen Methoden\nPredictive Modelling: Die Verwendung von statistischen Methoden zur Vorhersage\nMachine Learning: Algorithmen, die aus Daten lernen, Muster erkennen und hierauf aufbauend neue Situationen oder zukünftige Entwicklungen vorhersagen"
  },
  {
    "objectID": "01_VL_D.html#data-science-2",
    "href": "01_VL_D.html#data-science-2",
    "title": "Business Intelligence & Data Science",
    "section": "Data Science",
    "text": "Data Science\nReifegrade von Data Analytics\n\n\n\nQuelle: Milind Desai on Medium.com\n\n\n\n\nDescriptive Analytics: Beschreibende Analyse des Ist-Zustandes und der Vergangenheit. Im Mittelpunkt steht die Frage: Was ist passiert?\nDiagnostic Analytics: Analysiert die Zusammenhänge, die zum Ist-Zustand geführt haben und führt oft mehrere deskriptive Charakteristika zusammen: Warum ist der Status Quo eingetreten?\nPredictive Modelling: Ausgehend von einem detaillierten Verständnis des Status Quo wird eine Prognose für die Zukunft erstellt: Was wird passieren?\nPrescriptive Analytics: Dient der Identifizierung möglicher Handlungsoptionen auf Basis der Prognosen, entweder um eine schlechte Prognose abzuwenden oder die Realisierung einer guten Prognose zu unterstützen. Mit anderen Worten: Was muss getan werden, um die Zukunft zu unseren Gunsten zu beeinflussen"
  },
  {
    "objectID": "01_VL_D.html#data-science-3",
    "href": "01_VL_D.html#data-science-3",
    "title": "Business Intelligence & Data Science",
    "section": "Data Science",
    "text": "Data Science\nData Science, Data Analytics, Data Mining?\n\n\nDie Unterscheidung zwischen den Begriffen Data Analytics, Data Mining und Data Science ist nicht immer trennscharf\nData Mining ist meist definiert als der Prozess der Informationsextraktion aus Daten und ist ebenso wie Data Analytics eine Teilmenge von Data Science\nIn dieser Vorlesung dient Data Science als methodischer Baukasten, um den BI-Prozess mit modellgestützten Methoden anzureichern und Zusammenhänge sichtbar zu machen\nHierbei steht der Zweck der Modelle, nämlich die Entscheidungsunterstützung, im Vordergrund"
  },
  {
    "objectID": "01_VL_D.html#business-intelligence-und-data-science",
    "href": "01_VL_D.html#business-intelligence-und-data-science",
    "title": "Business Intelligence & Data Science",
    "section": "Business Intelligence und Data Science",
    "text": "Business Intelligence und Data Science\nZusammenführung der Begriffe und inhaltlicher Aufbau der Vorlesung\n\n\n\nBIA Gesamtansatz. Eigene Darstellung in Anlehnung an Baars und Kemper (2021).\n\n\n\nDie Abbildung illustriert den BIA-Ansatz als dreiteiligen Ordnungsrahmen, bestehend aus Datenbereitstellung, Informationsgenerierung und Informationsbereitstellung. Die Datenerfassung aus operativen und externen Systemen ist diesem Ordnungsrahmen hier vorgelagert. Das hieraus entstehende Modell entspricht der prozessualen BI-Dimension.\nGanz unten startenw ir mit zahlreichen operative und externen Quellsysteme, beispielsweise ERP-Systeme (häufig SAP), Produktdatenmanagement Systeme (PDM) oder Manufacturing Execution Systeme (MES).\nHinzu kommen häufig offene Daten wie Wetter- oder Konjunkturdaten und insbesondere im industriellen Kontext verstärkt Sensordaten aus internetfähigen Maschinen, sogenannte Internet of Things (IoT) Devices. Die strukturierte und systematische Integration dieser Daten mittels ETL Methoden (Extract, Transfer, Load), ist die erste Herausforderung jedes integrierten BIA-Systems.\nDie sogenannte Datenbereitstellung dient der konsistenten und strukturierten Speicherung und Persistierung aller relevanten Daten aus den oben genannten Quellsystemen. Hier gilt es verschiedene Konzepte näher zu beleuchten, insbesondere gängige Data Warehouse Konzepte, die meist aus sogenannten Data Marts und Core Data Warehouses bestehen und der themenbezogenen und integrierten Datenhaltung dienen. Je nach Anwendung wird das Datenmaterial meist voraggregiert. Zur Integration großvolumiger und schnell einlaufender Daten hat sich ergänzend das Konzept eines Data Lakes etabliert, in dem anders als im Data Warehouse Rohdaten ohne Aggregation abgelegt und verfügbar gemacht werden.\nDie Informationsgenerierung als zweite Schicht dient der Umwandlung der Rohdaten in entscheidungsfreundliche Formate, bspw. berichtsorientierte oder modellgestützte Analysen. Hier werden aus Daten erste Informationen generiert, auf Basis derer weitere Erkenntnisse über den Status Quo entstehen und mögliche Prognosen für die Zukunft erstellt werden können. Das Bindeglied zwischen Datenbereitstellung und Informationsgenerierung sind Systeme zur Datenabfrage und Exploration.\nDie Darstellung, Kanalisierung und Verbreitung von Informationen folgt in der dritten Schicht, der Informationsbereitstellung. Neben modernen Self-Service BI-Tools umfasst dies auch zielgruppenadäquate Präsentationen oder statische Berichte."
  },
  {
    "objectID": "01_VL_D.html#dispositive-und-operative-daten",
    "href": "01_VL_D.html#dispositive-und-operative-daten",
    "title": "Business Intelligence & Data Science",
    "section": "Dispositive und operative Daten",
    "text": "Dispositive und operative Daten\nOperative versus dispositive Aufgaben\n\n\nAnders als die anfänglichen MSS unterstützen moderne BI-Systeme sowohl operative, als auch dispositive Aufgaben\nDispositive Aufgaben sind Leitungs- und Lenkungstätigkeiten im betrieblichen Ablauf\nOperative Aufgaben umfassen die Leistungserstellung oder -verwertung\nAn beide Aufgabenfelder gelten unterschiedliche Anforderungen, die in Daten und Systemen abgebildet werden müssen"
  },
  {
    "objectID": "01_VL_D.html#dispositive-und-operative-daten-1",
    "href": "01_VL_D.html#dispositive-und-operative-daten-1",
    "title": "Business Intelligence & Data Science",
    "section": "Dispositive und operative Daten",
    "text": "Dispositive und operative Daten\nOperative versus dispositive Daten\n\n\n\n\nDispositive Daten\n\nUnterstützen Leitungs- und Lenkungstätigkeiten im betrieblichen Ablauf\nHäufig verdichtet, transformiert und themenbezogen aufbereitet und mit Historie angereichert\n\n\nOperative Daten\n\nDienen der Abwicklung von Geschäftsprozessen und werden im Rahmen von Transaktionen erzeugt\nTransaktionen: Atomare und logisch untrennbare Datenbankvorgänge\nSehr granular und mit hoher Änderungsrate\nBeispiele sind Bestellungen, Aufträge und Lagerbestände oder Stammdaten"
  },
  {
    "objectID": "01_VL_D.html#dispositive-und-operative-daten-2",
    "href": "01_VL_D.html#dispositive-und-operative-daten-2",
    "title": "Business Intelligence & Data Science",
    "section": "Dispositive und operative Daten",
    "text": "Dispositive und operative Daten\nOperative versus dispositive Daten\n\n\nCharakteristika operativer und entscheidungsorientierter Daten im Vergleich. In Anlehnung an Baars und Kemper (2021)\n\n\n\n\n\n\n\n\nOperative Daten\nEntscheidungsorientierte Daten\n\n\n\n\nZiel\nAbwicklung der Geschäftsprozesse\nInformationen für Entscheidungen\n\n\nAusrichtung\nDetailliert und granular\nMeist verdichtet und transformiert mit Metadaten\n\n\nZeitbezug\nAktualität steht im Vordergrund, Zeitpunkt der Transaktion, keine Historisierung\nAktualität variiert mit der Aufgabe, Historienbetrachtung ist möglich\n\n\nModellierung\nKeine Altbestände\nSachgebiets- und themenbezogen orientiert und anwendungstauglich\n\n\nZustand\nHäufig redundant und inkonsistent zwischen Systemen\nKonsistent modelliert, Redundanz bewusst\n\n\nUpdate\nLaufend, Real-time\nErgänzend als Fortschreibung\n\n\nQueries\nStrukturiert, standardisiert und meistens statisch\nAd-hoc und dynamisch für wechselnde Fragestellungen sowie Standardberichte"
  },
  {
    "objectID": "01_VL_D.html#dispositive-und-operative-daten-3",
    "href": "01_VL_D.html#dispositive-und-operative-daten-3",
    "title": "Business Intelligence & Data Science",
    "section": "Dispositive und operative Daten",
    "text": "Dispositive und operative Daten\nÜberführung von Daten in Information\n\n\n\nHauptziel dieser Vorlesung ist die Überführung von operationalen Daten in entscheidungsrelevante Informationen\nDies hat zwei Hauptaspekte:\n\nBlock 1-4: Technische Infrastruktur und Architektur\nBlock 5-9: Methodische Konzepte wie modellorientierte Analysen\n\n\n\n\n\n\n\n\nBlock\nThema\n\n\n\n\n1\nOrganisation, Einleitung\n\n\n2\nDatenbereitstellung: Data Warehousing\n\n\n3\nDatentransformation\n\n\n4\nBig Data und Data Lake\n\n\n5\nInformationsgenerierung: Berichtsorientierte Analysen\n\n\n6\nAdvanced und Predictive Analytics: Grundlagen\n\n\n7\nAdvanced und Predictive Analytics: Klassifikation\n\n\n8\nAdvanced und Predictive Analytics: Klassifikation\n\n\n9\nRestinhalte Klassifikation; Assoziationsanalyse; Probeklausur (30 Minuten)"
  },
  {
    "objectID": "01_VL_D.html#apache-superset",
    "href": "01_VL_D.html#apache-superset",
    "title": "Business Intelligence & Data Science",
    "section": "Apache Superset",
    "text": "Apache Superset\nErster Login und Überblick\n\n\n\nApache Superset ist ein Open Source BI-Tool, das über die Vorlesung hinweg neben R als zentrales Tool dient\nDas Tool ist in einem sogenannten Kubernetes Cluster in der Google Cloud gehosted und kann über folgenden Link erreicht werden:\n\nhttp://34.95.70.95/login/\n\nDie Verbindung ist (aktuell noch) nicht https verschlüsselt, daher wird ein Warnhinweis erscheinen, der sich je nach Browser unterschiedlich umgehen lässt\nDie Anmeldung erfolgt mit den Zugangsdaten, die auf StudIP veröffentlicht sind\nDie Superset Instanz bleibt bis zur letzten Vorlesung online, bis dahin erstellte Dashboards und Grafiken werden als Export auf StudIP bereitgestellt"
  },
  {
    "objectID": "01_VL_D.html#self-service-bi",
    "href": "01_VL_D.html#self-service-bi",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI",
    "text": "Self-Service BI\nTerminologie\n\n\nDashboard:\n\nEine Sammlung von Visualisierungen, die in einem gemeinsamen Kontext dargestellt werden\nEin Dashboard kann mehrere Visualisierungen enthalten, die auf unterschiedlichen Datasets basieren\nDie Visualisierungen können über globale Filter gefiltert werden\n\nChart:\n\nEigenständige Visualisierung eines Datasets, die eigenständig oder in einem Dashboard dargestellt werden kann\nEs gibt eine Vielzahl von Chart-Typen, die in Superset dargestellt werden können\nPreset.io gibt eine Übersicht über die gängigsten Chart-Typen"
  },
  {
    "objectID": "01_VL_D.html#self-service-bi-1",
    "href": "01_VL_D.html#self-service-bi-1",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI",
    "text": "Self-Service BI\nTerminologie\n\n\nDimensions:\n\nDimensionen sind die Kategorien, nach denen Daten gruppiert werden\nIdealerweise kategoriale Variablen, die nicht aggregiert werden\nWenn die Daten eine Zeitdimension enthalten (erkennbar am Uhren-Symbol) ist eine spezielle Time Dimension verfügbar\nBei korrekter Pflege der Zeitvariable lässt sich direkt auf Tages-, Wochen-, Monats- und Jahreswerte aggregieren mittels Time Grain\n\nMetrics\n\nQuantitative Variablen, die sich mit Funktionen aggregieren lassen\nBeispielfunktionen sind Summen, Durchschnitte, Min und Max oder Counts\nBesonderheit: Viele BI-Tools erstellen per Default eine Count-Metrik, die Datenpunkte pro Dimension zählt"
  },
  {
    "objectID": "01_VL_D.html#self-service-bi-2",
    "href": "01_VL_D.html#self-service-bi-2",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI",
    "text": "Self-Service BI\nTerminologie\n\n\nAggregationsfunktionen\n\nSelf-Service BI Tools werden meist mit Daten von höchster Granularität verknüpft, um die Flexibilität der Analyse zu erhöhen und Aggregationen seitens der Nutzenden zu ermöglichen\nAggregationsfunktionen werden auf Metriken angewendet, um die Daten zu verdichten\nEinfache Beispiele sind Summen, Durchschnitte, Min und Max\nBesonderheit Count und Count Distinct:\n\nCount zählt die Anzahl der Datenpunkte,\nCount Distinct zählt die Anzahl der einzigartigen Werte über die Dimension(en)\n\nSemantisches Wissen und Fragestellung sind für die Wahl der Aggregation entscheidend"
  },
  {
    "objectID": "01_VL_D.html#self-service-bi-3",
    "href": "01_VL_D.html#self-service-bi-3",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI",
    "text": "Self-Service BI\nBesonderheiten Superset\n\n\nDatabase:\n\nBackend-Datenbank, in der die Rohdaten liegen, das Pendant zu einem Data Warehouse\nIn unserem Fall Google BigQuery, aber auch andere Datenbanken wie MySQL, PostgreSQL oder SQLite sind möglich\nAuch der Upload von Excel und CSV Dateien ist möglich\n\nDataset:\n\nEinzelne Tabellen in der Datenbank, die als Grundlage für Analysen und Visualisierungen dienen\nBasieren auf Rohdaten, die für die Visualisierung verarbeitet werden\nPhysische Datasets “leben” auf der Backend-Datenbank, virtuelle Datasets sind direkt in Superset generiert und gespeichert"
  },
  {
    "objectID": "01_VL_D.html#warmup",
    "href": "01_VL_D.html#warmup",
    "title": "Business Intelligence & Data Science",
    "section": "Warmup",
    "text": "Warmup\nDatenexploration in Superset\n\nWir starten mit einer kurzen Datenexploration in Superset mit dem Dataset “Warmup”\nHierzu gehen wir auf der Startseite auf Charts und wählen oben rechts +Chart aus\nNun sind 2 Schritte notwendig:\n\nAuswahl des Datasets\n\nHier wählen wir “Warmup” aus\n\nAuswahl des Chart-Typs\n\nEin guter Startpunkt für die explorative Analyse ist eine Tabelle, also wählen wir Table"
  },
  {
    "objectID": "01_VL_D.html#warmup-1",
    "href": "01_VL_D.html#warmup-1",
    "title": "Business Intelligence & Data Science",
    "section": "Warmup",
    "text": "Warmup\nDatenexploration in Superset"
  },
  {
    "objectID": "01_VL_D.html#warmup-2",
    "href": "01_VL_D.html#warmup-2",
    "title": "Business Intelligence & Data Science",
    "section": "Warmup",
    "text": "Warmup\nDatenexploration in Superset"
  },
  {
    "objectID": "01_VL_D.html#warmup-3",
    "href": "01_VL_D.html#warmup-3",
    "title": "Business Intelligence & Data Science",
    "section": "Warmup",
    "text": "Warmup\nDatenexploration in Superset\n\nFragen:\n\nWas ist die durchschnittliche Beliebtheit pro Genre?\nWie viele Zeilen hat der Datensatz?\nWelche Genres und Artists gibt es?\nWie viele Songs gibt es pro Genre?\nWie viele verschiedene Artists gibt es pro Genre?\nWann ist das aktuellste (älteste) Album erschienen?"
  },
  {
    "objectID": "01_VL_D.html#warmup-4",
    "href": "01_VL_D.html#warmup-4",
    "title": "Business Intelligence & Data Science",
    "section": "Warmup",
    "text": "Warmup\nDatenexploration in Superset mit SQL Lab (Alternative)"
  },
  {
    "objectID": "01_VL_D.html#quellen",
    "href": "01_VL_D.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nBaars, Henning, und Hans-Georg Kemper. 2021. Business Intelligence & Analytics: Grundlagen und praktische Anwendungen: Ansätze der IT-basierten Entscheidungsunterstützung. 4., überarbeitete und erweiterte Auflage. Lehrbuch. Wiesbaden [Heidelberg]: Springer Vieweg.\n\n\nGesellschaft für Informatik. 2019. „Data Science: Lern- und Ausbildungsinhalte“. Gesellschaft für Informatik.\n\n\nSchieder, Christian. 2016. „Historische Fragmente einer Integrationsdisziplin – Beitrag zur Konstruktgeschichte der Business Intelligence“. In Analytische Informationssysteme, herausgegeben von Peter Gluchowski und Peter Chamoni, 13–32. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-47763-2_2.\n\n\nSharma, Vinod, Jeanne Poulose, und Chandan Maheshkar. 2023. „Analytics Enabled Decision Making ‚Tracing the Journey from Data to Decisions‘“. In Analytics Enabled Decision Making, herausgegeben von Vinod Sharma, Chandan Maheshkar, und Jeanne Poulose, 1–22. Singapore: Springer Nature Singapore. https://doi.org/10.1007/978-981-19-9658-0_1.\n\n\nWinter, Robert. 2016. „Analytische Informationssysteme aus Managementsicht: lokale Entscheidungsunterstützung vs. unternehmensweite Informations-Infrastruktur“. In Analytische Informationssysteme, herausgegeben von Peter Gluchowski und Peter Chamoni, 67–95. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-47763-2_5."
  },
  {
    "objectID": "01_VL.html#der-plan-für-heute",
    "href": "01_VL.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung 1\n\n\nVorstellung & kurzes Kennenlern-Quiz\nOrganisatorisches\nEinleitung:\n\nWas sind Business Intelligence & Data Science?\nWas sind operative und dispositive Daten?\nWie werden aus operativen Daten entscheidungsrelevante Informationen?\n\n\n\n\n\nWir starten mit einer kurzen Vorstellung, in der ich mich kurz vorstelle und anschließend gibt es ein kurzes Quiz, das einerseits Quizzes als Tool einführt, mit dem wir künftig die Doppelblöcke eröffnet und mir andererseits die Gelegenheit gibt, den Wissenstand abzuklopfn\nDann folgt ein Stresstest für unser BI-Tool Apache Superset, das wir künftig für die Visualisierung von Daten nutzen werden, indem wir uns alle gleichzeitig einloggen und testen, ob das System der Last standhält\nHiernach folgen ein paar organisatorische Hinweise, bevor wir in die inhaltliche Einführung starten:\n\nWas ist das eigentlich, Business Intelligence und Data Science?\nWie werden Informationen aus operativen Daten gewonnen?"
  },
  {
    "objectID": "01_VL.html#vorstellung",
    "href": "01_VL.html#vorstellung",
    "title": "Business Intelligence & Data Science",
    "section": "Vorstellung",
    "text": "Vorstellung\nKennenlern-Quiz\n\n\nDas Kennenlern-Quiz umfasst Fragen zu Vorkenntnissen rund um BI-Software und R, um ein Gefühl für den Wissensstand zu bekommen.\nAußerdem gibt es wichtige Fachfragen rund um BI und Data Science.\nDie Gewinnerin/Der Gewinner erhält einen überragenden Preis\nAls Plattform dient Quizziz.com, es ist keine Registrierung notwendig, Verwendung über Smartphone und Laptop möglich\nKünftig starten wir einen Doppelblock mit einem Quiz zum vorangeganenen Block mit einem kleinen Preis für den Tagessieg"
  },
  {
    "objectID": "01_VL.html#warum-bi-und-data-science",
    "href": "01_VL.html#warum-bi-und-data-science",
    "title": "Business Intelligence & Data Science",
    "section": "Warum BI und Data Science?",
    "text": "Warum BI und Data Science?\nEntscheidende Skills für die berufliche Zukunft\n\n\n\nQuelle: McKinsey\n\n\n\n\nDie Grafik zeigt die sogenannten Delta Skills nach McKinsey, das sind jene Fähigkeiten, die einen Unterschied machen werden, um in der Arbeitswelt der Zukunft erfolgreich zu sein.\nDie Kategorien sind: Kognitiv, Interpersonell, Selbstführung und Digital\nUnser FOkus ist die Kategorie Digital, die sich wiederum in die Bereiche Digital Fluency, Software Nutzung und Entwicklung sowie Verständnis digitaler Systeme aufteilt.\nWas ist digital Fluency? Es ist die Fähigkeit, digitale Tools und Technologien zu nutzen und hier Digital Learning und Collaboration besonders wichtig.\nSoftware Nutzung und Entwicklung umfasst insbesondere Programming und Datenanalyse, insbesondere den zweiten Punkt werden wir sehr ausführlich im Methodenteil zu Data Science behandeln.\nZiel ist hier nicht, in wenigen Blöcken eine vollwertige Ausbildung in diesem Bereich zu ersetzen, sondern vielmehr das Big Picture zu vermitteln, um im Unternehmen als guter Sparringspartner für Data Science Themen und Projekte zu fungieren.\nProgramming wird eher nebenbei mit einfließen\nEin entscheidender Punkt ist unten rechts die Data Literacy. Das umfasst die Fähigkeit, Daten zu lesen, zu interpretieren und zu kommunizieren. Das ist ein zentraler Punkt, den wir in dieser Vorlesung adressieren werden. Meines Erachtens der größte Knackpunkt im Unternehmensumfeld, da hier die größte Lücke zwischen den Fachbereichen und der IT besteht."
  },
  {
    "objectID": "01_VL.html#warum-bi-und-data-science-1",
    "href": "01_VL.html#warum-bi-und-data-science-1",
    "title": "Business Intelligence & Data Science",
    "section": "Warum BI und Data Science?",
    "text": "Warum BI und Data Science?\nTechnologische Kompetenzen\n\n\n\nLink zur Quelle\n\n\n\n\nDie Grafik zeigt die Ergebnisse einer Studie des Stifterverbands, die sich mit den zukünftigen Anforderungen an die Arbeitswelt befasst.\nEs handelt sich hier um eine Umfrage und die Grafik gibt den Anteil der teilnehmenden an, welche die gegeben Aspekte jetzt und in 5 Jahren als wichtig erachten.\nFür uns wichtig sind IT-Architektur und Data Analytics & KI\nDie Vorlesung wird einige Architektur-Komponenten umfassen, nämlich den Aufbau einer BI-Architektur und eine Einbettung ins Unternehmen allgemein\nData Analytics und KI umfasst die Methoden, die wir in der zweiten Hälfte der Vorlesung behandeln werden, wobei der Fokus weniger auf KI liegt und mehr auf den Data Science Methoden, die vielen KI-Technologien zu Grunde liegen"
  },
  {
    "objectID": "01_VL.html#kursaufbau",
    "href": "01_VL.html#kursaufbau",
    "title": "Business Intelligence & Data Science",
    "section": "Kursaufbau",
    "text": "Kursaufbau\nZeitplan Gruppe C\n\n\n\n\n\nBlock\nGruppe C\nThema\n\n\n\n\n1\n13.03.2024:13:00 – 14:30\nOrganisation, Einleitung\n\n\n2\n13.03.2024:14:45 – 16:15\nDatenbereitstellung: Data Warehousing\n\n\n3\n22.03.2024:09:00 – 10:30\nDatentransformation\n\n\n4\n22.03.2024:10:45 – 12:15\nBig Data und Data Lake\n\n\n5\n28.03.2024:09:00 – 10:30\nInformationsgenerierung: Berichtsorientierte Analysen\n\n\n6\n28.03.2024:10:45 – 12:15\nAdvanced und Predictive Analytics: Grundlagen\n\n\n7\n16.04.2024:13:00 – 14:30\nAdvanced und Predictive Analytics: Klassifikation\n\n\n8\n16.04.2024:14:45 – 16:15\nAdvanced und Predictive Analytics: Klassifikation\n\n\n9\n30.04.2024:13:00 – 14:30\nRestinhalte Klassifikation; Assoziationsanalyse; Probeklausur (30 Minuten)\n\n\n10\n30.04.2024:14:45 – 16:15\nBesprechung Probeklausur; Evaluation; Fragen\n\n\n-\n13.05.2024:13:00 – 14:30\nKlausur (60 Minuten)"
  },
  {
    "objectID": "01_VL.html#kursaufbau-1",
    "href": "01_VL.html#kursaufbau-1",
    "title": "Business Intelligence & Data Science",
    "section": "Kursaufbau",
    "text": "Kursaufbau\nZeitplan Gruppe D\n\n\n\n\n\nBlock\nGruppe D\nThema\n\n\n\n\n1\n21.03.2024:09:00 – 10:30\nOrganisation, Einleitung\n\n\n2\n21.03.2024:10:45 – 12:15\nDatenbereitstellung: Data Warehousing\n\n\n3\n28.03.2024:13:00 – 14:30\nDatentransformation\n\n\n4\n28.03.2024:14:45 – 16:15\nBig Data und Data Lake\n\n\n5\n02.04.2024:13:00 – 14:30\nInformationsgenerierung: Berichtsorientierte Analysen\n\n\n6\n02.04.2024:14:45 – 16:15\nAdvanced und Predictive Analytics: Grundlagen\n\n\n7\n15.04.2024:13:00 – 14:30\nAdvanced und Predictive Analytics: Klassifikation\n\n\n8\n15.04.2024:14:45 – 16:15\nAdvanced und Predictive Analytics: Klassifikation\n\n\n9\n02.05.2024:13:00 – 14:30\nRestinhalte Klassifikation; Assoziationsanalyse; Probeklausur (30 Minuten)\n\n\n10\n02.05.2024:14:45 – 16:15\nBesprechung Probeklausur; Evaluation; Fragen\n\n\n-\n13.05.2024:13:00 – 14:30\nKlausur (60 Minuten)"
  },
  {
    "objectID": "01_VL.html#kursmaterialen",
    "href": "01_VL.html#kursmaterialen",
    "title": "Business Intelligence & Data Science",
    "section": "Kursmaterialen",
    "text": "Kursmaterialen\nKurs-Website\n\nAlle Kursinhalte sind auf der Kurs-Website verfügbar:\n\nhttps://sebschroen.github.io/bi_and_ds-lecture_notes/\n\n\n\n\nAktuell passwortgeschützt aufgrund Nutzung Copyright-geschützter Materialien, daher nur Nutzung in diesem Personenkreis\n3 gefundene und gemeldete Typos (E-Mail, StudIP) = 1 Packung Bahlsen-Kekse nach Wahl\nTipp: Bookmark des Links nach Passworteingabe erübrigt künftige Eingabe des Passworts\n\n\n\n\nPasswort anschreiben\nEinfach zusammen durchklicken und kurz Zeit zur Orientierung geben, Trick zum Bookmark zeigen"
  },
  {
    "objectID": "01_VL.html#kursmaterialen-1",
    "href": "01_VL.html#kursmaterialen-1",
    "title": "Business Intelligence & Data Science",
    "section": "Kursmaterialen",
    "text": "Kursmaterialen\nFolien\n\n\nFolien sind auf der Startseite der Kurs-Website verlinkt\nDarstellung ist für den Browser (Chrome, Safari und Firefox) optimiert, um interaktive Elemente darzustellen\nIm Burger-Menü oben rechts lässt sich zu jeder Zeit auch eine PDF Version zum Ausdruck in Papierform oder für Notizen erstellen:\n\nTools -&gt; PDF Export Mode -&gt; Strg + P (Cmd + P) -&gt; Druck als PDF\n\nDie Folien werden rechtzeitig vor der Vorlesung als PDF auf StudIP hochgeladen\nInteraktive Elemente werden separat verlinkt.\n\n\n\n\nDie Folien sind im Ablaufplan auf der Startseite in aktueller Fassung verlinkt und lassen sich auf mehreren Wegen darstellen.\n\n\nKonzipiert sind die Folien für die Darstellung im Browser, um interaktive Elemente darzustellen.\nIm Menü oben rechts lässt sich zu jeder Zeit auch eine PDF Version zum Ausdrucken in Papierform erstellen oder zur Darstellung auf Endgeräten, auf denen Sie Notizen machen möchten.\nAm Ende jeder Vorlesung wird die finale Fassung der Folien auf StudIP im PDF Format hochgeladen.\nDie HTML Fassung bleibt verfügbar."
  },
  {
    "objectID": "01_VL.html#kursmaterialen-2",
    "href": "01_VL.html#kursmaterialen-2",
    "title": "Business Intelligence & Data Science",
    "section": "Kursmaterialen",
    "text": "Kursmaterialen\nPDF-Skript\n\nNeben der HTML-Version ist auch ein PDF-Skript verfügbar und kann auf der Startseite heruntergeladen werden\nDie PDF-Version entspricht inhaltlich immer der Website\nDie Darstellung ist für HTML optimiert und kann für Artefakte beim PDF Rendering sorgen, erleichtert aber das Ausdrucken\n\n\n\nDie PDF-Version zum Skript ist auf der Startseite der Website verlinkt und entspricht inhaltlich immer der Website.\nDie Darstellung ist für html optimiert und kann für Artefakte beim PDF Rendering sorgen, erleichtert aber das Ausdrucken"
  },
  {
    "objectID": "01_VL.html#kursmaterialien",
    "href": "01_VL.html#kursmaterialien",
    "title": "Business Intelligence & Data Science",
    "section": "Kursmaterialien",
    "text": "Kursmaterialien\nErgänzende Literatur\n\n\n\nAlle klausurrelevanten Inhalte lassen sich auf der Kurs-Website finden und nachlesen, zusätzliche Literatur ist nicht notwendig\nDer Aufbau des Kurses richtet sich nach dem Lehrbuch Business Intelligence & Analytics - Grundlagen und praktische Anwendungen, 4. Auflage von Henning Baars und Hans-Georg Kemper\nDas Buch ist über die Bibliothek der Leibniz FH als E-Book verfügbar\nBlock 1 - 5 entstammen größtenteils in Baars und Kemper (2021)\nMethodische Aspekte rund um Predictive Analytics entstammen größtenteils dem frei verfügbaren Introduction to Statistical Learning, 2. Auflage von Gareth James, Daniela Witten, Trevor Hastie und Robert Tibshirani\nDie Quellen zu jeder Vorlesung sind jeweils auf der letzten Folien angegeben."
  },
  {
    "objectID": "01_VL.html#business-intelligence",
    "href": "01_VL.html#business-intelligence",
    "title": "Business Intelligence & Data Science",
    "section": "Business Intelligence",
    "text": "Business Intelligence\nBegriffsabgrenzung\n\n\n\n\n\n\nDefinition\n\n\nBusiness Intelligence (BI) ist eine Reihe von Architekturen und Technologien, die Rohdaten in sinnvolle und nutzbare, entscheidungsrelevante Informationen umwandeln. Es ermöglicht Anwendenden, informierte Entscheidungen auf der Grundlage von Daten zu treffen, die ein Unternehmen gegenüber seinen Wettbewerbern in Vorteil bringen können (siehe Forrester.com).\n\n\n\n\n\nAbgeleitet vom Intelligence-Begriff in der militärischen Informationsverarbeitung Großbritanniens im 2. Weltkrieg:\n\nDie richtigen Informationen zur richtigen Zeit an die richtigen Personen.\n\nFrühe kommerzielle Ansätze in den 60er Jahren im Zuge der Entwicklung relationaler Datenbanken."
  },
  {
    "objectID": "01_VL.html#business-intelligence-1",
    "href": "01_VL.html#business-intelligence-1",
    "title": "Business Intelligence & Data Science",
    "section": "Business Intelligence",
    "text": "Business Intelligence\nBegriffsabgrenzung\n\n\nZunächst Fokus auf Management Support Systeme (MSS) und daher eher auf oberste Ebenen zugeschnitten\nDer Begriff Business Intelligence (BI) wurde in den 1990ern geprägt\nHeute wird BI laut Gartner Group charakterisiert durch:\n\nBreite Verfügbarkeit von BI Tools auf allen Ebenen des Unternehmens\nGeschäftsentscheidung auf Basis aktueller Informationen und Daten und nicht auf Intuition\nUmfangreiche Analyse- und Reportingmöglichkeiten mit Self-Service Tools für Fachbereiche"
  },
  {
    "objectID": "01_VL.html#business-intelligence-2",
    "href": "01_VL.html#business-intelligence-2",
    "title": "Business Intelligence & Data Science",
    "section": "Business Intelligence",
    "text": "Business Intelligence\nBetriebliche Dimensionen\n\n\n\nBegriffsdimensionen von BI nach Schieder (2016).\n\n\n\n\nEine Personengruppe innerhalb der Organisation ist mit der Realisierung von BI-Prozessen vertraut.\nDie Generierung geschäftsrelevanter Informationen, Erkenntnisse und Wissen erfordert die Überführung fragmentierter Unternehmens- und Wettbewerbsdaten in handlungsgerichtetes Wissen. Hierbei liegt der Fokus auf dem Geschäftsprozess von der Datenerfassung hin zur Wissenskommunnikation.\nBezeichnet das Ergebnis eines Erkenntnissprozesses, beispielsweise ziel- und zweckorientiertes Wissen in Form von Berichten, Analysen und Prognosen für das Management.\nEine Sammlung von informationstechnischen Werkzeugen, Architekturen, Systemen und Technologien zur Aufbereitung und Bereitstellung reschäftsrelevanter Daten zum Zweck der Informationsgewinnung.\n\nIm Mittelpunkt dieser Vorlesung steht der BI-Prozess, beginnend mit der Datenerfassung und -bereitstellung im unternehmenseigenen Data Warehouse, über die Informationsentdeckung bzw. -generierung mit modellgestützten Analysemethoden, bis hin zur Kommunikation. Jeder Bestandteil des BI-Prozesses wird dabei um wichtige technische Aspekte ergänzt, bspw. Data Warehouse Architekturen, ausgewählten Analysemethoden und Einblicken in moderne BI-Dashboard-Tools. Ziel ist eine ganzheitliche Sicht auf den BI-Prozess im Unternehmenskontext."
  },
  {
    "objectID": "01_VL.html#data-science",
    "href": "01_VL.html#data-science",
    "title": "Business Intelligence & Data Science",
    "section": "Data Science",
    "text": "Data Science\nBegriffsabgrenzung\n\n\n\n\n\n\nDefinition\n\n\nData Science beschäftigt sich mit einer zweckorientierten Datenanalyse und der systematischen Generierung von Entscheidungshilfen und -grundlagen, um Wettbewerbsvorteile erzielen zu können. Der Schwerpunkt liegt dabei nicht auf den Daten selbst, sondern auf der Art und weise, wie diese verarbeitet und analysiert werden (siehe Gesellschaft für Informatik 2019).\n\n\n\n\n\nData Science ist ein vergleichsweise neues wissenschaftliches Feld, eine Kombination aus Statistik und Informatik, insbesondere Software Engineering\nDa es sich um ein junges Feld handelt sind Definitionen und die damit verbundenen Rollen im stetigen Wandel\n\n\n\nDieser Wandel geht so weit, dass die Rolle “Data Scientist” heute seltener auf Job-Portalen zu finden ist, als noch vor einigen Jahren. Stattdessen werden vermehrt spezialisierte Rollen wie “Data Engineer”, “Data Analyst” oder “Machine Learning Engineer” ausgeschrieben."
  },
  {
    "objectID": "01_VL.html#data-science-1",
    "href": "01_VL.html#data-science-1",
    "title": "Business Intelligence & Data Science",
    "section": "Data Science",
    "text": "Data Science\nSchwerpunkte\n\nAufgrund der potentiellen Breite des Felds erfolgt oft eine genauere Aufteilung in vier Kernbereiche:\n\n\n\nData Engineering: Methoden und Prozesse für die Speicherung, Haltung und Replikation von Daten\nData Analytics: Datenanalyse mit statistischen Methoden\nPredictive Modelling: Die Verwendung von statistischen Methoden zur Vorhersage\nMachine Learning: Algorithmen, die aus Daten lernen, Muster erkennen und hierauf aufbauend neue Situationen oder zukünftige Entwicklungen vorhersagen"
  },
  {
    "objectID": "01_VL.html#data-science-2",
    "href": "01_VL.html#data-science-2",
    "title": "Business Intelligence & Data Science",
    "section": "Data Science",
    "text": "Data Science\nReifegrade von Data Analytics\n\n\n\nQuelle: Milind Desai on Medium.com\n\n\n\n\nDescriptive Analytics: Beschreibende Analyse des Ist-Zustandes und der Vergangenheit. Im Mittelpunkt steht die Frage: Was ist passiert?\nDiagnostic Analytics: Analysiert die Zusammenhänge, die zum Ist-Zustand geführt haben und führt oft mehrere deskriptive Charakteristika zusammen: Warum ist der Status Quo eingetreten?\nPredictive Modelling: Ausgehend von einem detaillierten Verständnis des Status Quo wird eine Prognose für die Zukunft erstellt: Was wird passieren?\nPrescriptive Analytics: Dient der Identifizierung möglicher Handlungsoptionen auf Basis der Prognosen, entweder um eine schlechte Prognose abzuwenden oder die Realisierung einer guten Prognose zu unterstützen. Mit anderen Worten: Was muss getan werden, um die Zukunft zu unseren Gunsten zu beeinflussen"
  },
  {
    "objectID": "01_VL.html#data-science-3",
    "href": "01_VL.html#data-science-3",
    "title": "Business Intelligence & Data Science",
    "section": "Data Science",
    "text": "Data Science\nData Science, Data Analytics, Data Mining?\n\n\nDie Unterscheidung zwischen den Begriffen Data Analytics, Data Mining und Data Science ist nicht immer trennscharf\nData Mining ist meist definiert als der Prozess der Informationsextraktion aus Daten und ist ebenso wie Data Analytics eine Teilmenge von Data Science\nIn dieser Vorlesung dient Data Science als methodischer Baukasten, um den BI-Prozess mit modellgestützten Methoden anzureichern und Zusammenhänge sichtbar zu machen\nHierbei steht der Zweck der Modelle, nämlich die Entscheidungsunterstützung, im Vordergrund"
  },
  {
    "objectID": "01_VL.html#business-intelligence-und-data-science",
    "href": "01_VL.html#business-intelligence-und-data-science",
    "title": "Business Intelligence & Data Science",
    "section": "Business Intelligence und Data Science",
    "text": "Business Intelligence und Data Science\nZusammenführung der Begriffe und inhaltlicher Aufbau der Vorlesung\n\n\n\nBIA Gesamtansatz. Eigene Darstellung in Anlehnung an Baars und Kemper (2021).\n\n\n\nDie Abbildung illustriert den BIA-Ansatz als dreiteiligen Ordnungsrahmen, bestehend aus Datenbereitstellung, Informationsgenerierung und Informationsbereitstellung. Die Datenerfassung aus operativen und externen Systemen ist diesem Ordnungsrahmen hier vorgelagert. Das hieraus entstehende Modell entspricht der prozessualen BI-Dimension.\nGanz unten startenw ir mit zahlreichen operative und externen Quellsysteme, beispielsweise ERP-Systeme (häufig SAP), Produktdatenmanagement Systeme (PDM) oder Manufacturing Execution Systeme (MES).\nHinzu kommen häufig offene Daten wie Wetter- oder Konjunkturdaten und insbesondere im industriellen Kontext verstärkt Sensordaten aus internetfähigen Maschinen, sogenannte Internet of Things (IoT) Devices. Die strukturierte und systematische Integration dieser Daten mittels ETL Methoden (Extract, Transfer, Load), ist die erste Herausforderung jedes integrierten BIA-Systems.\nDie sogenannte Datenbereitstellung dient der konsistenten und strukturierten Speicherung und Persistierung aller relevanten Daten aus den oben genannten Quellsystemen. Hier gilt es verschiedene Konzepte näher zu beleuchten, insbesondere gängige Data Warehouse Konzepte, die meist aus sogenannten Data Marts und Core Data Warehouses bestehen und der themenbezogenen und integrierten Datenhaltung dienen. Je nach Anwendung wird das Datenmaterial meist voraggregiert. Zur Integration großvolumiger und schnell einlaufender Daten hat sich ergänzend das Konzept eines Data Lakes etabliert, in dem anders als im Data Warehouse Rohdaten ohne Aggregation abgelegt und verfügbar gemacht werden.\nDie Informationsgenerierung als zweite Schicht dient der Umwandlung der Rohdaten in entscheidungsfreundliche Formate, bspw. berichtsorientierte oder modellgestützte Analysen. Hier werden aus Daten erste Informationen generiert, auf Basis derer weitere Erkenntnisse über den Status Quo entstehen und mögliche Prognosen für die Zukunft erstellt werden können. Das Bindeglied zwischen Datenbereitstellung und Informationsgenerierung sind Systeme zur Datenabfrage und Exploration.\nDie Darstellung, Kanalisierung und Verbreitung von Informationen folgt in der dritten Schicht, der Informationsbereitstellung. Neben modernen Self-Service BI-Tools umfasst dies auch zielgruppenadäquate Präsentationen oder statische Berichte."
  },
  {
    "objectID": "01_VL.html#dispositive-und-operative-daten",
    "href": "01_VL.html#dispositive-und-operative-daten",
    "title": "Business Intelligence & Data Science",
    "section": "Dispositive und operative Daten",
    "text": "Dispositive und operative Daten\nOperative versus dispositive Aufgaben\n\n\nAnders als die anfänglichen MSS unterstützen moderne BI-Systeme sowohl operative, als auch dispositive Aufgaben\nDispositive Aufgaben sind Leitungs- und Lenkungstätigkeiten im betrieblichen Ablauf\nOperative Aufgaben umfassen die Leistungserstellung oder -verwertung\nAn beide Aufgabenfelder gelten unterschiedliche Anforderungen, die in Daten und Systemen abgebildet werden müssen"
  },
  {
    "objectID": "01_VL.html#dispositive-und-operative-daten-1",
    "href": "01_VL.html#dispositive-und-operative-daten-1",
    "title": "Business Intelligence & Data Science",
    "section": "Dispositive und operative Daten",
    "text": "Dispositive und operative Daten\nOperative versus dispositive Daten\n\n\n\n\nDispositive Daten\n\nUnterstützen Leitungs- und Lenkungstätigkeiten im betrieblichen Ablauf\nHäufig verdichtet, transformiert und themenbezogen aufbereitet und mit Historie angereichert\n\n\nOperative Daten\n\nDienen der Abwicklung von Geschäftsprozessen und werden im Rahmen von Transaktionen1 erzeugt\nSehr granular und mit hoher Änderungsrate\nBeispiele sind Bestellungen, Aufträge und Lagerbestände oder Stammdaten\n\n\n\n\n\nAtomare und logisch untrennbare Datenbankvorgänge"
  },
  {
    "objectID": "01_VL.html#dispositive-und-operative-daten-2",
    "href": "01_VL.html#dispositive-und-operative-daten-2",
    "title": "Business Intelligence & Data Science",
    "section": "Dispositive und operative Daten",
    "text": "Dispositive und operative Daten\nOperative versus dispositive Daten\n\n\nCharakteristika operativer und entscheidungsorientierter Daten im Vergleich. In Anlehnung an Baars und Kemper (2021)\n\n\n\n\n\n\n\n\nOperative Daten\nEntscheidungsorientierte Daten\n\n\n\n\nZiel\nAbwicklung der Geschäftsprozesse\nInformationen für Entscheidungen\n\n\nAusrichtung\nDetailliert und granular\nMeist verdichtet und transformiert mit Metadaten\n\n\nZeitbezug\nAktualität steht im Vordergrund, Zeitpunkt der Transaktion, keine Historisierung\nAktualität variiert mit der Aufgabe, Historienbetrachtung ist möglich\n\n\nModellierung\nKeine Altbestände\nSachgebiets- und themenbezogen orientiert und anwendungstauglich\n\n\nZustand\nHäufig redundant und inkonsistent zwischen Systemen\nKonsistent modelliert, Redundanz bewusst\n\n\nUpdate\nLaufend, Real-time\nErgänzend als Fortschreibung\n\n\nQueries\nStrukturiert, standardisiert und meistens statisch\nAd-hoc und dynamisch für wechselnde Fragestellungen sowie Standardberichte"
  },
  {
    "objectID": "01_VL.html#dispositive-und-operative-daten-3",
    "href": "01_VL.html#dispositive-und-operative-daten-3",
    "title": "Business Intelligence & Data Science",
    "section": "Dispositive und operative Daten",
    "text": "Dispositive und operative Daten\nÜberführung von Daten in Information\n\n\n\nHauptziel dieser Vorlesung ist die Überführung von operationalen Daten in entscheidungsrelevante Informationen\nDies hat zwei Hauptaspekte:\n\nBlock 1-4: Technische Infrastruktur und Architektur\nBlock 5-9: Methodische Konzepte wie modellorientierte Analysen\n\n\n\n\n\n\n\n\nBlock\nThema\n\n\n\n\n1\nOrganisation, Einleitung\n\n\n2\nDatenbereitstellung: Data Warehousing\n\n\n3\nDatentransformation\n\n\n4\nBig Data und Data Lake\n\n\n5\nInformationsgenerierung: Berichtsorientierte Analysen\n\n\n6\nAdvanced und Predictive Analytics: Grundlagen\n\n\n7\nAdvanced und Predictive Analytics: Klassifikation\n\n\n8\nAdvanced und Predictive Analytics: Klassifikation\n\n\n9\nRestinhalte Klassifikation; Assoziationsanalyse; Probeklausur (30 Minuten)"
  },
  {
    "objectID": "01_VL.html#quellen",
    "href": "01_VL.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nBaars, Henning, und Hans-Georg Kemper. 2021. Business Intelligence & Analytics: Grundlagen und praktische Anwendungen: Ansätze der IT-basierten Entscheidungsunterstützung. 4., überarbeitete und erweiterte Auflage. Lehrbuch. Wiesbaden [Heidelberg]: Springer Vieweg.\n\n\nGesellschaft für Informatik. 2019. „Data Science: Lern- und Ausbildungsinhalte“. Gesellschaft für Informatik.\n\n\nSchieder, Christian. 2016. „Historische Fragmente einer Integrationsdisziplin – Beitrag zur Konstruktgeschichte der Business Intelligence“. In Analytische Informationssysteme, herausgegeben von Peter Gluchowski und Peter Chamoni, 13–32. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-47763-2_2.\n\n\nSharma, Vinod, Jeanne Poulose, und Chandan Maheshkar. 2023. „Analytics Enabled Decision Making ‚Tracing the Journey from Data to Decisions‘“. In Analytics Enabled Decision Making, herausgegeben von Vinod Sharma, Chandan Maheshkar, und Jeanne Poulose, 1–22. Singapore: Springer Nature Singapore. https://doi.org/10.1007/978-981-19-9658-0_1.\n\n\nWinter, Robert. 2016. „Analytische Informationssysteme aus Managementsicht: lokale Entscheidungsunterstützung vs. unternehmensweite Informations-Infrastruktur“. In Analytische Informationssysteme, herausgegeben von Peter Gluchowski und Peter Chamoni, 67–95. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-47763-2_5."
  },
  {
    "objectID": "05_VL_D.html#der-plan-für-heute",
    "href": "05_VL_D.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung\n\n\nQuiz und Recap\nInformationsgenerierung\n\nBeispiel für OLAP Report\nWo genau unterscheiden sich Self-Sevice BI und traditionelle BI?\n\nModellgestützte Analysen\n\nWie läuft ein Data Science Projekt ab?\nBegriffliche Grundlagen Data Science"
  },
  {
    "objectID": "05_VL_D.html#kurzer-überblick-wo-gehts-weiter",
    "href": "05_VL_D.html#kurzer-überblick-wo-gehts-weiter",
    "title": "Business Intelligence & Data Science",
    "section": "Kurzer Überblick: Wo geht’s weiter?",
    "text": "Kurzer Überblick: Wo geht’s weiter?\nInformationsgenerierung\n\n\n\nBIA Gesamtansatz. Eigene Darstellung in Anlehnung an Baars und Kemper (2021)."
  },
  {
    "objectID": "05_VL_D.html#self-service-bi",
    "href": "05_VL_D.html#self-service-bi",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI",
    "text": "Self-Service BI\nDas Versprechen von Self-Service BI\n\nSelf Service BI setzt beim Versprechen an, die Anwendenden in die Lage zu versetzen eigenständig und flexibel den Datenbestand nach neuen Verknüpfungen zu untersuchen\nDas kann zudem weitgehend unabhängig von der Unternehmens-IT betrieben und bedient werden\nTraditionelle BI-Lösungen sind oft als zu starr angesehen, wenn es darum geht, neue Reporting Anforderungen rasch umzusetzen und damit innovative Analysen zu testen"
  },
  {
    "objectID": "05_VL_D.html#self-service-bi-vs.-traditionelle-bi",
    "href": "05_VL_D.html#self-service-bi-vs.-traditionelle-bi",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI vs. Traditionelle BI",
    "text": "Self-Service BI vs. Traditionelle BI\nBlick auf die alte Welt\n\n\n\nTraditionelle BI, Quelle: Fidelity"
  },
  {
    "objectID": "05_VL_D.html#self-service-bi-vs.-traditionelle-bi-1",
    "href": "05_VL_D.html#self-service-bi-vs.-traditionelle-bi-1",
    "title": "Business Intelligence & Data Science",
    "section": "Self-Service BI vs. Traditionelle BI",
    "text": "Self-Service BI vs. Traditionelle BI\nVersprechen der neuen Welt am Beispiel von Looker\n\n\n\nTraditionelle BI, Quelle: Fidelity"
  },
  {
    "objectID": "05_VL_D.html#der-data-science-prozess",
    "href": "05_VL_D.html#der-data-science-prozess",
    "title": "Business Intelligence & Data Science",
    "section": "Der Data Science Prozess",
    "text": "Der Data Science Prozess\nAblauf von Data Science Use Cases\n\n\n\n\nData Science Projekte sind in der Praxis aufwendig und erfordern stets die Zusammenarbeit zwischen Fachabteilungen und Data Science Team\nDeshalb wurden feste Prozesse etabliert, bspw. der Cross Reference Industry Standard Process for Data Mining (CRISP-DM)\nCRISP-DM besteht aus 6 Phasen, die iterativ durchlaufen werden\nRückkopplungen zwischen den Phasen sind dabei oft notwendig\n\n\n\n\n\n\nCRISP-DM Prozess. Eigene Darstellung in Anlehnung an Meier (2021)."
  },
  {
    "objectID": "05_VL_D.html#crisp-dm",
    "href": "05_VL_D.html#crisp-dm",
    "title": "Business Intelligence & Data Science",
    "section": "CRISP-DM",
    "text": "CRISP-DM\nGeschäftsmodell\n\nVerständnis des Geschäftsmodells\n\nVerständnis des Geschäftsmodells und der Unternehmensziele\nBerücksichtigung von Chancen, Risiken und zeitlichen Aspekten\nDefinition des erwartbaren Nutzens und messbarer Erfolgskriterien"
  },
  {
    "objectID": "05_VL_D.html#crisp-dm-1",
    "href": "05_VL_D.html#crisp-dm-1",
    "title": "Business Intelligence & Data Science",
    "section": "CRISP-DM",
    "text": "CRISP-DM\nAnwendungs-/Datendomäne und Datenvorbereitung\n\nVerständnis der Anwendungs- und Datendomäne\n\nAnalyse der Unternehmensprozesse und Datenquellen\nZusammenführung, Beschreibung und Exploration der Zieldaten\nEruierung der Datenqualität\n\nDatenvorbereitung\n\nZusammenführung und Beschreibung polystrukturierter Daten\nBerechnung von Kennzahlen und Durchführung von Datentransformationen"
  },
  {
    "objectID": "05_VL_D.html#crisp-dm-2",
    "href": "05_VL_D.html#crisp-dm-2",
    "title": "Business Intelligence & Data Science",
    "section": "CRISP-DM",
    "text": "CRISP-DM\nModellierung, Evaluation und Bereitstellung\n\nModellierung\n\nModellauswahl und -erstellung\nIterativer Prozess zur Weiterentwicklung von Modellen\nMöglicher Einsatz vortrainierter Modelle für die Verfeinerung\n\nEvaluation\n\nBewertung der erstellten Modelle anhand definierter Erfolgskriterien\n\nEinsatz\n\nUmsetzung der Ergebnisse in die Praxis und Integration in die Unternehmensprozesse"
  },
  {
    "objectID": "05_VL_D.html#begriffliche-grundlagen",
    "href": "05_VL_D.html#begriffliche-grundlagen",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nEine Gleichung, viele Anwendungen\n\n\nDie Generierung von modellbasierten Erkenntnissen aus Daten erfordert eine Definition des Lernproblems, oft mit einer simplen Gleichung:\n\n\n\\[\ny = f(X) + \\epsilon\n\\]\n\n\n\\(y\\) ist ein \\(N \\times 1\\) Vektor mit einer Ergebnisvariablen\n\\(X\\) ist eine \\(N \\times P\\) Matrix mit Prädiktoren \\(X_1, X_2,..., X_P\\)\n\\(N\\) entspricht der Anzahl von Beobachtungen im vorliegenden Datensatz, \\(P\\) ist die Anzahl der Prädiktoren\nAnstelle von Prädiktoren werden oft die Begriffe erklärende oder unabhängige Variablen oder Features verwendet\n\\(f\\) beschreibt alle systematischen Zusammenhänge zwischen \\(X\\) und \\(y\\), während der Fehlerterm \\(\\epsilon\\) alle Variation in \\(X\\) aufnimmt, die nicht von \\(f\\) erklärbar sind"
  },
  {
    "objectID": "05_VL_D.html#begriffliche-grundlagen-1",
    "href": "05_VL_D.html#begriffliche-grundlagen-1",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nVorhersage vs. Inferenz\n\nAngenommen, wir haben eine Funktion \\(f\\) sowie einen Algorithmus gefunden, um \\(f\\) an unsere Variablen \\(y\\) und \\(X\\) anzupassen\nDie Modellschätzung wird auch als Modelltraining oder Modellanpassung bezeichnet und unser trainiertes Modell hat die Form:\n\n\n\\[\n\\hat{y} = \\hat{f}(X)\n\\]\n\n\nDas trainierte Modell hat keinen Fehlerterm \\(\\epsilon\\), da dieser die Variation in den Daten darstellt, die vom Modell nicht erfasst wird und unvorhersehbar ist\nGeschätzte Größen werden mit einem Zirkumflex-Symbol (^) gekennzeichnet"
  },
  {
    "objectID": "05_VL_D.html#begriffliche-grundlagen-2",
    "href": "05_VL_D.html#begriffliche-grundlagen-2",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nVorhersage vs. Inferenz\n\nDas Modell kann nun für zwei Zwecke verwendet werden:\n\nVorhersage: Schätzung von \\(y\\) für neue, nicht im Modell enthaltene Daten\nInferenz: Verständnis der Beziehungen und Muster in den Daten, die das Modell gelernt hat\n\nInferenz erfordert ein genaues Verständnis der Zusammenhänge\nBei Vorhersage kümmern wir uns nicht um seine genaue Struktur oder Parameter des Modells, sondern ausschließlich um die generierten Vorhersagen."
  },
  {
    "objectID": "05_VL_D.html#begriffliche-grundlagen-3",
    "href": "05_VL_D.html#begriffliche-grundlagen-3",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nKlassifikation vs. Regression\n\nDie Art der Variablen \\(y\\) bestimmt die Art des Modells, das wir verwenden\nVariablen sind entweder kategorisch oder numerisch, oft auch als qualitativ und quantitativ bezeichnet\nWenn die abhängige Variable numerisch ist, ist es möglich, den exakten Wert der Variable vorherzusagen, dann liegt ein Regressionsproblem vor\nWenn die abhängige Variable kategorisch ist, lässt sich lediglich die erwartete Klasse vorhersagen, basierend auf einer Wahrscheinlichkeit, dann liegt ein Klassifikationsproblem vor\nWichtig: Hierbei ist nur die Art der abhängigen Variable entscheidend"
  },
  {
    "objectID": "05_VL_D.html#begriffliche-grundlagen-4",
    "href": "05_VL_D.html#begriffliche-grundlagen-4",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nKategoriale Daten\n\nKategoriale Daten umfassen verschiedene Kategorien oder Labels\nBei kategorialen Daten wird wiederum zwischen nominalen und ordinalen Variablen unterschieden\nNominaldaten repräsentieren Kategorien ohne eine inhärente Rangfolge:\n\nFarben (rot, blau, grün),\nFamilienstand (ledig, verheiratet, geschieden, …),\n\nOrdinaldaten weisen eine spezifische Reihenfolge oder Hierarchie auf, aber keine gleichmäßigen Abstände zwischen den Kategorien:\n\nSchärfe von Essen (mild, pikant, scharf),\nExpertise mit Programmiersprachen (keine, wenig, fortgeschritten, professionell)"
  },
  {
    "objectID": "05_VL_D.html#begriffliche-grundlagen-5",
    "href": "05_VL_D.html#begriffliche-grundlagen-5",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nNumerische Daten\n\n\n\nNumerische Daten reräsentieren Mengen, Messungen, und allgemein numerische Werte\nLassen sich mit mathematichen Operationen verarbeiten und manipulieren\nNumerische Daten können entweder diskret oder kontinuierlich sein, Beispiele:\n\nAlter,\nTemperatur,\nFinanzwerte ($ oder €)\n\n\n\n\n\n\n\nStetig vs. Diskret, Quelle: Allison Horst"
  },
  {
    "objectID": "05_VL_D.html#begriffliche-grundlagen-6",
    "href": "05_VL_D.html#begriffliche-grundlagen-6",
    "title": "Business Intelligence & Data Science",
    "section": "Begriffliche Grundlagen",
    "text": "Begriffliche Grundlagen\nÜberwachtes vs. Unüberwachtes Lernen\n\nÜberwachte Lernprobleme weisen eine mess- oder beobachtbare abhängige Variable \\(y_i, i = 1,2,...N\\) für jede Beobachtung \\(i\\) auf sowie eine oder mehrere Prädiktoren\nUnüberwachte Lernprobleme hingegen haben keine abhängige Variable, sondern versuchen, Muster in den Daten zu finden\nDie Mehrheit der Data Science Projekte sind überwacht, da sie auf der Vorhersage von \\(y\\) basieren\nAuch wir konzentrieren uns bei der Klassifikation auf überwachte Lernprobleme"
  },
  {
    "objectID": "05_VL_D.html#quellen",
    "href": "05_VL_D.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nBaars, Henning, und Hans-Georg Kemper. 2021. Business Intelligence & Analytics: Grundlagen und praktische Anwendungen: Ansätze der IT-basierten Entscheidungsunterstützung. 4., überarbeitete und erweiterte Auflage. Lehrbuch. Wiesbaden [Heidelberg]: Springer Vieweg.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, und Robert Tibshirani. 2021. An Introduction to Statistical Learning: with Applications in R. Second edition. Springer texts in statistics. New York NY: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nMeier, Andreas. 2021. „Rundgang Big Data Analytics – Hard & Soft Data Mining“. In Big Data Analytics, herausgegeben von Sara D’Onofrio und Andreas Meier, 3–23. Wiesbaden: Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-32236-6_1."
  },
  {
    "objectID": "06_VL_D.html#der-plan-für-heute",
    "href": "06_VL_D.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung"
  },
  {
    "objectID": "06_VL_D.html#der-plan-für-heute-1",
    "href": "06_VL_D.html#der-plan-für-heute-1",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung\n\nNeue Daten von Tofispy\nVisualisierung von Verteilungen\nModellgestützte Analysen\n\nEinfache Logistische Regression\nModellevaluation\nMultivariate Logistische Regression\n\nWie gut kann man Musik-Genres klassifizieren?"
  },
  {
    "objectID": "06_VL_D.html#business-case",
    "href": "06_VL_D.html#business-case",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nRecap Tofispy\n\nUnsere bisherigen Analysen haben gezeigt:\n\nTofispy verliert Marktanteile gegenüber der Konkurrenz, insbesondere gegenüber Youtube Music\nDer Grund ist vorallem ein stark ansteigender Trend bei den Kündigungen, während die Registrierungen linear wachsen\nEine Befragung nach der Kündigung hat ergeben:\n\nDie Mehrheit hört Musik über Playlists (55.8%)\nDer Großteil ist unzufrieden mit den Empfehlungen (69.2%)\n\n\nHandlungsempfehlung: Verbesserung der empfohlenen Playlists"
  },
  {
    "objectID": "06_VL_D.html#business-case-1",
    "href": "06_VL_D.html#business-case-1",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nNeue Daten von Tofispy\n\n\nZum Einstieg in die Empfehlungsverbesserung hat Tofispy neue Daten bereitgestellt\nDie Daten enthalten eine Auswahl an Songs aus den beiden Genres Klassik und Dance/Electronic (EDM)\nDie Daten enthalten:\n\nSong ID, Name und Artists,\nGenre als Label, manuell erstellt\nFeatures:\n\nTempo (BPM)\nDanceability (0-1), beschreibt wie gut der Song zum Tanzen geeignet ist\nEnergy (0-1), beschreibt wie energiegeladen der Song ist, wobei energiegeladene Songs schnell, intensiv und laut sind"
  },
  {
    "objectID": "06_VL_D.html#business-case-2",
    "href": "06_VL_D.html#business-case-2",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nNeue Daten von Tofispy\n\nDie Daten sind via Superset zur Verfügung gestellt und befinden sich im Schema “Classification” und Dataset “training_logistic”\nDie Daten sind bereits bereinigt und enthalten keine fehlenden Werte\nTofispy generiert jeden Freitag – pünktlich zum Wochenende – eine neue EDM Playlist mit den neuen Releases der Woche und bittet uns, die Klassifizierung für die nächste Playlist zu erstellen\nErste Frage: Wie gut sind die Daten geeignet, um zwischen den beiden Genres zu unterscheiden?"
  },
  {
    "objectID": "06_VL_D.html#visualisierung-von-verteilungen",
    "href": "06_VL_D.html#visualisierung-von-verteilungen",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nHistogramm\n\n\n\n\nEinfache Möglichkeit, die Verteilung einer numerischen Variable zu visualisieren\nDie einzelnen Werte der entsprechenden Variable werden in sogenannte Bins gruppiert\nDie Anzahl der Werte in jedem Bin wird gezählt und als Balken dargestellt\nDie Breite der Balken entspricht der Breite der Bins und die Höhe der Balken entspricht der Anzahl der Werte pro Bin\nDie Anzahl der Bins ist wichtig, zu wenige verschleiern den Detailgrad der Verteilung, zu viele Bins können zu zu granular sein\nViele Tools optimieren die Bin-Zahl, oft aber Trial & Error"
  },
  {
    "objectID": "06_VL_D.html#visualisierung-von-verteilungen-1",
    "href": "06_VL_D.html#visualisierung-von-verteilungen-1",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nHistogramm bei mehreren Dimensionen\n\n\n\n\nHistogramme sind ideal, um einzelne Verteilung zu visualisieren\nBei mehreren Dimensionen und ähnlichen Verteilungen drohen sich die Verteilungen zu überlagern\nHistogramme sind nicht mehr aussagekräftig\nIn diesen Fällen sind mehrere Histogramme zu empfehlen\nHierbei auf passende X-Achsen achten, um Vergleiche zu erleichtern"
  },
  {
    "objectID": "06_VL_D.html#visualisierung-von-verteilungen-2",
    "href": "06_VL_D.html#visualisierung-von-verteilungen-2",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nBox-Plot\n\nBox Plots sind eine weitere Möglichkeit, die Verteilung einer numerischen Variable über verschiedene Gruppen hinweg zu visualisieren\nErmöglichen es, viele Gruppen gleichzeitig zu visualisieren und zu vergleichen\nBox-Plots sind einfach, aber hochinformativ, wurden in den 1970er Jahren von John Tukey entwickelt und gewannen schnell an Popularität, da sie sich einfach per Hand zeichnen ließen\nBox-Plots werden oft auch als Box-and-Whisker-Plots bezeichnet"
  },
  {
    "objectID": "06_VL_D.html#visualisierung-von-verteilungen-3",
    "href": "06_VL_D.html#visualisierung-von-verteilungen-3",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nBox-Plot\n\n\n\n\nDie Punktewolke illustriert die Verteilung der Y-Werte der Rohdaten\nDie Linie in der Mitte des Box-Plots repräsentiert den Median, und die Box umschließt die mittleren 50% der Daten\nDie Obergrenze (Untergrenze) der Box ist damit das obere bzw. untere Quartil\nDie oberen und unteren sogenannten Whisker erstrecken sich meist bis zum Maximum und Minimum der Daten\nAlternativ entsprechen die Whisker das 1,5 fache des Interquartilabstands (IQR)\n\nEinzelne Datenpunkte, die über die Grenzen hinausgehen sind Ausreißer bezeichnet und werden als einzelne Punkte dargestellt\n\n\n\n\n\n\nAnatomie eines Boxplots Quelle: Wilke (2019)"
  },
  {
    "objectID": "06_VL_D.html#visualisierung-von-verteilungen-4",
    "href": "06_VL_D.html#visualisierung-von-verteilungen-4",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nBox-Plot\n\n\n\n\n\nDie Punktewolke illustriert die Verteilung der Y-Werte der Rohdaten\nDie Linie in der Mitte des Box-Plots repräsentiert den Median, und die Box umschließt die mittleren 50% der Daten\nDie Obergrenze (Untergrenze) der Box ist damit das obere bzw. untere Quartil\nDie oberen und unteren sogenannten Whisker erstrecken sich meist bis zum Maximum und Minimum der Daten\nAlternativ entsprechen die Whisker das 1,5 fache des Interquartilabstands (IQR)\n\nEinzelne Datenpunkte, die über die Grenzen hinausgehen sind Ausreißer bezeichnet und werden als einzelne Punkte dargestellt"
  },
  {
    "objectID": "06_VL_D.html#visualisierung-von-verteilungen-5",
    "href": "06_VL_D.html#visualisierung-von-verteilungen-5",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nViolin-Plot\n\n\n\n\nDa händische Zeichnungen heute weniger wichtig sind, werden Boxplots in letzter Zeit verstärkt von Violin-Plots abgelöst.\nStatt Boxen und Whiskern zeigen Violin-Plots die gesamte Verteilung der Daten entlang der Y-Achse\nDer dickste Teil des Violins entspricht der höchsten Punktendichte im Datensatz\nViolins sind symmetrisch und beginnen und enden bei den minimalen und maximalen Datenwerten und vergleichbar mit stetigen Histogrammen, die um 90 Grad gedreht sind\n\n\n\n\n\n\nAnatomie eines Boxplots Quelle: Wilke (2019)"
  },
  {
    "objectID": "06_VL_D.html#visualisierung-von-verteilungen-6",
    "href": "06_VL_D.html#visualisierung-von-verteilungen-6",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nViolin-Plot\n\n\n\n\n\nDa händische Zeichnungen heute weniger wichtig sind, werden Boxplots in letzter Zeit verstärkt von Violin-Plots abgelöst.\nStatt Boxen und Whiskern zeigen Violin-Plots die gesamte Verteilung der Daten entlang der Y-Achse\nDer dickste Teil des Violins entspricht der höchsten Punktendichte im Datensatz\nViolins sind symmetrisch und beginnen und enden bei den minimalen und maximalen Datenwerten und vergleichbar mit stetigen Histogrammen, die um 90 Grad gedreht sind"
  },
  {
    "objectID": "06_VL_D.html#visualisierung-von-verteilungen-7",
    "href": "06_VL_D.html#visualisierung-von-verteilungen-7",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nBox-Plots mit Superset\n\nSuperset erlaubt es, Box-Plots direkt zu erstellen, Violin-Plots in der verfügbaren Version jedoch nicht\nHierzu wählen wir den üblichen Weg:\n\nAuf der Startseite oben rechts + Chart\nUnter Dataset wählen wir “training_logistic”\nAnschließend suchen wir nach “Box Plot” und wählen es aus"
  },
  {
    "objectID": "06_VL_D.html#visualisierung-von-verteilungen-8",
    "href": "06_VL_D.html#visualisierung-von-verteilungen-8",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nBox-Plots mit Superset"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression",
    "href": "06_VL_D.html#logistische-regression",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nTrainings- und Testdaten\n\nBevor wir die von Tofispy bereitgestellten Daten analysieren, teilen wir die Daten in Trainings- und Testdaten auf\nDas ist ein übliches Prozedere, um die Qualität des Modells zu evaluieren und Overfitting zu vermeiden\nDa der Trainingsdatensatz für das Modelltraining verwendet wird, wählt an oft einen Anteil von 70-80% der Daten für das Training und 20-30% für das Testen\nIn unserem Fall verwenden wir nur 300 Beobachtungen, um die Visualisierungen zum Einstieg nicht zu überfrachten\nNeben diesen einfachen Splits gibt es auch komplexere Verfahren wie Kreuzvalidierung"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-1",
    "href": "06_VL_D.html#logistische-regression-1",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nBinäre Klassifikation vs. Multiklassen-Klassifikation\n\nKlassifikationsmodelle können in zwei Kategorien unterteilt werden:\n\nBinäre Klassifikation: Die abhängige Variable hat nur zwei Kategorien\nMultiklassen-Klassifikation: Die abhängige Variable hat mehr als zwei Kategorien\n\nIn unserem Fall haben wir nur zwei Genres, Klassik und EDM, also eine binäre Klassifikation\nDa die Intuition hinter der logistischen Regression einfacher zu verstehen ist, beginnen wir mit der binären Klassifikation"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-2",
    "href": "06_VL_D.html#logistische-regression-2",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nZurück zu unserer Ausgangsgleichung\n\nAusgangspunkt für die logistische Regression ist erneut unsere Grundgleichung \\[\ny = f(X) + \\epsilon,\n\\]\nWie besprochen ist die Variable \\(y\\) bei Klassifikationsproblemen qualitativ oder kategorial\nFür die binäre logistische Regression erfolgt eine Umkodierung in eine sog. Dummy-Variable, also 0 oder 1, im Sinne von Falsch und Richtig\nBeispiel bei zwei Genres in der Variable \\(y\\): \\[\\begin{equation}\ny =\n  \\begin{cases}\n    0 & \\text{Song $i$ Klassik,}\\\\\n    1 & \\text{Song $i$ EDM}\n  \\end{cases}       \n\\end{equation}\\]"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-3",
    "href": "06_VL_D.html#logistische-regression-3",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nIntuition\n\n\n\n\nDie Wahl der Kodierung ist theoretisch willkürlich, aber aus praktischer Sicht ist es sinnvoll, die Kategorie, die interessante Kategorie als 1 zu kodieren\nAls erklärende Variable \\(X\\) können wir beliebig viele Features verwenden, wir beschränken uns aber zunächst auf eine, nämlich Energy\nNach der Umkodierung der abhängigen Variable \\(y\\) sehen unsere Daten aus wie der Auszug rechts\nUnser Ziel ist es, die Wahrscheinlichkeit zu berechnen, mit der ein Song EDM ist, gegeben die Energy\n\n\n\n\n\n\n\n\nedm\nenergy\n\n\n\n\n0\n0.1920\n\n\n1\n0.7250\n\n\n1\n0.9320\n\n\n1\n0.7470\n\n\n1\n0.9220\n\n\n0\n0.0741\n\n\n0\n0.0026\n\n\n0\n0.0592\n\n\n0\n0.0162\n\n\n0\n0.2510"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-4",
    "href": "06_VL_D.html#logistische-regression-4",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nIntuition\n\nNoch intuitiver wird der Zusammenhang, wenn wir die Daten visualisieren mit der abhängigen Variable EDM auf der Y-Achse und der unabhängigen Variable Energy auf der X-Achse\n\n\n\n\n\n\n\nScatter Plot mit Dummy Variable als abhängiger Variable. Eigene Darstellung"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-5",
    "href": "06_VL_D.html#logistische-regression-5",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nWarum nicht einfach lineare Regression?\n\n\nEine lineare Regression ist erste Option, den Zusammenhang zu modellieren\nWir erhalten folgende Koeffizienten:\n\n\n\n\n\\[\n\\operatorname{\\widehat{EDM}} = -0.1 + 1.24 \\cdot \\text{Energy}\n\\]"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-6",
    "href": "06_VL_D.html#logistische-regression-6",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nWarum nicht einfach lineare Regression?\n\nDas lineare Modell erstellt auf der blauen Geraden nun eine Prognose für die Wahrscheinlichkeit, dass ein Song EDM ist und modelliert den Zusammenhang zwischen \\(y\\) und \\(X\\) mit \\[\ny = \\beta_0 + \\beta_1 \\cdot X\n\\]\nDiese Prognose ist aus mehreren Gründen nicht sinnvoll:\n\nDie Prognose kann Werte außerhalb des Intervalls \\([0,1]\\) annehmen\nWie kann man ein Modell mit mehr als 2 Klassen darstellen, bei denen es keine natürliche Ordnung gibt?"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-7",
    "href": "06_VL_D.html#logistische-regression-7",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nDie Logistische Funktion\n\n\nDie logistische Funktion ist eine Sigmoid-Funktion, die Werte zwischen 0 und 1 annimmt und eine S-Form aufweist und damit das erste Problem behebt\nSigmoid Funktionen garantieren, dass alle vorgehersagten Wahrscheinlichkeiten zwischen 0 und 1 liegen, auch wenn die unabhängige Variable \\(X\\) sehr groß oder sehr klein ist"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-8",
    "href": "06_VL_D.html#logistische-regression-8",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nDie Logistische Funktion\n\nDie logistische Regression modelliert die Wahrscheinlichkeit, dass \\(y\\) eine bestimmte Kategorie annimmt: \\[\nP(y = 1|X) = \\frac{ e^{(\\beta_0 + \\beta_1 \\cdot X)}}{1 + e^{(\\beta_0 + \\beta_1 \\cdot X)}}\n\\]\nwobei \\(P(y = 1|X)\\) die Wahrscheinlichkeit ist, dass \\(y\\) die Kategorie 1 annimmt, also EDM ist, gegeben einen Wert für \\(X\\), in unserem Fall Energy\nDas finale Modell sieht dann aus wie folgt: \\[\nP(EDM = 1|Energy) = \\frac{ e^{(\\beta_0 + \\beta_1 \\cdot \\text{Energy})}}{1 + e^{(\\beta_0 + \\beta_1 \\cdot \\text{Energy})}}\n\\]"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-9",
    "href": "06_VL_D.html#logistische-regression-9",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nErgebnis der logistischen Regression\n\nWenn wir statt des linearen Modells ein logistisches Modell verwenden, erhalten wir folgenden Zusammenhang zwischen Energy und der Wahrscheinlichkeit, dass ein Song zum Genre EDM gehört:"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-10",
    "href": "06_VL_D.html#logistische-regression-10",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nErgebnis der logistischen Regression\n\n\n\n\nAus dem Modell erhalten für für jeden Wert von Energy eine Wahrscheinlichkeit, dass ein Song EDM ist\nDie Wahrscheinlichkeit steigt (sinkt) mit steigender Energy und nähert sich 1 (0) an\nDie Steigung der Kurve ist in der Mitte am größten und nimmt zu den Rändern hin ab und die Kurve hat die typische S-Form\nDie Tabelle rechts zeigt die Wahrscheinlichkeiten für 10 zufällig ausgewählte Songs, sortiert nach der Wahrscheinlichkeit\n\n\n\n\n\n\n\n\n\n\nenergy\ntrack.name\ntrack.artist\ncategory\np_edm\n\n\n\n\n0.255\nNaruto: Alone Theme\nToshio Masuda\nKlassik\n0.012\n\n\n0.285\nThe Wife\nJocelyn Pook\nKlassik\n0.022\n\n\n0.323\nbad guy\nVitamin String Quartet\nKlassik\n0.048\n\n\n0.433\nMIDNIGHT\nPLAYAMANE\nEDM\n0.343\n\n\n0.570\nCNTRTE\nAQUIHAYAQUIHAY\nEDM\n0.907\n\n\n0.628\nI'll Take That Back\nAvangart Tabldot\nEDM\n0.971\n\n\n0.629\nOneHundred\nHEDEGAARD\nEDM\n0.972\n\n\n0.636\nEtude No. 9\nKummerspeck\nKlassik\n0.975\n\n\n0.680\nNow\nNiklas Dee\nEDM\n0.990\n\n\n0.690\nMe Provocas\nFumaratto\nEDM\n0.992"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-11",
    "href": "06_VL_D.html#logistische-regression-11",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nGenerierung von Prognosen\n\nAus dem Modell erhalten wir eine Schätzung für die beiden Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\)\nDas gefittete Modell hat folgende Koeffizienten:\n\n\n\\[\n\\hat{P}(EDM = 1|Energy) = \\frac{ e^{(-9.89 + 21.34 \\cdot \\text{Energy})}}{1 + e^{(-9.89 +  21.34\\cdot \\text{Energy})}}\n\\]\n\n\nDurch einfaches Einsetzen des jeweiligen Energy-Wertes erhalten wir die Wahrscheinlichkeit gibt das Modell eine entsprechende Prognose aus"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-12",
    "href": "06_VL_D.html#logistische-regression-12",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nVon Wahrscheinlichkeiten zu Klassifikationen\n\n\nBisher haben wir nur eine Wahrscheinlichkeit für die Klassenzugehörigkeit von Song \\(i\\) berechnet, gegeben den Wert für Energy, also\n\n\n\n\\[\nP(\\text{EDM}_i = 1|\\text{X = Energy}_i)\n\\]\n\n\n\nIm binären Klassifikationsmodell wird ein Schwellenwert oder Cut-Off Point \\(C\\) festgelegt, der bestimmt, ob ein Song als EDM klassifiziert wird oder nicht\nDie Zuordnung folgt dann allgemein nach der Form:\n\n\n\n\\[\\begin{equation}\n  \\text{Klasse} =\n    \\begin{cases}\n      0 & \\text{wenn } P(y_i = 1|X= x_i) \\leq C \\\\\n      1 & \\text{wenn } P(y_i = 1|X= x_i) &gt; C\n    \\end{cases}       \n\\end{equation}\\]\n\n\n\nEin häufig anzutreffender Default-Wert ist \\(C=0.5\\)"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-13",
    "href": "06_VL_D.html#logistische-regression-13",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nErgebnis und Evaluation\n\n\n\n\nMit dem Default-Wert \\(C=0.5\\) erhalten wir die rechts dargestellten Klassifikationen für die 10 Beispiel-Songs\nMit den generierten Klassifikationen lassen sich nun verschiedene Metriken berechnen, um die Qualität des Modells zu bewerten\nDie einfachste Metrik ist die Accuracy, die den Anteil der korrekt klassifizierten Songs angibt\n\n\n\n\n\n\n\n\n\n\nenergy\ntrack.name\ntrack.artist\nedm\np_edm\n.pred_class\n\n\n\n\n0.255\nNaruto: Alone Theme\nToshio Masuda\n0\n0.012\n0\n\n\n0.285\nThe Wife\nJocelyn Pook\n0\n0.022\n0\n\n\n0.323\nbad guy\nVitamin String Quartet\n0\n0.048\n0\n\n\n0.433\nMIDNIGHT\nPLAYAMANE\n1\n0.343\n0\n\n\n0.570\nCNTRTE\nAQUIHAYAQUIHAY\n1\n0.907\n1\n\n\n0.628\nI'll Take That Back\nAvangart Tabldot\n1\n0.971\n1\n\n\n0.629\nOneHundred\nHEDEGAARD\n1\n0.972\n1\n\n\n0.636\nEtude No. 9\nKummerspeck\n0\n0.975\n1\n\n\n0.680\nNow\nNiklas Dee\n1\n0.990\n1\n\n\n0.690\nMe Provocas\nFumaratto\n1\n0.992\n1"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation",
    "href": "06_VL_D.html#modellevaluation",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKonfusionsmatrix\n\n\nZur Berechnung der Modellgüte wird die sog. Konfusionsmatrix verwendet, die die Anzahl der korrekt und inkorrekt klassifizierten Beobachtungen zusammenfasst\nIm binären Modell gilt:\n\npositiv: Beobachtung \\(i\\) gehört zur interessanten Klasse (in unserem Fall EDM)\nnegativ: Beobachtung \\(i\\) gehört nicht dazu\n\n\n\n\n\n\n\n\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\nSumme Vorhersage\n\n\n\n\nVorhergesagt Positiv\nTrue Positive (TP)\nFalse Positive (FP)\nSumme Positiv\n\n\nVorhergesagt Negativ\nFalse Negative (FN)\nTrue Negative (TN)\nSumme Negativ\n\n\nSumme Tatsächlich\nSumme Positiv Tats.\nSumme Negativ Tats.\nGesamtsumme"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-1",
    "href": "06_VL_D.html#modellevaluation-1",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKonfusionsmatrix\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\n\n\n\n\nVorhergesagt Positiv\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nVorhergesagt Negativ\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\nTrue Positive (TP): Ein TP liegt vor, wenn das Modell ein Objekt korrekt der relevanten Klasse zuordnet. Beispiele hierfür sind die korrekte Identifikation eines Schadens, die richtige Diagnose einer Krankheit oder die richtige Erkennung von Spam.\nFalse Positive (FP): Ein FP liegt vor, wenn das Modell ein Objekt fälschlicherweise als positiv bzw. relevant klassifiziert, obwohl es tatsächlich negativ ist. Beispiele hierfür sind die Meldung eines nicht vorhandenen Schadens oder die Diagnose einer nicht existierenden Krankheit. FP wird auch als Typ-I Fehler oder Alpha-Fehler bezeichnet."
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-2",
    "href": "06_VL_D.html#modellevaluation-2",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKonfusionsmatrix\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\n\n\n\n\nVorhergesagt Positiv\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nVorhergesagt Negativ\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\nTrue Negative (TN): Ein TN liegt vor, wenn das Modell ein Objekt korrekt als negativ, also nicht der relevanten Klasse zugehörig klassifiziert. Beispiele hierfür sind die korrekte Identifikation eines funktionsfähigen Teils oder die richtige Klassifizierung einer Person als gesund.\nFalse Negative (FN): Ein FN liegt vor, wenn das Modell ein Objekt falsch als negativ klassifiziert, obwohl es tatsächlich positiv – also relevant – ist. Synonyme hierfür sind Typ-II Fehler oder Beta-Fehler. Beispiele: die ausbleibende Meldung eines aufgetretenen Schadens oder die falsche Nicht-Diagnose einer existierenden Krankheit"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-3",
    "href": "06_VL_D.html#modellevaluation-3",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKonfusionsmatrix im Beispiel und Berechnung der Accuracy\n\n\n\nUnser einfaches Modell zeigt die rechts dargestellte Konfusionsmatrix\nAus den vier Quadranten lässt sich dann die Accuracy berechnen:\n\n\n\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\] \\[\n= \\frac{156 + 140}{156 + 140 + 2 + 2} = 0.987\n\\]"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-4",
    "href": "06_VL_D.html#modellevaluation-4",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nVor- und Nachteile der Accuracy\n\nDie Accuracy ist eine einfache und intuitive Metrik, die den Anteil der korrekt klassifizierten Beobachtungen angibt\nAllerdings sollte die Accuracy nur bei einem ausgewogenen Datensatz verwendet werden\nAusgewogen oder balanciert bedeutet hier, dass die Anzahl der Beobachtungen in den Klassen ungefähr gleich ist\nIm einfachen Beispiel hier ist das der Fall, weshalb die Accuracy ausreichend ist"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-5",
    "href": "06_VL_D.html#modellevaluation-5",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nAccuracy bei unbalancierten Datensätzen\n\n\n\nIn der Praxis liegen häufig unbalancierte Datensätze vor, sodass die Accuracy allein meist nur geringe Aussagekraft hat\nWir nehmen ein extremes Beispiel mit 1000 E-Mails, von denen 100 Spam-Mails sind\nUnser Modell zur Spam-Erkennung liefert die Konfusionsmatrix rechts\nDas Modell erkennt nur eine Spam-Nachricht, erreicht jedoch eine Accuracy von über 90%, weil die Zahl der TN sehr hoch ist\n\n\n\n\n\n\n\n\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\nSumme Vorhersage\n\n\n\n\nVorhergesagt Positiv\n1\n0\n1\n\n\nVorhergesagt Negativ\n99\n900\n999\n\n\nSumme Tatsächlich\n100\n900\n1000"
  },
  {
    "objectID": "06_VL_D.html#logistische-regression-14",
    "href": "06_VL_D.html#logistische-regression-14",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nInteraktive Visualisierung\n\nFür eine bessere Intuition hinter den Konzepten der logistischen Regression und der Konfusionsmatrix gibt es eine interaktive Visualisierung als Shiny App\nErreichbar über:\n\nhttps://bi-and-ds-logistic-regression-qkwupfgvpq-ey.a.run.app/\n\nAlternativ Download der R-Files von StudIP und Ausführung in RStudio nach Installation der notwendigen Packages im Script install_packages.R\nFrage: Welche Auswirkungen hat eine Änderung des Cut-Off Points auf die Konfusionsmatrix und die Accuracy? Welche ist die bestmöglich Accuracy?"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-6",
    "href": "06_VL_D.html#modellevaluation-6",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision\n\n\nAndere Metriken beheben die Schwächen der Accuracy\nPrecision oder Präzision gibt an, wie viele der vom Klassifikator als positiv identifizierten Fälle tatsächlich positiv sind und entspricht dem Anteil der tatsächlich positiven Fälle an der Menge aller als positiv klassifizierten Fälle\n\n\n\\[\\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\\]\n\n\nWelcher Anteil der positiven Identifikationen war tatsächlich korrekt? Oder: Wenn das Modell einen Datenpunkt positiv klassifiziert, wie wahrscheinlich ist es, dass diese Klassifikation richtig ist?\nEine hohe Precision bedeutet also, dass der Klassifikator nur wenige irrelevante Fälle als relevant einstuft und ein Klassifikator mit einer Precision von 1,0 liefert keine FP."
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-7",
    "href": "06_VL_D.html#modellevaluation-7",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nRecall\n\n\nRecall hingegen gibt an, wie viele der tatsächlich positiven Fälle vom Klassifikator als positiv bzw. relevant erkannt wurden:\n\n\n\\[\\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}}\\]\n\n\nEin hoher Recall-Wert bedeutet, dass der Klassifikator viele relevante Fälle erkennt und beantwortet die Frage, welcher Anteil der positiven Ergebnisse richtig identifiziert wurde.\nWie wahrscheinlich ist es, dass das Modell einen positiven Datenpunkt erkennt?\nAuch häufig als Sensitivität oder True Positive Rate bezeichnet\n\n\n\n\nKasten: Sample aller Beobachtungen\nLinker Kasten: Positive Beobachtungen\nRechte Kasten: Negative Beobachtungen\nKreis: Vorhersage des Modells, also ALLE als positiv vorhergesagte Datenpunkte\nKreis besteht aus zwei Hälften: TP und FN"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-8",
    "href": "06_VL_D.html#modellevaluation-8",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision und Recall\n\n\n\nIllustration Precision, Recall, Accuracy, Quelle: Maleki u. a. (2020)\n\n\n\n\nKasten: Sample aller Beobachtungen\nLinker Kasten: Positive Beobachtungen\nRechte Kasten: Negative Beobachtungen\nKreis: Vorhersage des Modells, also ALLE als positiv vorhergesagte Datenpunkte\nKreis besteht aus zwei Hälften: TP und FN"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-9",
    "href": "06_VL_D.html#modellevaluation-9",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision und Recall für den Spam-Filter\n\n\n\n\nUm Precision und Recall bei unausgewogenen Datensätzen zu illustrieren, betrachten wir erneut den Spam-Filter\nFür Precision erhalten wir\n\n\n\\[\\text{Precision} = \\frac{\\text{1}}{\\text{1 + 0}} = 1\\]\n\n\nFür Recall ergibt sich\n\n\n\\[\\text{Recall} = \\frac{\\text{1}}{\\text{1 + 99}} = 0,01.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\nSumme Vorhersage\n\n\n\n\nVorhergesagt Positiv\n1\n0\n1\n\n\nVorhergesagt Negativ\n99\n900\n999\n\n\nSumme Tatsächlich\n100\n900\n1000\n\n\n\n\n\n\nWenn das Modell eine Mail als Spam klassifiziert, ist diese Prognose zu 100% korrekt\nDas Modell erkennt aber nur 1% der tatsächlichen Spam-Mails."
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-10",
    "href": "06_VL_D.html#modellevaluation-10",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision und Recall für den Spam-Filter\n\nWelches Maß ist nun das Richtige?\n\n\nWenn ein balancierter Datensatz vorliegt, kann Accuracy bedenkenlos verwendet werden, um das Modell zu evaluieren\nWenn wir sicher sein wollen, dass eine positive Vorhersage korrekt ist, dann ist Precision die angemessene Evaluationsmetrik. Dies ist oft der Fall, wenn FP mit höheren Kosten verbunden sind, als FN.\nWenn es wichtig ist, so viele positive Fälle wie möglich zu identifizieren, sollte ein Modell mit hohem Recall verwendet werden. In diesem Fall sind die Kosten von FN besonders hoch, sodass es besser ist, einige Fälle fälschlicherweise negativ zu klassifizieren, als dass uns tatsächlich positive Fälle durch die Lappen gehen"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-11",
    "href": "06_VL_D.html#modellevaluation-11",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nF1-Score\n\nOft sind Kosten und Nutzen von FP und FN Klassifikationen nicht eindeutig\nDer F1 Score kompensiert die Nachteile der Accuracy bei unbalancierten Datensätzen zu kompensieren und legt gleichzeitig einen ausgewogenen Fokus auf Precision und Recall:\n\n\n\\[\nF1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\nF1 ist das harmonische Mittel von Precision und Recall und liegt zwischen 0 und 1"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-12",
    "href": "06_VL_D.html#modellevaluation-12",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nF1-Score\n\nIm Spam Beispiel:\n\n\n\\[\nF1 = 2 \\cdot \\frac{\\text{1} \\cdot \\text{0,01}}{\\text{1} + \\text{0,01}}= 0,0198\n\\]\n\n\nDieser Wert ist deutlich niedriger, als uns die 90,1% Accuracy zunächst suggerieren.\nTrotzdem ist das Beispielmodell nahezu nutzlos, wie anhand des niedrigen F1-Scores erkennbar wird."
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-13",
    "href": "06_VL_D.html#modellevaluation-13",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nAbschließende Bemerkungen\n\nBei einem balancierten Datensatz und gleicher Gewichtung von FP und FN ist die Accuracy die einfachste und intuitivste Metrik\nBei unbalancierten Datensätzen sollten Precision, Recall und F1-Score verwendet werden\nWenn FP höhere Kosten haben, dann sollte Precision im Fokus stehen\nWenn FN höhere haben, dann sollte Recall im Fokus stehen\nDer F1-Score ist besonders nützlich, wenn Precision und Recall gleiche Gewichtung haben"
  },
  {
    "objectID": "06_VL_D.html#modellevaluation-14",
    "href": "06_VL_D.html#modellevaluation-14",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKosten von FP und FN\n\n\nDie meisten Klassifikationsmethoden basieren auf der Annahme, dass FP und FN gleich problematisch sind\nIn praktischen Szenarios ist dies jedoch selten der Fall, Beispiel:\nBetrugserkennung: Eine Haftpflichtversicherung prüft Schadensmeldungen mit ML. Das Modell soll den Schaden automatisch abwickelnd und entweder als “Zahlung” oder “Keine Zahlung” klassifizieren, je nachdem ob ein Betrugsversuch vorliegt\nFragen:\n\nWas ist die positive (interessante) Klasse aus Sicht der Versicherung?\nWelche Fälle sind dann FN und FP?\nWelche Fehlklassifikation ist teurer? Welches Maß sollte für das Modell maximiert werden?\n\n\n\n\n\nAntworten:\n\nDie interessante Klasse ist die Zahlung des Schadens\nFN: Die Forderung ist berechtigt, aber die Versicherung zahlt nicht\nFP: Die Forderung ist unberechtigt, aber die Versicherung zahlt\nFP ist teurer, da die Versicherung zahlen muss, deshalb sollte Precision im Vordergrund stehen"
  },
  {
    "objectID": "06_VL_D.html#quellen",
    "href": "06_VL_D.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, und Robert Tibshirani. 2021. An Introduction to Statistical Learning: with Applications in R. Second edition. Springer texts in statistics. New York NY: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nMaleki, Farhad, Katie Ovens, Keyhan Najafian, Behzad Forghani, Caroline Md, und Reza Forghani. 2020. „Overview of Machine Learning Part 1“. Neuroimaging Clinics of North America 30 (November): e17–32. https://doi.org/10.1016/j.nic.2020.08.007.\n\n\nWilke, C. 2019. Fundamentals of data visualization: a primer on making informative and compelling figures. First edition. Sebastopol, CA: O’Reilly Media. https://clauswilke.com/dataviz/."
  },
  {
    "objectID": "02_VL_D.html#der-plan-für-heute",
    "href": "02_VL_D.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung 2\n\nDatenbereitstellung:\n\nData Warehouse\nData Mart\nArchitekturkonzepte\n\nBusiness Case: Der Musikstreaming Anbieter Tofispy braucht unsere Hilfe\nVerknüpfung eines ersten Data Marts mit Superset"
  },
  {
    "objectID": "02_VL_D.html#überblick-zum-bia-gesamteinsatz",
    "href": "02_VL_D.html#überblick-zum-bia-gesamteinsatz",
    "title": "Business Intelligence & Data Science",
    "section": "Überblick zum BIA Gesamteinsatz",
    "text": "Überblick zum BIA Gesamteinsatz\nDatenbereitstellung\n\n\n\nBIA Gesamtansatz. Eigene Darstellung in Anlehnung an Baars und Kemper (2021).\n\n\n\n\nFür den Moment nehmen wir erst einmal an, die Daten befänden sich bereits in unserem Zugriff in unserer unternehmenseigenen IT-Infrastuktur\nDer eigentliche Weg dahin, der ETL Prozess: Extract, Transform, Load oder auf Deutsch Extraktion, Transformation, Laden, kommt dann in der nächsten Woche\nHeute schauen wir uns den rechten Teil in der Datenbereitstellung an, den ersten Schritt auf dem Weg von operativen Rohdaten hin zu informationsgetriebenen Entscheidungen\nWir schauen uns an, wie Daten in einem Data Warehouse und Data Marts aufbereitet und bereitgestellt werden, welche Architekturkonzepte es gibt, was deren Vor- und Nachteile sind"
  },
  {
    "objectID": "02_VL_D.html#data-warehouse",
    "href": "02_VL_D.html#data-warehouse",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nBegriff\n\n\n\n\n\n\nDefinition\n\n\nEin Data Warehouse (DWH) ist ein Datenhaltungssystem dispositiver Daten, das von den operativen Datenbeständen getrennt, themenorientiert aufbereitet und logisch zentralisiert ist. Ein DWH integriert unternehmensweit Datenbestände aus verschiedenen operativen internen Systemen (z.B. Kernbanksystemen und Enterprise-Ressource-Planning-Systemen) sowie externen Systemen (z.B. Börseninformationssystemen und Systeme für externe Ratings) und dient idealtypisch als unternehmensweite, einheitliche und konsistente Datenbasis für alle Arten von Systemen der Entscheidungsunterstützung (siehe Kemper und Sun 2023).\n\n\n\n\n\nDie entscheidenden Punkte sind hier:\n\nTrennung von operativen und dispositiven Daten\nIntegration von Datenbeständen aus verschiedenen und oftmals sehr heterogenen Quellen\nEinheitliche und konsistente Datenbasis für das gesamte Unternehmen"
  },
  {
    "objectID": "02_VL_D.html#data-warehouse-1",
    "href": "02_VL_D.html#data-warehouse-1",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nEigenschaften\n\n\n\nThemenorientierung:\n\nDispositive Daten des DWH sind explizit an den Interessenslagen der Entscheidenden ausgerichtet\nDie operativen bzw. externen Daten werden vor der Speicherung im DWH aufbereitet, harmonisiert und ggf. voraggregiert\nThemen sind Produkthierarchien, vordefinierte Zeiträume wie Quartale oder betriebswirtschaftliche Kennzahlen wie DB1\n\nIntegration:\n\nDaten aus den unterschiedlichen operativen und externen Systemen werden im DWH integriert\nZusammenführung zu einer inhaltlich widerspruchfreien Datenquelle, sogenannter “single point of truth”\n\n\n\n\n\n\nThemenorientierung bedeutet vor allem auch, dass die Quellsystelogik aufgebrochen wird, es also egal ist, aus welchen Systemen die Daten ursprünglich mal gekommen\nAuch liegen hier nicht die Rohdaten, vielmehr haben schon Tranformationen stattgefunden, beispielsweise eine Harmonisierung und Aggregation\nDiese Themen wirken relativ abstrakt, aber Beispiele sind Umsätze pro Produkthierarchieebene im aktuellen Jahr, oder der DB1 in den letzten Jahren"
  },
  {
    "objectID": "02_VL_D.html#data-warehouse-2",
    "href": "02_VL_D.html#data-warehouse-2",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nEigenschaften\n\n\nZeitraumbezug:\n\nOperative Systeme sind transaktionsorientiert und bilden einen bestimmten Zeitpunkt ab\nDaten im DWH werden üblicherweise auf Zeiträume aggregiert, bspw. ein Monat oder ein Jahr\n\nNicht-Volatilität:\n\nDaten im DWH werden dauerhaft abgelegt und für die Analyse zur Verfügung gestellt\nDWH-Daten werden somit in der Regel nicht mehr geändert, überschrieben oder entfernt\n\n\n\n\n\nOperative Daten sind immer ein Snapshot, der einen bestimmten Zeitpunkt abbildet.\nBeispielsweise durchläuft eine Order verschiedene Zustände und ein operatives System zeigt üblicherweise nur den Ist-Zustand. Beispiel: Bestellstatus bei einem Online-Händler\nIm DWH wiederum wird eine Historie gebildet, die aggregiert wird, beispielsweise die Zustände der Orders pro Tag"
  },
  {
    "objectID": "02_VL_D.html#data-warehouse-3",
    "href": "02_VL_D.html#data-warehouse-3",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nKomponenten des Data Warehouse\n\n\n\nData Mart\n\nSeparater Datenpool für einen bestimmten Anwendungsbereich spezifischer Abteilungen\nNur ein Ausschnitt aus dem gesamten Datenpool, häufig aus Performance-Erwägungen\nHäufig mit Reporting und OLAP assoziiert, zunehmend aber auch für Analysen\n\n\nCore Data Warehouse\n\nRückrat der meisten Architekturkonzepte und oft als Basisdatenbank bezeichnet\nBefüllung über ETL-Prozesse aus operativen Quellsystemen\nMeist auf relationalen Datenhaltungssystemen basierend mit großen Datenvolumina (TB Bereich)\nApplikationsneutral modelliert"
  },
  {
    "objectID": "02_VL_D.html#data-warehouse-4",
    "href": "02_VL_D.html#data-warehouse-4",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nFunktionen des CDWH\n\nSammel- und Integrationsfunktion:\n\n\n\nAufnahme aller wichtigen Daten für die Analyse in Form eines zentralen Datenlagers\n\n\n\nDistributionsfunktion:\n\n\n\nVersorgung aller nachgeschalteten Data Marts mit Daten\n\n\n\nQualitätssicherungsfunktion:\n\n\n\nDatentransformation sichert die syntaktische und semantische Stimmigkeit der dispositiven Datenbasis\n\n\n\n\nJemand eine Idee was das bedeutet, syntaktisch und semantische Stimmigkeit?\nSyntaktisch: Die Daten sind in der richtigen Form, also beispielsweise die richtigen Datentypen, die richtigen Werte\nSemantisch: Die Daten haben die richtige Bedeutung, also beispielsweise die richtigen Bezeichnungen, die richtigen Einheiten, Währungen, Periodenzuordnungen"
  },
  {
    "objectID": "02_VL_D.html#data-warehouse-5",
    "href": "02_VL_D.html#data-warehouse-5",
    "title": "Business Intelligence & Data Science",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nData Mart vs. CDWH\n\n\n\nCharakteristika von Data Mart und Core Data Warehouse im Vergleich. In Anlehnung an Baars und Kemper (2021)\n\n\n\n\n\n\n\n\nData Mart\nCore Data Warehouse\n\n\n\n\nZiel\nEntscheidungsunterstützung für ausgewählte Bereiche, spezifisch auf Analyseanforderungen zugeschnitten\nEntscheidungsunterstützung für alle Bereiche in einem Unternehmen\n\n\nAusrichtung\nBereichsspezifisch oder Abteilungsbezogen\nZentral und unternehmensweit\n\n\nGranularität\nHöhere Aggregationen\nFeinste verfügbare Granularität\n\n\nVerfügbarkeit für Endanwendende\nIn der Regel möglich\nHäufig nicht erlaubt da zentral durch IT betrieben und als Quellsystem für Marts genutzt\n\n\nFlexibilität der Analysen\nTendenziell gering und auf Anwendungsbereich beschränkt\nSehr flexibel\n\n\nVolumina\nGering bis moderat\nModerat bis umfangreich\n\n\n\n\n\n\nBeispiel: Tages und Wochendaten Fabrikläden vs. DSP wegen unterschiedlicher Planungszeiträume"
  },
  {
    "objectID": "02_VL_D.html#dwh-architekturen",
    "href": "02_VL_D.html#dwh-architekturen",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nUnabhängige Data Marts\n\n\n\n\n\n\n\n\nUnabhängige Data Marts. Eigene Darstellung in Anlehnung an Hahne (2016)\n\n\n\n\n\n\nAuch Stove-Pipe Ansatz\nBedienen sich direkt aus den operativen und externen Systemen\nBereiten die enthaltenen Daten für relevante Anwendungsfelder auf\nDaten werden isoliert bezogen und fließen direkt in Datensilos auf Basis bereichsspezifischer Fragestellungen\nVerschiedene Marts können unterschiedliche externe Datenquellen zusammenführen\n\n\n\n\n\n\nKurz Formen besprechen und was sie darstellen:\n\nZylinder sind Datenbanken und Systeme, hier MES, PDM und SAP\nWürfel sind mehrdimensinale Tabellen, unsere Data Marts\n\nStove-Pipe Ansatz heißt so viel wie Ofenrohr, also eine direkte Verbindung zwischen Quelle und Ziel"
  },
  {
    "objectID": "02_VL_D.html#dwh-architekturen-1",
    "href": "02_VL_D.html#dwh-architekturen-1",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nUnabhängige Data Marts\n\n\n\nVorteile:\n\nSchnelle und bereichsspezifische Informationsbereitstellung\nSinnvoll bei fehlender Governance Strategie\nErfüllung maßgeschneideter bereichsspezifischer Fragestellungen\n\n\nNachteile:\n\nHäufig historisch gewachsene Strukturen und damit geringe Governance\nMehrfache Aufbereitung der Quelldaten\nGefahr von Inkonsistenzen bei der Kennzahlenberechnung zwischen Marts\nMangelnde Möglichkeit bereichsübergreifender Analysen\n\n\n\n\n\n\nGovernance ist hier eine Art Regelwerk, das die Datenqualität und -sicherheit sicherstellt und gemeinsame Datenmodelle definiert\nIm Extremfall haben die Marts dann inkompatible Datenmodelle, beispielsweise heißt die Datumsspalte in einem Mart DATE und im anderen YMD oder die Aggregationslevel weichen ab, sodass keine gemeinsame Analyse der Marts mehr möglich ist."
  },
  {
    "objectID": "02_VL_D.html#dwh-architekturen-2",
    "href": "02_VL_D.html#dwh-architekturen-2",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nAbgestimmte Data Marts\n\n\n\n\n\n\n\nAbhängige Data Marts. Eigene Darstellung in Anlehnung an Hahne (2016)\n\n\n\n\n\n\nKonzeptionell abgestimmte Datenmodelle um die Integrität des Datenmaterials zu gewährleisten\nDas abgestimmte Datenmodell dient der syntaktischen und semantischen Vereinheitlichung\nInhaltliche und zeitliche Übereinstimmung der Datenextraktionen ist entscheidend für die Konsistenz der Data Marts\n\n\n\n\n\nFrage: Was bedeutet hier inhaltliche und zeitliche Übereinstimmung und warum ist das wichtig?"
  },
  {
    "objectID": "02_VL_D.html#dwh-architekturen-3",
    "href": "02_VL_D.html#dwh-architekturen-3",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nAbgestimmte Data Marts\n\n\n\nVorteile:\n\nIntegrität des Datenmodells wird gewährleistet\nMöglichkeit bereichsübergreifender Analysen bei hoher Flexibilität innerhalb der Bereiche\nEntscheidungsunterstützung für alle Bereiche in einem Unternehmen\n\n\nNachteile:\n\nDurch hohen Bereichsbezug oft unterschiedliche Granularität oder Aufbereitung, damit nur bedingte Integration zwischen Marts\nMöglicherweise höherer Abstimmungsbedarf zwischen Abteilungen bei der Kennzahldefinition\nInformationsverlust bei übergreifenden Analysen"
  },
  {
    "objectID": "02_VL_D.html#dwh-architekturen-4",
    "href": "02_VL_D.html#dwh-architekturen-4",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nCore Data Warehouse\n\n\n\n\n\n\n\nCore Data Warehouse. Eigene Darstellung in Anlehnung an Hahne (2016)\n\n\n\n\n\n\nDas CDWH wird direkt aus den operativen Quellsystemen befüllt und basiert auf einer relationalen Datenbank\nDieser Ansatz wird oft als Monolith bezeichnet\nDie Daten decken unterschiedliche Auswertungszwecke ab und sind weniger anwendungsbezogen als Datensilos"
  },
  {
    "objectID": "02_VL_D.html#dwh-architekturen-5",
    "href": "02_VL_D.html#dwh-architekturen-5",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nCore Data Warehouse\n\n\n\nVorteile:\n\nHoher Grad an Mehrfachverwendbarkeit der Daten\nHoher Detailgrad möglich\nBei kleineren Anwendungsfällen oft ausreichend\n\n\nNachteile:\n\nBerechtigungsmanagement und Performance stoßen bei komplexen Anwendungsfällen schnell an Grenzen\nBei größeren Einheiten mit eigenen Geschäftsprozessen und stark abweichenden Hierarchiestrukturen sehr komplex, hier bietet sich der Einsatz mehrerer CDWH an"
  },
  {
    "objectID": "02_VL_D.html#dwh-architekturen-6",
    "href": "02_VL_D.html#dwh-architekturen-6",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nCore Data Warehouse mit abhängigen Data Marts\n\n\n\n\nAuch Hub-and-Spoke Ansatz genannt\nCDWH wird nicht direkt für Analysen herangezogen, sondern dient der Befüllung von Marts\nMarts sind dann anwendungsbezogen, weisen aber ein einheitliches Datenmodell auf\nCDWH als Hub erfüllt Aufgaben der Integration, Qualitätssicherung und Datenverteilung an die Marts\n\n\n\n\n\n\n\n\nCore Data Warehouse mit abhängigen Data Marts. Eigene Darstellung in Anlehnung an Hahne (2016)"
  },
  {
    "objectID": "02_VL_D.html#dwh-architekturen-7",
    "href": "02_VL_D.html#dwh-architekturen-7",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nCore Data Warehouse mit abhängigen Data Marts\n\n\n\nVorteile:\n\nEinmaliger und einheitlicher Transformationsprozess ohne redundante Transformationslogik\nGeringere Anzahl an Extraktionsprozessen\nReduzierte Anzahl direkter Schnittstellen zwischen Marts und operativen Daten\n\n\nNachteile:\n\nNach wie vor eher traditionelle BI-Sicht"
  },
  {
    "objectID": "02_VL_D.html#dwh-architekturen-8",
    "href": "02_VL_D.html#dwh-architekturen-8",
    "title": "Business Intelligence & Data Science",
    "section": "DWH Architekturen",
    "text": "DWH Architekturen\nArchitekturen-Mix\n\n\n\n\n\nArchitekturen-Mix. Eigene Darstellung in Anlehnung an Hahne (2016)"
  },
  {
    "objectID": "02_VL_D.html#business-case",
    "href": "02_VL_D.html#business-case",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nDas Wachstum bei Tofispy stagniert\n\n\n\nTofispy ist ein Musikstreaming Anbieter aus Deutschland\nDas Unternehmen kämpft mit stagnierendem Wachstum\nWir – die Unternehmensberatung LeinbizConsult – wurden beauftragt, die Gründe für das stagnierende Wachstum zu identifizieren und Handlungsempfehlungen zu entwickeln\nDie Geschäftsführung hat uns erste Daten zur Verfügung gestellt, um die aktuelle Situation zu evaluieren und Ursachenforschung zu betreiben"
  },
  {
    "objectID": "02_VL_D.html#business-case-1",
    "href": "02_VL_D.html#business-case-1",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nDatensatz\n\nÜberblick über den Streamingmarkt:\n\nTabelle “market_share” im Dataset Market Research\nAnzahl der Nutzer pro Plattform in Mio\nDaten von 2016 bis 2024\nDie Daten für 2024 sind eine Hochrechnung für das laufende Jahr"
  },
  {
    "objectID": "02_VL_D.html#business-case-2",
    "href": "02_VL_D.html#business-case-2",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nAufgabe\n\n\nArbeit in 2er Teams mit mindestens einem Laptop oder Tablet pro Team, Laptop ideal\nTrial & Error in Superset mit Hilfe der Dokumentation:\n\nDokumentation unter preset.io\nStartpunkt: Creating a Chart (in der preset Doku)\nTable oder Preview im SQL Editor als Ausgangspunkt für explorative Analyse\n\nZiel:\n\nGestapeltes Balkendiagramm für die relativen Marktanteile der Plattformen im Zeitverlauf\nLiniendiagramm für die absolute Marktentwicklung"
  },
  {
    "objectID": "02_VL_D.html#business-case-3",
    "href": "02_VL_D.html#business-case-3",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nFragen (ggf. als Hausaufgabe)\n\n\nWächst Tofispy schneller oder langsamer als der Gesamtmarkt?\nWie hoch ist der Marktanteil am Ende von 2024 voraussichtlich?\nWelcher Konkurrent wächst am stärksten?\nIst der Datensatz operativ oder dispositiv?\nWelcher Reifegrad von Analytics liegt vor?"
  },
  {
    "objectID": "02_VL_D.html#quellen",
    "href": "02_VL_D.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nBaars, Henning, und Hans-Georg Kemper. 2021. Business Intelligence & Analytics: Grundlagen und praktische Anwendungen: Ansätze der IT-basierten Entscheidungsunterstützung. 4., überarbeitete und erweiterte Auflage. Lehrbuch. Wiesbaden [Heidelberg]: Springer Vieweg.\n\n\nHahne, Michael. 2016. „Architekturkonzepte und Modellierungsverfahren für BI-Systeme“. In Analytische Informationssysteme, herausgegeben von Peter Gluchowski und Peter Chamoni, 147–85. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-47763-2_8.\n\n\nKemper, Hans-Georg, und Xuanpu Sun. 2023. „Data Warehouse“. In Gabler Bankenlexikon. https://www.gabler-banklexikon.de/definition/data-warehouse-56847/version-377924."
  },
  {
    "objectID": "07_VL_D.html#der-plan-für-heute",
    "href": "07_VL_D.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung\n\nQuiz und kurzes Recap logistische Regression\nModellgestützte Analysen\n\nWeitere Modellevaluation\nMultiple Logistische Regression\nMultinominale Logistische Regression"
  },
  {
    "objectID": "07_VL_D.html#ablauf",
    "href": "07_VL_D.html#ablauf",
    "title": "Business Intelligence & Data Science",
    "section": "Ablauf",
    "text": "Ablauf\nAktualisierter Ablaufplan\n\n\n\n\n\nBlock\nGruppe D\nThema\n\n\n\n\n1\n21.03.2024:09:00 – 10:30\nOrganisation, Einleitung\n\n\n2\n21.03.2024:10:45 – 12:15\nDatenbereitstellung: Data Warehousing\n\n\n3\n28.03.2024:13:00 – 14:30\nDatentransformation\n\n\n4\n28.03.2024:14:45 – 16:15\nBig Data und Data Lake\n\n\n5\n02.04.2024:13:00 – 14:30\nInformationsgenerierung: Berichtsorientierte Analysen\n\n\n6\n02.04.2024:14:45 – 16:15\nAdvanced und Predictive Analytics: Grundlagen\n\n\n7\n15.04.2024:13:00 – 14:30\nAdvanced und Predictive Analytics: Klassifikation\n\n\n8\n15.04.2024:14:45 – 16:15\nAdvanced und Predictive Analytics: Klassifikation\n\n\n9\n02.05.2024:13:00 – 14:30\nRestinhalte Klassifikation; Assoziationsanalyse; Probeklausur (30 Minuten)\n\n\n10\n02.05.2024:14:45 – 16:15\nBesprechung Probeklausur; Evaluation; Fragen\n\n\n-\n13.05.2024:13:00 – 14:30\nKlausur (60 Minuten)"
  },
  {
    "objectID": "07_VL_D.html#modellevaluation",
    "href": "07_VL_D.html#modellevaluation",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision\n\n\nAndere Metriken beheben die Schwächen der Accuracy\nPrecision oder Präzision gibt an, wie viele der vom Klassifikator als positiv identifizierten Fälle tatsächlich positiv sind und entspricht dem Anteil der tatsächlich positiven Fälle an der Menge aller als positiv klassifizierten Fälle\n\n\n\\[\\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\\]\n\n\nWelcher Anteil der positiven Identifikationen war tatsächlich korrekt? Oder: Wenn das Modell einen Datenpunkt positiv klassifiziert, wie wahrscheinlich ist es, dass diese Klassifikation richtig ist?\nEine hohe Precision bedeutet also, dass der Klassifikator nur wenige irrelevante Fälle als relevant einstuft und ein Klassifikator mit einer Precision von 1,0 liefert keine FP."
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-1",
    "href": "07_VL_D.html#modellevaluation-1",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nRecall\n\n\nRecall hingegen gibt an, wie viele der tatsächlich positiven Fälle vom Klassifikator als positiv bzw. relevant erkannt wurden:\n\n\n\\[\\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}}\\]\n\n\nEin hoher Recall-Wert bedeutet, dass der Klassifikator viele relevante Fälle erkennt und beantwortet die Frage, welcher Anteil der positiven Ergebnisse richtig identifiziert wurde.\nWie wahrscheinlich ist es, dass das Modell einen positiven Datenpunkt erkennt?\nAuch häufig als Sensitivität oder True Positive Rate bezeichnet\n\n\n\n\nKasten: Sample aller Beobachtungen\nLinker Kasten: Positive Beobachtungen\nRechte Kasten: Negative Beobachtungen\nKreis: Vorhersage des Modells, also ALLE als positiv vorhergesagte Datenpunkte\nKreis besteht aus zwei Hälften: TP und FN"
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-2",
    "href": "07_VL_D.html#modellevaluation-2",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision und Recall\n\n\n\nIllustration Precision, Recall, Accuracy, Quelle: Maleki u. a. (2020)\n\n\n\n\nKasten: Sample aller Beobachtungen\nLinker Kasten: Positive Beobachtungen\nRechte Kasten: Negative Beobachtungen\nKreis: Vorhersage des Modells, also ALLE als positiv vorhergesagte Datenpunkte\nKreis besteht aus zwei Hälften: TP und FN"
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-3",
    "href": "07_VL_D.html#modellevaluation-3",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision und Recall für den Spam-Filter\n\n\n\n\nUm Precision und Recall bei unausgewogenen Datensätzen zu illustrieren, betrachten wir erneut den Spam-Filter\nFür Precision erhalten wir\n\n\n\\[\\text{Precision} = \\frac{\\text{1}}{\\text{1 + 0}} = 1\\]\n\n\nFür Recall ergibt sich\n\n\n\\[\\text{Recall} = \\frac{\\text{1}}{\\text{1 + 99}} = 0,01.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\nSumme Vorhersage\n\n\n\n\nVorhergesagt Positiv\n1\n0\n1\n\n\nVorhergesagt Negativ\n99\n900\n999\n\n\nSumme Tatsächlich\n100\n900\n1000\n\n\n\n\n\n\nWenn das Modell eine Mail als Spam klassifiziert, ist diese Prognose zu 100% korrekt\nDas Modell erkennt aber nur 1% der tatsächlichen Spam-Mails."
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-4",
    "href": "07_VL_D.html#modellevaluation-4",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision und Recall für den Spam-Filter\n\nWelches Maß ist nun das Richtige?\n\n\nWenn ein balancierter Datensatz vorliegt, kann Accuracy bedenkenlos verwendet werden, um das Modell zu evaluieren\nWenn wir sicher sein wollen, dass eine positive Vorhersage korrekt ist, dann ist Precision die angemessene Evaluationsmetrik. Dies ist oft der Fall, wenn FP mit höheren Kosten verbunden sind, als FN.\nWenn es wichtig ist, so viele positive Fälle wie möglich zu identifizieren, sollte ein Modell mit hohem Recall verwendet werden. In diesem Fall sind die Kosten von FN besonders hoch, sodass es besser ist, einige Fälle fälschlicherweise negativ zu klassifizieren, als dass uns tatsächlich positive Fälle durch die Lappen gehen"
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-5",
    "href": "07_VL_D.html#modellevaluation-5",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nF1-Score\n\nOft sind Kosten und Nutzen von FP und FN Klassifikationen nicht eindeutig\nDer F1 Score kompensiert die Nachteile der Accuracy bei unbalancierten Datensätzen zu kompensieren und legt gleichzeitig einen ausgewogenen Fokus auf Precision und Recall:\n\n\n\\[\nF1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\nF1 ist das harmonische Mittel von Precision und Recall und liegt zwischen 0 und 1"
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-6",
    "href": "07_VL_D.html#modellevaluation-6",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nF1-Score\n\nIm Spam Beispiel:\n\n\n\\[\nF1 = 2 \\cdot \\frac{\\text{1} \\cdot \\text{0,01}}{\\text{1} + \\text{0,01}}= 0,0198\n\\]\n\n\nDieser Wert ist deutlich niedriger, als uns die 90,1% Accuracy zunächst suggerieren.\nTrotzdem ist das Beispielmodell nahezu nutzlos, wie anhand des niedrigen F1-Scores erkennbar wird."
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-7",
    "href": "07_VL_D.html#modellevaluation-7",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nAbschließende Bemerkungen\n\nBei einem balancierten Datensatz und gleicher Gewichtung von FP und FN ist die Accuracy die einfachste und intuitivste Metrik\nBei unbalancierten Datensätzen sollten Precision, Recall und F1-Score verwendet werden\nWenn FP höhere Kosten haben, dann sollte Precision im Fokus stehen\nWenn FN höhere haben, dann sollte Recall im Fokus stehen\nDer F1-Score ist besonders nützlich, wenn Precision und Recall gleiche Gewichtung haben"
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-8",
    "href": "07_VL_D.html#modellevaluation-8",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKosten von FP und FN\n\n\nDie meisten Klassifikationsmethoden basieren auf der Annahme, dass FP und FN gleich problematisch sind\nIn praktischen Szenarios ist dies jedoch selten der Fall, Beispiel:\nBetrugserkennung: Eine Haftpflichtversicherung prüft Schadensmeldungen mit ML. Das Modell soll den Schaden automatisch abwickelnd und entweder als “Zahlung” oder “Keine Zahlung” klassifizieren, je nachdem ob ein Betrugsversuch vorliegt\nFragen:\n\nWas ist die positive (interessante) Klasse aus Sicht der Versicherung?\nWelche Fälle sind dann FN und FP?\nWelche Fehlklassifikation ist teurer? Welches Maß sollte für das Modell maximiert werden?\n\n\n\n\n\nAntworten:\n\nDie interessante Klasse ist die Zahlung des Schadens\nFN: Die Forderung ist berechtigt, aber die Versicherung zahlt nicht\nFP: Die Forderung ist unberechtigt, aber die Versicherung zahlt\nFP ist teurer, da die Versicherung zahlen muss, deshalb sollte Precision im Vordergrund stehen"
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-9",
    "href": "07_VL_D.html#modellevaluation-9",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKosten von FP und FN\n\nIdentifikation einer ansteckenden Krankheit: Ein medizinisches Modell identifiziert eine ansteckende Krankheit mit der Eingabe weniger Symptome. Anschließend wird die Person isoliert und umfänglich getestet\n\n\nFragen:\n\nWas ist die interessante Klasse aus Sicht einer Gesundheitsbehörde?\nWelche Fälle sind dann FN und FP?\nWelche Fehlklassifikation ist teurer? Welches Maß sollte für das Modell maximiert werden?\n\n\n\nAntworten:\n\nDie interessante Klasse ist die Identifikation der Krankheit\nFN: Die Person ist krank, wird aber nicht isoliert\nFP: Die Person ist nicht krank, wird aber isoliert\nFN ist teurer, da die Krankheit weiterverbreitet wird, deshalb sollte das Modell einen hohen Recall haben"
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-10",
    "href": "07_VL_D.html#modellevaluation-10",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nTauziehen zwischen Precision und Recall\n\nPrecision und Recall sind in binären Klassifikationsmodellen oft invers proportional, eine Erhöhung von Precision führt zu einer Verringerung von Recall (und umgekehrt)\nDieses Tauziehen kann durch die Anpassung des Cut-Off Points \\(C\\) beeinflusst werden\nZur Illustration noch einmal zurück zur interaktiven Visualisierung:\n\nhttps://bi-and-ds-logistic-regression-qkwupfgvpq-ey.a.run.app/\n\nFragen:\n\nWas passiert mit Precision und Recall, wenn \\(C\\) ansteigt (sinkt)?\nFür welche Werte von \\(C\\) erhalten wir maximale Precision und Recall?"
  },
  {
    "objectID": "07_VL_D.html#modellevaluation-11",
    "href": "07_VL_D.html#modellevaluation-11",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nTauziehen zwischen Precision und Recall\n\n\n\nPrecision und Recall in Abhängigkeit von \\(C\\). Quelle: Shi u. a. (2009)"
  },
  {
    "objectID": "07_VL_D.html#logistische-regression",
    "href": "07_VL_D.html#logistische-regression",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrere unabhängige Variablen\n\n\n\nBisher haben wir nur eine unabhängige Variable, Energy, betrachtet\nDa die Aufteilung jedoch nicht perfekt war, ist es sinnvoll, weitere Features zu verwenden, bspw. Danceability\nZur Identifikation geeigneter Variablen lassen sich erneut Box-Plots oder Violin-Plots verwenden\nZur Erinnerung: Auch Danceability scheint geeignet, um zwischen EDM und Klassik zu unterscheiden"
  },
  {
    "objectID": "07_VL_D.html#logistische-regression-1",
    "href": "07_VL_D.html#logistische-regression-1",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrere unabhängige Variablen\n\nBei der Hinzunahme mehrerer Variablen ist neben der Korrelation mit der abhängigen Variable auch die Korrelation zwischen den unabhängigen Variablen zu beachten\nBei hoher positiver Korrelation zwischen den Variablen ist es möglich, dass die Hinzunahme dieser Variablen zum Modell keinen zusätzlichen Informationsgewinn liefert\nAuch besteht das Risiko der Multikollinearität, die zu instabilen Koeffizienten und einer schlechten Modellperformance führen kann\nDie Korrelation zwischen Danceability und Energy beträgt 0,8, ist also hoch, aber noch nicht bedenklich"
  },
  {
    "objectID": "07_VL_D.html#logistische-regression-2",
    "href": "07_VL_D.html#logistische-regression-2",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrere unabhängige Variablen\n\n\n\nBei zwei unabhängigen Variablen lässt sich der Zusammenhang zwischen den Variablen und der abhängigen Variable nach wie vor grafisch darstellen, diesmal als Scatter Plot"
  },
  {
    "objectID": "07_VL_D.html#logistische-regression-3",
    "href": "07_VL_D.html#logistische-regression-3",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrere unabhängige Variablen\n\nAllgemein hat das logistische Modell mit mehreren unabhängigen Variablen die Form:\n\n\n\\[\nP(y = 1|X) = \\frac{ e^{(\\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\ldots + \\beta_P \\cdot X_P)}}{1 + e^{(\\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\ldots + \\beta_P \\cdot X_P)}}\n\\]\n\n\nMit den beiden Variablen Energy und Danceability erhalten wir folgendes Modell:\n\n\n\\[\nP(EDM = 1|\\text{Energy, Danceability}) = \\\\ \\frac{ e^{(\\beta_0 + \\beta_1 \\cdot \\text{Energy} + \\beta_2 \\cdot \\text{Danceability})}}{1 + e^{(\\beta_0 + \\beta_1 \\cdot \\text{Energy} + \\beta_2 \\cdot \\text{Danceability})}}\n\\]"
  },
  {
    "objectID": "07_VL_D.html#logistische-regression-4",
    "href": "07_VL_D.html#logistische-regression-4",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrere unabhängige Variablen\n\n\nDie Koeffizienten für die unabhängigen Variablen sind:\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n-38.1001\n\n\nenergy\n27.2445\n\n\ndanceability\n41.1560\n\n\n\n\n\n\n\n\n\nAuch wenn die Interpretation der Koeffizienten bei Klassifikation weniger wichtig ist, zwei Anmerkungen zur Interpretation:\n\nDa der Zusammenhang zwischen den unabhängigen Variablen und der abhängigen Variable nicht linear ist, können wir die Koeffizienten nicht direkt interpretieren wie bei der linearen Regression\nDie Vorzeichen der Koeffizienten geben jedoch an, ob die unabhängige Variable positive oder negative Auswirkungen auf die abhängige Variable hat"
  },
  {
    "objectID": "07_VL_D.html#logistische-regression-5",
    "href": "07_VL_D.html#logistische-regression-5",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMultiple Logistische Regression\n\n\n\nFür das Modell mit zwei Variablen erhalten wir die Konfusionsmatrix auf der rechten Seite.\nAccuracy, Precision, Recall und F1-Score sind:\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\naccuracy\n0.9933\n\n\nrecall\n0.9930\n\n\nprecision\n0.9930\n\n\nf_meas\n0.9930"
  },
  {
    "objectID": "07_VL_D.html#logistische-regression-6",
    "href": "07_VL_D.html#logistische-regression-6",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMultiple Logistische Regression\n\n\n\n\nMit dem Scatter Plot lassen sich die Vorhersagen des Modells visualisieren und evaluieren\nDie gestrichelte Linie ist die Decision Boundary, also die Linie, auf der die Wahrscheinlichkeit für beide Klassen gleich 0,5 ist"
  },
  {
    "objectID": "07_VL_D.html#logistische-regression-7",
    "href": "07_VL_D.html#logistische-regression-7",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMultiple Logistische Regression\n\n\n\nUm die Performance des Modells systematisch zu optimieren, können wir erneut den Cut-Off Point \\(C\\) anpassen\nStatt Trial & Error ist es sinnvoll, Precision und Recall für verschiedene Werte von \\(C\\) zu berechnen und zu visualisieren"
  },
  {
    "objectID": "07_VL_D.html#logistische-regression-8",
    "href": "07_VL_D.html#logistische-regression-8",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nZusammenfassung binäre Klassifikation\n\nDie binäre Klassifikation erfordert zunächst die Umwandlung der abhängigen Variable in eine Dummy-Variable, kodiert mit 0 und 1\nZur Vereinfachung der Interpretation wird die interessante Klasse als 1 kodiert\nDas Modell berechnet für jeden Datenpunkt eine Wahrscheinlichkeit, dass die Beobachtung zur interessanten Klasse gehört\nAnhand eines Cut-Off Points \\(C\\) wird entschieden, ob die Beobachtung zur interessanten Klasse gehört\nDie Wahl der Evaluationsmetrik hängt von den Kosten von FP und FN und der Klassenungleichgewichtung ab\nKonfusionsmatrix, Accuracy, Precision, Recall und F1-Score sind gängige Metriken zur Modellbewertung"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression",
    "href": "07_VL_D.html#multinominale-logistische-regression",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nErweiterung auf mehrere Klassen\n\nAuch bei mehreren unabhängigen Variablen bleibt die logistische Regression ein binäres Modell\nZur Erweiterung auf \\(K&gt;2\\) Klassen wird die multinomiale logistische Regression verwendet, bei der zunächst eine Referenz- oder Baseline-Klasse festgelegt wird\nZur Schätzung der Koeffizienten werden anschließend alle \\(K-1\\) Klassen paarweise separat gegen die Referenzklasse regressiert\nAnschließend lassen sich die Klassenzugehörigkeiten ähnlich wie im binären Modell vorhersagen"
  },
  {
    "objectID": "07_VL_D.html#logistische-regression-9",
    "href": "07_VL_D.html#logistische-regression-9",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrklassen-Klassifikation\n\n\nFür die Klassen \\(k = 1, ..., K-1\\) geschieht dies mit\n\n\n\\[\np(y_i = k|X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + ... + \\beta_{kp}x_p}}{1+\\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + ... + \\beta_{lp}x_p} }\n\\]\n\nund für die Referenzklasse \\(K\\) mit\n\n\\[\np(y_i = k|X = x) = \\frac{1}{1+\\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + ... + \\beta_{lp}x_p} }\n\\]\n\n\nDabei sind \\(\\beta_{k0}, ..., \\beta_{kp}\\) die Koeffizienten für die Klasse \\(k\\) und \\(x_1, ..., x_p\\) die unabhängigen Variablen"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression-1",
    "href": "07_VL_D.html#multinominale-logistische-regression-1",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nNeue Daten von Tofispy\n\nTofispy hat uns neue Daten zur Verfügung gestellt:\n\nNeben Klassik und EDM ist jetzt auch Hip-Hop enthalten\nNach wie vor haben wir die beiden Variablen Energy und Danceability zur Verfügung\n\nErneut teilen wir die Daten in Trainings- und Testdaten auf\nDiesmal verwenden wir 70% der Daten für das Training und 30% für das Testen\nDie Bewertung des Modells erfolgt dann anhand der Testdaten"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression-2",
    "href": "07_VL_D.html#multinominale-logistische-regression-2",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nErgebnisse\n\nFür das multinominale Modell erhalten wir jeweils Koeffizienten für \\(K-1\\) Klassen, bis auf die Referenzklasse \\(K\\)\nFür das Beispiel mit den drei Genres Klassik, EDM und Hip-Hop erhalten wir folgende Koeffizienten\n\n\n\n\n\n\n\n\nClass\nVariable\nCoefficient\n\n\n\n\nHip-Hop\n(Intercept)\n5.1263\n\n\nHip-Hop\nenergy\n-9.7410\n\n\nHip-Hop\ndanceability\n3.2574\n\n\nKlassik\n(Intercept)\n23.7448\n\n\nKlassik\nenergy\n-30.5953\n\n\nKlassik\ndanceability\n-15.4205"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression-3",
    "href": "07_VL_D.html#multinominale-logistische-regression-3",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nWahrscheinlichkeit und Klassenzugehörigkeit\n\nAuch das Modell der multinomialen logistischen Regression gibt für jede Klasse eine Wahrscheinlichkeit aus\nAnders als im binären Modell wird jedoch nicht nur die Wahrscheinlichkeit für eine Klasse ausgegeben, sondern für alle Klassen\nDie Klassenzugehörigkeit wird dann anhand der höchsten Wahrscheinlichkeit bestimmt, ein Cut-Off Point ist nicht notwendig"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression-4",
    "href": "07_VL_D.html#multinominale-logistische-regression-4",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nPrognose aus dem multinominalen Modell\n\nDie Prognose für die jeweilligen Klassen erfolgt erneut durch einsetzen der Koeffizienten in die logistische Funktion\nFür Hip-Hop erhalten wir beispielsweise\n\n\n\n\\[\n\\operatorname{\\widehat{P}(Hip-Hop)|\\text{Energy, Dance}} = \\\\\n\\frac{e^{5.13  -9.74 \\cdot \\text{Energy} + 3.26 \\cdot \\text{Dance}}}{1 + e^{5.13  -9.74 \\cdot \\text{Energy} + 3.26 \\cdot \\text{Dance}} +\ne^{23.74  -30.6 \\cdot \\text{Energy}  -15.42 \\cdot \\text{Dance}}}  \n\\]\n\n\n\nUnd für die Referenzklasse EDM:\n\n\n\n\\[\n\\operatorname{\\widehat{P}(Hip-Hop)|\\text{Energy, Dance}} = \\\\\n\\frac{1}{1 + e^{5.13  -9.74 \\cdot \\text{Energy} + 3.26 \\cdot \\text{Dance}} +\ne^{23.74  -30.6 \\cdot \\text{Energy}  -15.42 \\cdot \\text{Dance}}}  \n\\]"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression-5",
    "href": "07_VL_D.html#multinominale-logistische-regression-5",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nPrognose aus dem multinominalen Modell\n\nDa die händische Berechnung viel zu aufwendig ist, hier ein Beispiel für 10 ausgewählte Songs:\n\n\n\n\n\n\n\n\npred_class\ncategory\np_EDM\np_Hip-Hop\np_Klassik\ntrack.name\ntrack.artist\nenergy\ndanceability\n\n\n\n\nEDM\nEDM\n0.8015\n0.1985\n0.0000\nSamaria\nEuggy\n0.9080\n0.713\n\n\nEDM\nEDM\n0.8665\n0.1335\n0.0000\nMake It Work\nBklava\n0.9540\n0.705\n\n\nHip-Hop\nHip-Hop\n0.1855\n0.8145\n0.0000\nOKAY!\nOG Keemo\n0.6850\n0.929\n\n\nEDM\nEDM\n0.6489\n0.3511\n0.0000\nBlack Swan\nAxel N.\n0.8070\n0.651\n\n\nKlassik\nKlassik\n0.0000\n0.0000\n1.0000\nPréludes - Book 2, L.123: 11. Les tierces alternées\nClaude Debussy\n0.0633\n0.279\n\n\nHip-Hop\nHip-Hop\n0.1179\n0.8796\n0.0025\nTipsy\nDizzy\n0.5520\n0.694\n\n\nHip-Hop\nHip-Hop\n0.4729\n0.5271\n0.0000\nOvernight Celebrity\nTwista\n0.7920\n0.828\n\n\nHip-Hop\nHip-Hop\n0.2642\n0.7358\n0.0000\nThe Humpty Dance\nDigital Underground\n0.6930\n0.813\n\n\nKlassik\nKlassik\n0.0000\n0.0000\n1.0000\nPapa, Can You Hear Me?\nElliott Jack Sansom\n0.0128\n0.452\n\n\nKlassik\nKlassik\n0.0000\n0.0005\n0.9994\nPeaceful Here Now\nColin Stetson\n0.3170\n0.241"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression-6",
    "href": "07_VL_D.html#multinominale-logistische-regression-6",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nEvaluationsmetriken\n\nDie Evaluationsmetriken für die multinominale logistische Regression sind analog zum binären Modell, Accuracy, Precision, Recall und F1-Score können auch hier verwendet werden\nAuch die Konfusionsmatrix ist ein geeignetes Mittel zur Modellbewertung, umfasst nun aber alle Klassen und nicht nur vier Felder\nAccuracy entspricht dann nach wie vor der Summe aller richtig klassifizierten Fälle geteilt durch die Gesamtanzahl der Fälle\nAndere Metriken können entweder für jede Klasse einzeln berechnet oder gemittelt werden, um ein Gesamtbild der Modellperformance zu erhalten"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression-7",
    "href": "07_VL_D.html#multinominale-logistische-regression-7",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nEvaluationsmetriken\n\nBei der Aggregation wird häufig Macro-Averaging verwendet, bei dem der gleichgewichtete Durchschnitt über alle Klassen berechnet wird\nZum Beispiel für Precision ergibt sich bei Macro-Averaging:\n\n\n\n\\[\nPrecision_{\\text{macro}} = \\frac{1}{K} \\sum_{k=1}^{K} Precision_k\n\\]\n\n\n\nWobei \\(K\\) die Anzahl der Klassen ist\nAnalog für Recall möglich"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression-8",
    "href": "07_VL_D.html#multinominale-logistische-regression-8",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nEvaluationsmetriken\n\n\n\nDie Konfusionsmatrix für das Modell mit drei Klassen ist auf der rechten Seite dargestellt\nIm Durchschnitt über alle Klassen erhalten wir die folgenden Metriken:\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\naccuracy\n0.8478\n\n\nrecall\n0.8454\n\n\nprecision\n0.8479\n\n\nf_meas\n0.8464"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression-9",
    "href": "07_VL_D.html#multinominale-logistische-regression-9",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nEvaluationsmetriken\n\nDie Performance des Modells weicht zwischen den Klassen oft ab, häufig auch in einer Klassenungleichgewichtung begründet\nAus diesem Grund ist es manchmal sinnvoll, die Metriken für jede Klasse einzeln zu betrachten\nFür Precision:\n\n\n\n\\[\nPrecision_{\\text{k}} = \\frac{TP_{\\text{k}}}{TP_{\\text{k}} + FP_{\\text{k}}} = \\frac{TP_{\\text{k}}}{\\text{# vorhergesagte Klasse k}}\n\\]\n\n\n\nund für Recall:\n\n\n\n\\[\nRecall_{\\text{k}} = \\frac{TP_{\\text{k}}}{TP_{\\text{k}} + FN_{\\text{k}}} = \\frac{TP_{\\text{k}}}{\\text{# tatsächliche Klasse k}}\n\\]"
  },
  {
    "objectID": "07_VL_D.html#multinominale-logistische-regression-10",
    "href": "07_VL_D.html#multinominale-logistische-regression-10",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nVergleich Aggregation vs. Einzelmetriken\n\n\n\nMacro-Averaging\n\nEinfach interpretierbar, Reduktion auf eine Zahl\nSinnvoll wenn alle Klassen gleich wichtig sind oder Datensatz ausgewogen\nKann schlechte Performance in kleinen Klassen verbergen\n\n\n\n\nBerechnung pro Klasse\n\nWenn nur bestimmte Klassen interessant sind kann man gezielt evaluieren\nAuch bei unbalancierten Datensätzen sinnvoll, um die Performance kleiner Klassen hervorzuheben\nKann zu unübersichtlichen Ergebnissen führen, wenn viele Klassen vorhanden sind\n\n\n\n\n\nDa wir vornehmlich balancierte Klassen betrachten, ist Macro-Averaging in unsere Zwecke ausreichend"
  },
  {
    "objectID": "07_VL_D.html#quellen",
    "href": "07_VL_D.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, und Robert Tibshirani. 2021. An Introduction to Statistical Learning: with Applications in R. Second edition. Springer texts in statistics. New York NY: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nMaleki, Farhad, Katie Ovens, Keyhan Najafian, Behzad Forghani, Caroline Md, und Reza Forghani. 2020. „Overview of Machine Learning Part 1“. Neuroimaging Clinics of North America 30 (November): e17–32. https://doi.org/10.1016/j.nic.2020.08.007.\n\n\nShi, Feng, Juanzi Li, Jie Tang, Guotong Xie, und Hanyu Li. 2009. „Actively Learning Ontology Matching via User Interaction“. In, 5823:585–600. https://doi.org/10.1007/978-3-642-04930-9_37."
  },
  {
    "objectID": "06_VL.html#der-plan-für-heute",
    "href": "06_VL.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung"
  },
  {
    "objectID": "06_VL.html#der-plan-für-heute-1",
    "href": "06_VL.html#der-plan-für-heute-1",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung\n\nNeue Daten von Tofispy\nVisualisierung von Verteilungen\nModellgestützte Analysen\n\nEinfache Logistische Regression\nModellevaluation\n\nWie gut kann man Musik-Genres klassifizieren?"
  },
  {
    "objectID": "06_VL.html#business-case",
    "href": "06_VL.html#business-case",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nRecap Tofispy\n\nUnsere bisherigen Analysen haben gezeigt:\n\nTofispy verliert Marktanteile gegenüber der Konkurrenz, insbesondere gegenüber Youtube Music\nDer Grund ist vorallem ein stark ansteigender Trend bei den Kündigungen, während die Registrierungen linear wachsen\nEine Befragung nach der Kündigung hat ergeben:\n\nDie Mehrheit hört Musik über Playlists (55.8%)\nDer Großteil ist unzufrieden mit den Empfehlungen (69.2%)\n\n\nHandlungsempfehlung: Verbesserung der empfohlenen Playlists"
  },
  {
    "objectID": "06_VL.html#business-case-1",
    "href": "06_VL.html#business-case-1",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nNeue Daten von Tofispy\n\n\nZum Einstieg in die Empfehlungsverbesserung hat Tofispy neue Daten bereitgestellt\nDie Daten enthalten eine Auswahl an Songs aus den beiden Genres Klassik und Dance/Electronic (EDM)\nDie Daten enthalten:\n\nSong ID, Name und Artists,\nGenre als Label, manuell erstellt\nFeatures:\n\nTempo (BPM)\nDanceability (0-1), beschreibt wie gut der Song zum Tanzen geeignet ist\nEnergy (0-1), beschreibt wie energiegeladen der Song ist, wobei energiegeladene Songs schnell, intensiv und laut sind"
  },
  {
    "objectID": "06_VL.html#business-case-2",
    "href": "06_VL.html#business-case-2",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nNeue Daten von Tofispy\n\nDie Daten sind via Superset zur Verfügung gestellt und befinden sich im Schema “Classification” und Dataset “training_logistic”\nDie Daten sind bereits bereinigt und enthalten keine fehlenden Werte\nTofispy generiert jeden Freitag – pünktlich zum Wochenende – eine neue EDM Playlist mit den neuen Releases der Woche und bittet uns, die Klassifizierung für die nächste Playlist zu erstellen\nErste Frage: Wie gut sind die Daten geeignet, um zwischen den beiden Genres zu unterscheiden?"
  },
  {
    "objectID": "06_VL.html#visualisierung-von-verteilungen",
    "href": "06_VL.html#visualisierung-von-verteilungen",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nHistogramm\n\n\n\n\nEinfache Möglichkeit, die Verteilung einer numerischen Variable zu visualisieren\nDie einzelnen Werte der entsprechenden Variable werden in sogenannte Bins gruppiert\nDie Anzahl der Werte in jedem Bin wird gezählt und als Balken dargestellt\nDie Breite der Balken entspricht der Breite der Bins und die Höhe der Balken entspricht der Anzahl der Werte pro Bin\nDie Anzahl der Bins ist wichtig, zu wenige verschleiern den Detailgrad der Verteilung, zu viele Bins können zu zu granular sein\nViele Tools optimieren die Bin-Zahl, oft aber Trial & Error"
  },
  {
    "objectID": "06_VL.html#visualisierung-von-verteilungen-1",
    "href": "06_VL.html#visualisierung-von-verteilungen-1",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nHistogramm bei mehreren Dimensionen\n\n\n\n\nHistogramme sind ideal, um einzelne Verteilung zu visualisieren\nBei mehreren Dimensionen und ähnlichen Verteilungen drohen sich die Verteilungen zu überlagern\nHistogramme sind nicht mehr aussagekräftig\nIn diesen Fällen sind mehrere Histogramme zu empfehlen\nHierbei auf passende X-Achsen achten, um Vergleiche zu erleichtern"
  },
  {
    "objectID": "06_VL.html#visualisierung-von-verteilungen-2",
    "href": "06_VL.html#visualisierung-von-verteilungen-2",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nBox-Plot\n\nBox Plots sind eine weitere Möglichkeit, die Verteilung einer numerischen Variable über verschiedene Gruppen hinweg zu visualisieren\nErmöglichen es, viele Gruppen gleichzeitig zu visualisieren und zu vergleichen\nBox-Plots sind einfach, aber hochinformativ, wurden in den 1970er Jahren von John Tukey entwickelt und gewannen schnell an Popularität, da sie sich einfach per Hand zeichnen ließen\nBox-Plots werden oft auch als Box-and-Whisker-Plots bezeichnet"
  },
  {
    "objectID": "06_VL.html#visualisierung-von-verteilungen-3",
    "href": "06_VL.html#visualisierung-von-verteilungen-3",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nBox-Plot\n\n\n\n\nDie Punktewolke illustriert die Verteilung der Y-Werte der Rohdaten\nDie Linie in der Mitte des Box-Plots repräsentiert den Median, und die Box umschließt die mittleren 50% der Daten\nDie Obergrenze (Untergrenze) der Box ist damit das obere bzw. untere Quartil\nDie oberen und unteren sogenannten Whisker erstrecken sich meist bis zum Maximum und Minimum der Daten\nAlternativ entsprechen die Whisker das 1,5 fache des Interquartilabstands (IQR)\n\nEinzelne Datenpunkte, die über die Grenzen hinausgehen sind Ausreißer bezeichnet und werden als einzelne Punkte dargestellt\n\n\n\n\n\n\nAnatomie eines Boxplots Quelle: Wilke (2019)"
  },
  {
    "objectID": "06_VL.html#visualisierung-von-verteilungen-4",
    "href": "06_VL.html#visualisierung-von-verteilungen-4",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nBox-Plot\n\n\n\n\n\nDie Punktewolke illustriert die Verteilung der Y-Werte der Rohdaten\nDie Linie in der Mitte des Box-Plots repräsentiert den Median, und die Box umschließt die mittleren 50% der Daten\nDie Obergrenze (Untergrenze) der Box ist damit das obere bzw. untere Quartil\nDie oberen und unteren sogenannten Whisker erstrecken sich meist bis zum Maximum und Minimum der Daten\nAlternativ entsprechen die Whisker das 1,5 fache des Interquartilabstands (IQR)\n\nEinzelne Datenpunkte, die über die Grenzen hinausgehen sind Ausreißer bezeichnet und werden als einzelne Punkte dargestellt"
  },
  {
    "objectID": "06_VL.html#visualisierung-von-verteilungen-5",
    "href": "06_VL.html#visualisierung-von-verteilungen-5",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nViolin-Plot\n\n\n\n\nDa händische Zeichnungen heute weniger wichtig sind, werden Boxplots in letzter Zeit verstärkt von Violin-Plots abgelöst.\nStatt Boxen und Whiskern zeigen Violin-Plots die gesamte Verteilung der Daten entlang der Y-Achse\nDer dickste Teil des Violins entspricht der höchsten Punktendichte im Datensatz\nViolins sind symmetrisch und beginnen und enden bei den minimalen und maximalen Datenwerten und vergleichbar mit stetigen Histogrammen, die um 90 Grad gedreht sind\n\n\n\n\n\n\nAnatomie eines Boxplots Quelle: Wilke (2019)"
  },
  {
    "objectID": "06_VL.html#visualisierung-von-verteilungen-6",
    "href": "06_VL.html#visualisierung-von-verteilungen-6",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nViolin-Plot\n\n\n\n\n\nDa händische Zeichnungen heute weniger wichtig sind, werden Boxplots in letzter Zeit verstärkt von Violinplots abgelöst.\nStatt Boxen und Whiskern zeigen Violin-Plots die gesamte Verteilung der Daten entlang der Y-Achse\nDer dickste Teil des Violins entspricht der höchsten Punktendichte im Datensatz\nViolins sind symmetrisch und beginnen und enden bei den minimalen und maximalen Datenwerten und vergleichbar mit stetigen Histogrammen, die um 90 Grad gedreht sind"
  },
  {
    "objectID": "06_VL.html#visualisierung-von-verteilungen-7",
    "href": "06_VL.html#visualisierung-von-verteilungen-7",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nBox-Plots mit Superset\n\nSuperset erlaubt es, Box-Plots direkt zu erstellen, Violin-Plots in der verfügbaren Version jedoch nicht\nHierzu wählen wir den üblichen Weg:\n\nAuf der Startseite oben rechts + Chart\nUnter Dataset wählen wir “training_logistic”\nAnschließend suchen wir nach “Box Plot” und wählen es aus"
  },
  {
    "objectID": "06_VL.html#visualisierung-von-verteilungen-8",
    "href": "06_VL.html#visualisierung-von-verteilungen-8",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Verteilungen",
    "text": "Visualisierung von Verteilungen\nBox-Plots mit Superset"
  },
  {
    "objectID": "06_VL.html#logistische-regression",
    "href": "06_VL.html#logistische-regression",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nTrainings- und Testdaten\n\nBevor wir die von Tofispy bereitgestellten Daten analysieren, teilen wir die Daten in Trainings- und Testdaten auf\nDas ist ein übliches Prozedere, um die Qualität des Modells zu evaluieren und Overfitting zu vermeiden\nDa der Trainingsdatensatz für das Modelltraining verwendet wird, wählt an oft einen Anteil von 70-80% der Daten für das Training und 20-30% für das Testen\nIn unserem Fall verwenden wir nur 300 Beobachtungen, um die Visualisierungen zum Einstieg nicht zu überfrachten\nNeben diesen einfachen Splits gibt es auch komplexere Verfahren wie Kreuzvalidierung"
  },
  {
    "objectID": "06_VL.html#logistische-regression-1",
    "href": "06_VL.html#logistische-regression-1",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nBinäre Klassifikation vs. Multiklassen-Klassifikation\n\nKlassifikationsmodelle können in zwei Kategorien unterteilt werden:\n\nBinäre Klassifikation: Die abhängige Variable hat nur zwei Kategorien\nMultiklassen-Klassifikation: Die abhängige Variable hat mehr als zwei Kategorien\n\nIn unserem Fall haben wir nur zwei Genres, Klassik und EDM, also eine binäre Klassifikation\nDa die Intuition hinter der logistischen Regression einfacher zu verstehen ist, beginnen wir mit der binären Klassifikation"
  },
  {
    "objectID": "06_VL.html#logistische-regression-2",
    "href": "06_VL.html#logistische-regression-2",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nZurück zu unserer Ausgangsgleichung\n\nAusgangspunkt für die logistische Regression ist erneut unsere Grundgleichung \\[\ny = f(X) + \\epsilon,\n\\]\nWie besprochen ist die Variable \\(y\\) bei Klassifikationsproblemen qualitativ oder kategorial\nFür die binäre logistische Regression erfolgt eine Umkodierung in eine sog. Dummy-Variable, also 0 oder 1, im Sinne von Falsch und Richtig\nBeispiel bei zwei Genres in der Variable \\(y\\): \\[\\begin{equation}\ny =\n  \\begin{cases}\n    0 & \\text{Song $i$ Klassik,}\\\\\n    1 & \\text{Song $i$ EDM}\n  \\end{cases}       \n\\end{equation}\\]"
  },
  {
    "objectID": "06_VL.html#logistische-regression-3",
    "href": "06_VL.html#logistische-regression-3",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nIntuition\n\n\n\n\nDie Wahl der Kodierung ist theoretisch willkürlich, aber aus praktischer Sicht ist es sinnvoll, die Kategorie, die interessante Kategorie als 1 zu kodieren\nAls erklärende Variable \\(X\\) können wir beliebig viele Features verwenden, wir beschränken uns aber zunächst auf eine, nämlich Energy\nNach der Umkodierung der abhängigen Variable \\(y\\) sehen unsere Daten aus wie der Auszug rechts\nUnser Ziel ist es, die Wahrscheinlichkeit zu berechnen, mit der ein Song EDM ist, gegeben die Energy\n\n\n\n\n\n\n\n\nedm\nenergy\n\n\n\n\n0\n0.1920\n\n\n1\n0.7250\n\n\n1\n0.9320\n\n\n1\n0.7470\n\n\n1\n0.9220\n\n\n0\n0.0741\n\n\n0\n0.0026\n\n\n0\n0.0592\n\n\n0\n0.0162\n\n\n0\n0.2510"
  },
  {
    "objectID": "06_VL.html#logistische-regression-4",
    "href": "06_VL.html#logistische-regression-4",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nIntuition\n\nNoch intuitiver wird der Zusammenhang, wenn wir die Daten visualisieren mit der abhängigen Variable EDM auf der Y-Achse und der unabhängigen Variable Energy auf der X-Achse\n\n\n\n\n\n\n\nScatter Plot mit Dummy Variable als abhängiger Variable. Eigene Darstellung"
  },
  {
    "objectID": "06_VL.html#logistische-regression-5",
    "href": "06_VL.html#logistische-regression-5",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nWarum nicht einfach lineare Regression?\n\n\nEine lineare Regression ist erste Option, den Zusammenhang zu modellieren\nWir erhalten folgende Koeffizienten:\n\n\n\n\n\\[\n\\operatorname{\\widehat{EDM}} = -0.1 + 1.24 \\cdot \\text{Energy}\n\\]"
  },
  {
    "objectID": "06_VL.html#logistische-regression-6",
    "href": "06_VL.html#logistische-regression-6",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nWarum nicht einfach lineare Regression?\n\nDas lineare Modell erstellt auf der blauen Geraden nun eine Prognose für die Wahrscheinlichkeit, dass ein Song EDM ist und modelliert den Zusammenhang zwischen \\(y\\) und \\(X\\) mit \\[\ny = \\beta_0 + \\beta_1 \\cdot X\n\\]\nDiese Prognose ist aus mehreren Gründen nicht sinnvoll:\n\nDie Prognose kann Werte außerhalb des Intervalls \\([0,1]\\) annehmen\nWie kann man ein Modell mit mehr als 2 Klassen darstellen, bei denen es keine natürliche Ordnung gibt?"
  },
  {
    "objectID": "06_VL.html#logistische-regression-7",
    "href": "06_VL.html#logistische-regression-7",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nDie Logistische Funktion\n\n\nDie logistische Funktion ist eine Sigmoid-Funktion, die Werte zwischen 0 und 1 annimmt und eine S-Form aufweist und damit das erste Problem behebt\nSigmoid Funktionen garantieren, dass alle vorgehersagten Wahrscheinlichkeiten zwischen 0 und 1 liegen, auch wenn die unabhängige Variable \\(X\\) sehr groß oder sehr klein ist"
  },
  {
    "objectID": "06_VL.html#logistische-regression-8",
    "href": "06_VL.html#logistische-regression-8",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nDie Logistische Funktion\n\nDie logistische Regression modelliert die Wahrscheinlichkeit, dass \\(y\\) eine bestimmte Kategorie annimmt: \\[\nP(y = 1|X) = \\frac{ e^{(\\beta_0 + \\beta_1 \\cdot X)}}{1 + e^{(\\beta_0 + \\beta_1 \\cdot X)}}\n\\]\nwobei \\(P(y = 1|X)\\) die Wahrscheinlichkeit ist, dass \\(y\\) die Kategorie 1 annimmt, also EDM ist, gegeben einen Wert für \\(X\\), in unserem Fall Energy\nDas finale Modell sieht dann aus wie folgt: \\[\nP(EDM = 1|Energy) = \\frac{ e^{(\\beta_0 + \\beta_1 \\cdot \\text{Energy})}}{1 + e^{(\\beta_0 + \\beta_1 \\cdot \\text{Energy})}}\n\\]"
  },
  {
    "objectID": "06_VL.html#logistische-regression-9",
    "href": "06_VL.html#logistische-regression-9",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nErgebnis der logistischen Regression\n\nWenn wir statt des linearen Modells ein logistisches Modell verwenden, erhalten wir folgenden Zusammenhang zwischen Energy und der Wahrscheinlichkeit, dass ein Song zum Genre EDM gehört:"
  },
  {
    "objectID": "06_VL.html#logistische-regression-10",
    "href": "06_VL.html#logistische-regression-10",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nErgebnis der logistischen Regression\n\n\n\n\nAus dem Modell erhalten für für jeden Wert von Energy eine Wahrscheinlichkeit, dass ein Song EDM ist\nDie Wahrscheinlichkeit steigt (sinkt) mit steigender Energy und nähert sich 1 (0) an\nDie Steigung der Kurve ist in der Mitte am größten und nimmt zu den Rändern hin ab und die Kurve hat die typische S-Form\nDie Tabelle rechts zeigt die Wahrscheinlichkeiten für 10 zufällig ausgewählte Songs, sortiert nach der Wahrscheinlichkeit\n\n\n\n\n\n\n\n\n\n\nenergy\ntrack.name\ntrack.artist\ncategory\np_edm\n\n\n\n\n0.255\nNaruto: Alone Theme\nToshio Masuda\nKlassik\n0.012\n\n\n0.285\nThe Wife\nJocelyn Pook\nKlassik\n0.022\n\n\n0.323\nbad guy\nVitamin String Quartet\nKlassik\n0.048\n\n\n0.433\nMIDNIGHT\nPLAYAMANE\nEDM\n0.343\n\n\n0.570\nCNTRTE\nAQUIHAYAQUIHAY\nEDM\n0.907\n\n\n0.628\nI'll Take That Back\nAvangart Tabldot\nEDM\n0.971\n\n\n0.629\nOneHundred\nHEDEGAARD\nEDM\n0.972\n\n\n0.636\nEtude No. 9\nKummerspeck\nKlassik\n0.975\n\n\n0.680\nNow\nNiklas Dee\nEDM\n0.990\n\n\n0.690\nMe Provocas\nFumaratto\nEDM\n0.992"
  },
  {
    "objectID": "06_VL.html#logistische-regression-11",
    "href": "06_VL.html#logistische-regression-11",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nGenerierung von Prognosen\n\nAus dem Modell erhalten wir eine Schätzung für die beiden Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\)\nDas gefittete Modell hat folgende Koeffizienten:\n\n\n\\[\n\\hat{P}(EDM = 1|Energy) = \\frac{ e^{(-9.89 + 21.34 \\cdot \\text{Energy})}}{1 + e^{(-9.89 +  21.34\\cdot \\text{Energy})}}\n\\]\n\n\nDurch einfaches Einsetzen des jeweiligen Energy-Wertes erhalten wir die Wahrscheinlichkeit gibt das Modell eine entsprechende Prognose aus"
  },
  {
    "objectID": "06_VL.html#logistische-regression-12",
    "href": "06_VL.html#logistische-regression-12",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nVon Wahrscheinlichkeiten zu Klassifikationen\n\n\nBisher haben wir nur eine Wahrscheinlichkeit für die Klassenzugehörigkeit von Song \\(i\\) berechnet, gegeben den Wert für Energy, also\n\n\n\n\\[\nP(\\text{EDM}_i = 1|\\text{X = Energy}_i)\n\\]\n\n\n\nIm binären Klassifikationsmodell wird ein Schwellenwert oder Cut-Off Point \\(C\\) festgelegt, der bestimmt, ob ein Song als EDM klassifiziert wird oder nicht\nDie Zuordnung folgt dann allgemein nach der Form:\n\n\n\n\\[\\begin{equation}\n  \\text{Klasse} =\n    \\begin{cases}\n      0 & \\text{wenn } P(y_i = 1|X= x_i) \\leq C \\\\\n      1 & \\text{wenn } P(y_i = 1|X= x_i) &gt; C\n    \\end{cases}       \n\\end{equation}\\]\n\n\n\nEin häufig anzutreffender Default-Wert ist \\(C=0.5\\)"
  },
  {
    "objectID": "06_VL.html#logistische-regression-13",
    "href": "06_VL.html#logistische-regression-13",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nErgebnis und Evaluation\n\n\n\n\nMit dem Default-Wert \\(C=0.5\\) erhalten wir die rechts dargestellten Klassifikationen für die 10 Beispiel-Songs\nMit den generierten Klassifikationen lassen sich nun verschiedene Metriken berechnen, um die Qualität des Modells zu bewerten\nDie einfachste Metrik ist die Accuracy, die den Anteil der korrekt klassifizierten Songs angibt\n\n\n\n\n\n\n\n\n\n\nenergy\ntrack.name\ntrack.artist\nedm\np_edm\n.pred_class\n\n\n\n\n0.255\nNaruto: Alone Theme\nToshio Masuda\n0\n0.012\n0\n\n\n0.285\nThe Wife\nJocelyn Pook\n0\n0.022\n0\n\n\n0.323\nbad guy\nVitamin String Quartet\n0\n0.048\n0\n\n\n0.433\nMIDNIGHT\nPLAYAMANE\n1\n0.343\n0\n\n\n0.570\nCNTRTE\nAQUIHAYAQUIHAY\n1\n0.907\n1\n\n\n0.628\nI'll Take That Back\nAvangart Tabldot\n1\n0.971\n1\n\n\n0.629\nOneHundred\nHEDEGAARD\n1\n0.972\n1\n\n\n0.636\nEtude No. 9\nKummerspeck\n0\n0.975\n1\n\n\n0.680\nNow\nNiklas Dee\n1\n0.990\n1\n\n\n0.690\nMe Provocas\nFumaratto\n1\n0.992\n1"
  },
  {
    "objectID": "06_VL.html#modellevaluation",
    "href": "06_VL.html#modellevaluation",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKonfusionsmatrix\n\n\nZur Berechnung der Modellgüte wird die sog. Konfusionsmatrix verwendet, die die Anzahl der korrekt und inkorrekt klassifizierten Beobachtungen zusammenfasst\nIm binären Modell gilt:\n\npositiv: Beobachtung \\(i\\) gehört zur interessanten Klasse (in unserem Fall EDM)\nnegativ: Beobachtung \\(i\\) gehört nicht dazu\n\n\n\n\n\n\n\n\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\nSumme Vorhersage\n\n\n\n\nVorhergesagt Positiv\nTrue Positive (TP)\nFalse Positive (FP)\nSumme Positiv\n\n\nVorhergesagt Negativ\nFalse Negative (FN)\nTrue Negative (TN)\nSumme Negativ\n\n\nSumme Tatsächlich\nSumme Positiv Tats.\nSumme Negativ Tats.\nGesamtsumme"
  },
  {
    "objectID": "06_VL.html#modellevaluation-1",
    "href": "06_VL.html#modellevaluation-1",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKonfusionsmatrix\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\n\n\n\n\nVorhergesagt Positiv\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nVorhergesagt Negativ\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\nTrue Positive (TP): Ein TP liegt vor, wenn das Modell ein Objekt korrekt der relevanten Klasse zuordnet. Beispiele hierfür sind die korrekte Identifikation eines Schadens, die richtige Diagnose einer Krankheit oder die richtige Erkennung von Spam.\nFalse Positive (FP): Ein FP liegt vor, wenn das Modell ein Objekt fälschlicherweise als positiv bzw. relevant klassifiziert, obwohl es tatsächlich negativ ist. Beispiele hierfür sind die Meldung eines nicht vorhandenen Schadens oder die Diagnose einer nicht existierenden Krankheit. FP wird auch als Typ-I Fehler oder Alpha-Fehler bezeichnet."
  },
  {
    "objectID": "06_VL.html#modellevaluation-2",
    "href": "06_VL.html#modellevaluation-2",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKonfusionsmatrix\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\n\n\n\n\nVorhergesagt Positiv\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nVorhergesagt Negativ\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\nTrue Negative (TN): Ein TN liegt vor, wenn das Modell ein Objekt korrekt als negativ, also nicht der relevanten Klasse zugehörig klassifiziert. Beispiele hierfür sind die korrekte Identifikation eines funktionsfähigen Teils oder die richtige Klassifizierung einer Person als gesund.\nFalse Negative (FN): Ein FN liegt vor, wenn das Modell ein Objekt falsch als negativ klassifiziert, obwohl es tatsächlich positiv – also relevant – ist. Synonyme hierfür sind Typ-II Fehler oder Beta-Fehler. Beispiele: die ausbleibende Meldung eines aufgetretenen Schadens oder die falsche Nicht-Diagnose einer existierenden Krankheit"
  },
  {
    "objectID": "06_VL.html#modellevaluation-3",
    "href": "06_VL.html#modellevaluation-3",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKonfusionsmatrix im Beispiel und Berechnung der Accuracy\n\n\n\nUnser einfaches Modell zeigt die rechts dargestellte Konfusionsmatrix\nAus den vier Quadranten lässt sich dann die Accuracy berechnen:\n\n\n\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\] \\[\n= \\frac{156 + 140}{156 + 140 + 2 + 2} = 0.987\n\\]"
  },
  {
    "objectID": "06_VL.html#modellevaluation-4",
    "href": "06_VL.html#modellevaluation-4",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nVor- und Nachteile der Accuracy\n\nDie Accuracy ist eine einfache und intuitive Metrik, die den Anteil der korrekt klassifizierten Beobachtungen angibt\nAllerdings sollte die Accuracy nur bei einem ausgewogenen Datensatz verwendet werden\nAusgewogen oder balanciert bedeutet hier, dass die Anzahl der Beobachtungen in den Klassen ungefähr gleich ist\nIm einfachen Beispiel hier ist das der Fall, weshalb die Accuracy ausreichend ist"
  },
  {
    "objectID": "06_VL.html#modellevaluation-5",
    "href": "06_VL.html#modellevaluation-5",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nAccuracy bei unbalancierten Datensätzen\n\n\n\nIn der Praxis liegen häufig unbalancierte Datensätze vor, sodass die Accuracy allein meist nur geringe Aussagekraft hat\nWir nehmen ein extremes Beispiel mit 1000 E-Mails, von denen 100 Spam-Mails sind\nUnser Modell zur Spam-Erkennung liefert die Konfusionsmatrix rechts\nDas Modell erkennt nur eine Spam-Nachricht, erreicht jedoch eine Accuracy von über 90%, weil die Zahl der TN sehr hoch ist\n\n\n\n\n\n\n\n\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\nSumme Vorhersage\n\n\n\n\nVorhergesagt Positiv\n1\n0\n1\n\n\nVorhergesagt Negativ\n99\n900\n999\n\n\nSumme Tatsächlich\n100\n900\n1000"
  },
  {
    "objectID": "06_VL.html#logistische-regression-14",
    "href": "06_VL.html#logistische-regression-14",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nInteraktive Visualisierung\n\nFür eine bessere Intuition hinter den Konzepten der logistischen Regression und der Konfusionsmatrix gibt es eine interaktive Visualisierung als Shiny App\nErreichbar über:\n\nLogistic Explorer\n\nAlternativ Download der R-Files von StudIP und Ausführung in RStudio nach Installation der notwendigen Packages im Script install_packages.R"
  },
  {
    "objectID": "06_VL.html#quellen",
    "href": "06_VL.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, und Robert Tibshirani. 2021. An Introduction to Statistical Learning: with Applications in R. Second edition. Springer texts in statistics. New York NY: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nWilke, C. 2019. Fundamentals of data visualization: a primer on making informative and compelling figures. First edition. Sebastopol, CA: O’Reilly Media. https://clauswilke.com/dataviz/."
  },
  {
    "objectID": "04_VL_D.html#der-plan-für-heute",
    "href": "04_VL_D.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung 4\n\n\nBig Data:\n\nWas ist Big Data?\nDefinitionen und Eigenschaften\nTechnologien und Werkzeuge\n\nData Lake:\n\nWas ist der Data Lake?\nBraucht man das?\nWas sind die Unterschiede zum Data Warehouse?\n\nWeitere Ursachenforschung für den Wachstumsschwund bei Tofispy\nVisualisierung von Anteilen: Kreisdiagramm und Treemap\nInformationsgenerierung:\n\nBerichtsorienterte Analyse"
  },
  {
    "objectID": "04_VL_D.html#einordnung-in-den-gesamtkontext",
    "href": "04_VL_D.html#einordnung-in-den-gesamtkontext",
    "title": "Business Intelligence & Data Science",
    "section": "Einordnung in den Gesamtkontext",
    "text": "Einordnung in den Gesamtkontext\nDer Data Lake & Big Data\n\n\n\nBIA Gesamtansatz. Eigene Darstellung in Anlehnung an Baars und Kemper (2021)."
  },
  {
    "objectID": "04_VL_D.html#big-data-1",
    "href": "04_VL_D.html#big-data-1",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\nWas ist Big Data?\n\n\n\nQuelle: Siemens Stiftung 2019"
  },
  {
    "objectID": "04_VL_D.html#big-data-2",
    "href": "04_VL_D.html#big-data-2",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\nDie 3Vs\n\n\n\n\nVolume:\n\nDie erhebliche Menge an Daten, die erfasst oder generiert wird und die vorgehalten sowie gespeichert werden muss\nD’Onofrio und Meier (2021) sprechen im Petabyte bis Zettabyte-Bereich von Big Data\n\nVariety:\n\nMeint die unterschiedlichen Arten von Daten in Big Data-Szenarien\nNeben strukturierte Daten in Tabellenform auch um unstrukturierte Daten wie Texte, Bilder, Videos, Audios sowie semi-strukturierte Daten\n\nVelocity:\n\nDie Rate, mit der Daten erzeugt, gesammelt und verarbeitet werden.\nIm Extremfall müssen Daten in Echtzeit analysiert werden, um sofortige Erkenntnisse zu gewinnen.\n\n\n\n\nExkurs Bits und Bytes:\n1 Bit = Kleinste Informationseinheit, kann 0 oder 1 sein 1 Byte = 8 Bit, entspricht einem Zeichen, bspw. einem Buchstaben und kann 256 verschiedene Kombinationen annehmen, nämlich 2^8. Warum 2^8? Weil 2 Zustände (0 oder 1) und 8 Stellen (Bit) = 2^8 = 256 1 Byte ist auch die kleinste Speichereinheit in Computern 1 Kilobyte (KB) = 1024 Byte 1 Megabyte (MB) = 1024 KB 1 Gigabyte (GB) = 1024 MB 1 Terabyte (TB) = 1024 GB 1 Petabyte (PB) = 1024 TB 1 Exabyte (EB) = 1024 PB 1 Zettabyte (ZB) = 1024 EB 1 Yottabyte (YB) = 1024 ZB\n\n\n\n\nQuelle: Siemens Stiftung 2019"
  },
  {
    "objectID": "04_VL_D.html#big-data-3",
    "href": "04_VL_D.html#big-data-3",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\n…und noch mehr Vs\n\nWeitere Vs in der Literatur zielen auf die Verwendung der Daten ab und weniger auf die technischen Eigenschaften der 3 Vs\n\n\nValue (Wert)\n\nDer Wert, den Unternehmen aus den gesammelten und analysierten Daten ziehen können\nDas Hauptziel von Big Data ist es, nutzbare Erkenntnisse zu gewinnen und daraus einen Mehrwert zu generieren"
  },
  {
    "objectID": "04_VL_D.html#big-data-4",
    "href": "04_VL_D.html#big-data-4",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\n…und noch mehr Vs\n\nVeracity (Genauigkeit)\n\nDie Qualität der Daten, besonders Präzision und Zuverlässigkeit, da große Daten oft Rauschen und ungenaue Informationen enthalten\nBaars und Kemper (2021) betonen, dass Genauigkeit auf die durch Big Data erzielten Ergebnisse abzielt und weniger auf die Daten\n\nValidity (Validität)\n\nDie Gültigkeit der Datenanalyse, inwiefern die durchgeführten Analysen tatsächlich die gewünschten Erkenntnisse liefern\n\nOutput Velocity (Analyse-Geschwindigkeit)\n\nDie Bereitstellung von relevanten Ergebnissen mit geringer Latenz, idealerweise nahezu in Echtzeit"
  },
  {
    "objectID": "04_VL_D.html#big-data-5",
    "href": "04_VL_D.html#big-data-5",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\nTechnologien und Werkzeuge\n\nBig Data Technologien müssen hohe Datenvolumina im mehrstelligen Petabyte-Bereich handhaben, um hierauf Analysen durchzuführen\nDie Ergebnisse dieser Analysen gilt es hiernach im IT-Entscheidungsunterstützungssystem zu integrieren\nDies ist mit den bisher vorgestellten relationalen CDWH-Lösungen nicht ohne Weiteres möglich\nDie aus den 3 Vs resultierenden Anforderungen erfordern hohe Rechen- und Speicherkapazitäten, die oft durch parallele Infrastruktur realisiert werden."
  },
  {
    "objectID": "04_VL_D.html#big-data-6",
    "href": "04_VL_D.html#big-data-6",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\nTechnologien und Werkzeuge\n\nAllgemein werden zwei Formen der Parallelisierung unterschieden:\n\nVertikale Skalierung:\n\nAuch Scale Up genannt\nNutzung von leistungsstärkeren Rechnern\n\nHorizontale Skalierung:\n\nAuch Scale Out genannt\nNutzung von vielen günstigen Standard-Servern oder Cloud Infrastruktur bei modernen Cloud Hyper Scalern, um Lasten zu verteilen\n\n\nVertikale Skalierung stößt schnell an technische Grenzen, horizontale Skalierung meist an Budgetgrenzen"
  },
  {
    "objectID": "04_VL_D.html#big-data-7",
    "href": "04_VL_D.html#big-data-7",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\nTechnologien und Werkzeuge\n\nVertikale Skalierung stößt schnell an technische Grenzen, horizontale Skalierung meist an Budgetgrenzen\nDie Datenhaltung in Big Data-Szenarien erfolgt in der Regel in NoSQL-Datenbanken (Not Only SQL)\nDies dient primär der Parallelisierung der Datenhaltung und -verarbeitung\nGleichzeitig ermöglichen diese Datenbanken die Speicherung von unstrukturierten Daten"
  },
  {
    "objectID": "04_VL_D.html#nosql-datenbanken",
    "href": "04_VL_D.html#nosql-datenbanken",
    "title": "Business Intelligence & Data Science",
    "section": "NoSQL Datenbanken",
    "text": "NoSQL Datenbanken\nKey-Value Stores\n\n\n\nDatenbanken, die paarweise einen (einmaligen) Schlüssel mit einem zugehörigen Wert ablegen\nDiese Werte können beliebige Datenstrukturen sein, wie z.B. Texte, Bilder, Videos, Audios, JSON-Objekte, XML-Dateien, etc.\nKey-Value Stores sind optimiert für schnelle Lese- und Schreibzugriffe auf Basis von Keys\n\n\n\n\n\nKey Value Store, Quelle: Data Engineering Wiki"
  },
  {
    "objectID": "04_VL_D.html#nosql-datenbanken-1",
    "href": "04_VL_D.html#nosql-datenbanken-1",
    "title": "Business Intelligence & Data Science",
    "section": "NoSQL Datenbanken",
    "text": "NoSQL Datenbanken\nDocument Stores\n\n\n\nEnthalten poly-strukturierte Dokumente beliebiger Länge, die auf Basis von Dokumenteninhalten recherchiert werden können.\nHäufig in Form von JSON oder XML Files abgelegt, da diese die Möglichkeit bieten, unstrukturierte Attribute zu hinterlegen und flexible Schemata zu definiere.\nDas Beispiel rechts entspricht dem JavaScript Object Notation JSON\n\n\n\n\n\nDocument Data Base, Quelle: Data Engineering Wiki"
  },
  {
    "objectID": "04_VL_D.html#nosql-datenbanken-2",
    "href": "04_VL_D.html#nosql-datenbanken-2",
    "title": "Business Intelligence & Data Science",
    "section": "NoSQL Datenbanken",
    "text": "NoSQL Datenbanken\nWide Column Stores und Graph-Databases\n\n\n\n\nWide Column Stores:\n\nDatenbanken, die Daten mit einer jeweils variablen (dynamischen) Anzahl von Spalten und Subspalten verwalten können\nEinzelne Zeilen können eine unterschiedlichen Anzahl von Spalten beinhalten, die darüber hinaus sehr groß sein kann\n\nGraph-Databases:\n\nSind auf Ablage, Verarbeitung und Suche von vernetzten Datenstrukturen ausgerichtet.\nBasieren auf graph-typischen Strukturen mit Knoten und Kanten (Nodes and Edges), mit jeweils eigenen Attributen (Properties)\n\n\n\n\n\n\n\n\nGraph Database, Quelle: Data Engineering Wiki"
  },
  {
    "objectID": "04_VL_D.html#data-lake-1",
    "href": "04_VL_D.html#data-lake-1",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake",
    "text": "Data Lake\nDas Konzept des Data Lake\n\nDas Data Warehouse galt lange Zeit als das zentrale Architekturkonzept für dispositive Reporting- und Analysezwecke\nBig Data Technologien haben den Data Lake in den Fokus gerückt, der meist als eine ergänzende Komponente zu DWH dient\nEin Data Lake erhebt den Anspruch, alle Quelldaten in roher Form als Rohdaten zu persistieren und zur Verfügung zu stellen\nBeim Data Lake steht der effiziente Umgang mit großen und polystrukturierten Datenmengen im Vordergrund, die es schnell zu verarbeiten gilt\nDies ermöglicht komplexe Analysen für neue Machine Learning Anwendungen, die verschiedene Datenquellen benötigen"
  },
  {
    "objectID": "04_VL_D.html#data-lake-vs.-data-warehouse",
    "href": "04_VL_D.html#data-lake-vs.-data-warehouse",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake vs. Data Warehouse",
    "text": "Data Lake vs. Data Warehouse\nIllustration\n\n\n\nData Lake versus Data Warehouse, Quelle: Twitter"
  },
  {
    "objectID": "04_VL_D.html#data-lake-vs.-data-warehouse-1",
    "href": "04_VL_D.html#data-lake-vs.-data-warehouse-1",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake vs. Data Warehouse",
    "text": "Data Lake vs. Data Warehouse\nCharakteristika\n\n\n\nTabelle 1: Wichtige Charakteristika von Data Lake und Data Warehouse im Vergleich. In Anlehnung an Dittmar und Schulz (2023), S. 159\n\n\n\n\n\n\nData Warehouse\nData Lake\n\n\n\n\nOptimiert für wiederholbare Prozesse\nOriginär eine Erweiterung der DWH Staging Area\n\n\nUnterstützt eine Vielzahl von unternehmensinternen Informationsbedarfen\nOptimiert Daten für Analytics-Lösungen\n\n\nFokus auf vergangenheitsbezogene Auswertungen\nFokus auf unbekannte explorative Datenanalyse und zukunftsorienterte Methoden\n\n\nSchema-on-Write mit harmonisiertem Datenmodell\nSchema-on-Read mit Echtzeit Rohdaten Bewirtschaftung"
  },
  {
    "objectID": "04_VL_D.html#data-lake-vs.-data-warehouse-2",
    "href": "04_VL_D.html#data-lake-vs.-data-warehouse-2",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake vs. Data Warehouse",
    "text": "Data Lake vs. Data Warehouse\nAnwendungsbereiche\n\n\n\nAnwendungsfälle Data Lake und Data Warehouse, Quelle: Twitter"
  },
  {
    "objectID": "04_VL_D.html#data-lake-2",
    "href": "04_VL_D.html#data-lake-2",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake",
    "text": "Data Lake\nArchitekturprinzipien\n\n\nDatenanbindung:\n\nEs werden ausschließlich primäre Datenquellen angebunden\nDie feinste Granularität der Datenquelle wird verwendet\n\nDatenhaltung\n\nEine Löschung der Rohdaten erfolgt nur aus regulatorischen Gründen (Datenschutz)\nEin Zonenkonzept ist zu empfehlen\nEine Anonymisierung persönlicher Daten erfolgt während der Beladung\n\nDatenplattform\n\nInfrastructure as Code wird angestrebt\nEine Portierbarkeit zur Vermeidung von Vendor-Lock-In wird angestrebt\nEin zentrales Identity Management System steuert Zugriffe granular\nEin Datenkatalog mit fachlicher und operativer Perspektive wird empfohlen"
  },
  {
    "objectID": "04_VL_D.html#data-lake-3",
    "href": "04_VL_D.html#data-lake-3",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake",
    "text": "Data Lake\nZonen\n\nÄhnlich wie im ETL Prozess umfasst auch der Data Lake verschiedene Zonen, in denen die Daten verschiedene Prozesse durchlaufen:\n\n\n\nTransient Zone: Eingangsbereich, in den alle Daten in Rohform extrahiert werden, Daten werden nicht dauerhaft vorgehalten und es wird ggf. anonymisiert\nRaw Data Zone: Alle Daten werden in ihrer möglichst rohen Form dauerhaft vorgehalten\nCurated Zone: Hier werden aufbereitete Daten hinterlegt, die bereits Filterprozesse durchlaufen haben und von Mängeln befreit wurden\nDiscovery Sandbox: Hier werden Daten für direkten Zugriff durch Analysten bereitgestellt\nConsumption Zone: Hier stehen vollständig transformierte, angereicherte und aggregierte Daten für die Endnutzung zur Verfügung"
  },
  {
    "objectID": "04_VL_D.html#business-case",
    "href": "04_VL_D.html#business-case",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nTofispy User Events\n\nNachdem wir die User Events in unserem Cloud Data Warehouse Big Query gespeichert haben, können wir nun weiter auf Ursachenforschung gehen\nDer Datensatz in Superset heißt nun “user_events_clean” und ist nach wie vor im Controlling Mart gespeichert.\nWie wäre es mit einem Line Chart?\nHierzu erneut in Superset:\n\nOben auf “+Chart” klicken\nDas Dataset “user_events_clean” auswählen\nAnschließend “Time Series Line Chart” auswählen"
  },
  {
    "objectID": "04_VL_D.html#business-case-1",
    "href": "04_VL_D.html#business-case-1",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nTofispy User Events\n\nDer Datensatz enthält die folgenden Spalten:\n\nDATE: Datum mit täglicher Granularität\nSUBSCRIPTIONS: Zahl der Neu-Registrierungen pro Tag\nCANCELLATIONS: Zahl der Kündigungen pro Tag\nNET: Netto-Zahl pro Tag als Differenz zwischen Neu-Registrierungen und Kündigungen\n\nFragen:\n\nWelche Granularität des Zeitstempels (Time Grain) generiert das anschaulichste Chart?\nWas ist eine sinnvolle Aggregation?\nWas ist der Hauptgrund für das schwindende Wachstum?"
  },
  {
    "objectID": "04_VL_D.html#business-case-2",
    "href": "04_VL_D.html#business-case-2",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nLine Plot Starthilfe"
  },
  {
    "objectID": "04_VL_D.html#business-case-3",
    "href": "04_VL_D.html#business-case-3",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nWarum kündigen User?\n\nIn Superset befindet sich ein weiterer Datensatz “cancellation_questionnaire”, der die Antworten einer Stichprobe von Usern erhält, die bei Kündigung einen Fragebogen bekommen\nDer Datensatz enthält zwei Spalten:\n\nQuestion: Die gestellte Frage mit vier Ausprägungen\nResponse: Die Antworten der User"
  },
  {
    "objectID": "04_VL_D.html#business-case-4",
    "href": "04_VL_D.html#business-case-4",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nWarum kündigen User?\n\nZiel: Eine passende Visualisierung, die aufzeigt, wieso User hauptsächlich kündigen\nSupport durch Google und preset.io Dokumentation\n\nStarthilfe:\n\nErneut gehen wir auf der Startseite oben rechts auf + Chart\nWir wählen dann das Dataset “cancellation_questionnaire” aus\nBei Chart Types gehen wir auf “Part of a Whole”: Welche Optionen gibt es?\n\nEmpfehlung:\n\nPie Chart oder Treemap v2\n\n\nWelche Metric ist sinnvoll?"
  },
  {
    "objectID": "04_VL_D.html#visualisierung-von-anteilen",
    "href": "04_VL_D.html#visualisierung-von-anteilen",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Anteilen",
    "text": "Visualisierung von Anteilen\nKreisdiagramme\n\n\n\n\nKreisdiagramme sind eine der häufigsten und umstrittensten Visualisierungen, um Anteile und Proportionen zu visualisieren\nNur bei einer geringen Anzahl von Kategorien (2-5) sinnvoll\nNicht sinnvoll, wenn die Anteile zwischen den Dimensionen sehr ähnlich sind\nAuch bei negativen Werten oder Measures, die sich nicht zu 100% summieren nicht geeignet"
  },
  {
    "objectID": "04_VL_D.html#visualisierung-von-anteilen-1",
    "href": "04_VL_D.html#visualisierung-von-anteilen-1",
    "title": "Business Intelligence & Data Science",
    "section": "Visualisierung von Anteilen",
    "text": "Visualisierung von Anteilen\nTreemap\n\n\n\n\nBei hierarchischen Datenstrukturen, die in Kategorien und Subkategorien unterteilt sind, eignet sich eine Treemap\nTreemaps nutzen die Fläche der Rechtecke, um die Größe der Kategorien zu visualisieren und geben damit einen Eindruck über die relativen Anteile\nDurch die Nutzung der Fläche statt des Winkels sind Treemaps leichter lesbar\nTreemaps sind auch in der Lage, mehrere Dimensionen gleichzeitig darzustellen\nBei zu vielen Kategorien oder zu ähnlichen Anteilen ist von Treemaps abzuraten"
  },
  {
    "objectID": "04_VL_D.html#berichtsorientierte-analyse",
    "href": "04_VL_D.html#berichtsorientierte-analyse",
    "title": "Business Intelligence & Data Science",
    "section": "Berichtsorientierte Analyse",
    "text": "Berichtsorientierte Analyse\nReporting\n\nEin Bericht oder Report gibt einen Überblick über betriebswirtschaftliche Sachverhalte eines abgegrenzten Verantwortungsbereichs\nIn der Regel durch Visualisierung von Zusammenhängen in grafischer Form\nBetriebliches Berichtswesen wird in interne und externe Berichterstattung unterteilt:\n\nInternes Berichtswesen: Informationen für das Management, bspw. internes Rechnungswesen\nExternes Berichtswesen: Informationen für externe Stakeholder, bspw. Jahresbericht"
  },
  {
    "objectID": "04_VL_D.html#berichtsorientierte-analyse-1",
    "href": "04_VL_D.html#berichtsorientierte-analyse-1",
    "title": "Business Intelligence & Data Science",
    "section": "Berichtsorientierte Analyse",
    "text": "Berichtsorientierte Analyse\nReporting\n\nAktive Berichtskomponenten:\n\nWerden nach einmaliger Spezifikation der Inhalte und Formate regelmäßig erstellt und aktiv versandt, entweder:\n\nPeriodisch: In festen Zeitabständen (jede Woche, jedes Quartal)\nAperiodisch: Bei Überschreitung bestimmter Grenzwerte (z.B. Umsatzgrenze)\n\n\nPassive Berichtskomponenten:\n\nWerden nur auf konkrete Anforderungen der Anwendenden erstellt\nIndividuelle und bedarfsspezifische Berichte\nAuch Ad-hoc Berichtskomponente genannt, oft mit OLAP und Self-Service Tools umgesetzt"
  },
  {
    "objectID": "04_VL_D.html#berichtsorientierte-analyse-2",
    "href": "04_VL_D.html#berichtsorientierte-analyse-2",
    "title": "Business Intelligence & Data Science",
    "section": "Berichtsorientierte Analyse",
    "text": "Berichtsorientierte Analyse\nOLAP\n\nOnline Analyitical Processing (OLAP) ermöglicht die Bereitstellung anwendungsfreundlicher und gleichermaßen flexibler Abfragen in multidimensionalen Datenräumen\nForm des Ad-hoc Reportings\nOLAP-Komponenten sind weitgehend mit Pivot Tabellen in Excel oder Google Sheets vergleichbar\nAber: Erweitert um ein zentrales Datenmodell (meist Data Mart basiert) und Rollenverwaltung"
  },
  {
    "objectID": "04_VL_D.html#olap-1",
    "href": "04_VL_D.html#olap-1",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nDatenmodell als Würfel\n\n\n\n\nBestehen konzeptionell aus Fakten, Dimensionen und Hierarchien\nDa oft mehrere Dimensionen vorliegen, spricht man von Cubes\nTheoretisch ist der Zahl an Dimensionen keine Grenzen gesetzt, in der Praxis aber im einstelligen Bereich begrenzt\nBei mehr als 3 Dimensionen spricht man oft von Hypercube\nBei der Erstellung von Reports aus OLAP Cubes spricht man oft von OLAP Operationen\n\n\n\n\n\n\nCube mit den Dimensionen Zeit, Produkt und Kunde. Quelle: Wikipedia"
  },
  {
    "objectID": "04_VL_D.html#olap-2",
    "href": "04_VL_D.html#olap-2",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nSlicing\n\nSlicing ist die Entnahme einer einzigen Dimension, indem eine ausgewählte Dimension auf einen Wert reduziert wird\nIm Beispiel wird der dreidimensionale Raum mit den Jahreswerten 2004–2006 auf das Jahr 2004 reduziert und so eine einzelne Scheibe aus dem Cube entnommen\n\n\n\n\n\nOLAP Slicing. Quelle: Wikipedia"
  },
  {
    "objectID": "04_VL_D.html#olap-3",
    "href": "04_VL_D.html#olap-3",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nDicing\n\nDicing ist die Einschränkung mehrerer Dimensionen auf ausgewählte Werte, sodass ein neuer, kleinerer Würfel entsteht\nIm Beispiel wird die Anzahl der Produktkategorien von fünf auf drei reduziert\n\n\n\n\n\nOLAP Dicing Quelle: Wikipedia"
  },
  {
    "objectID": "04_VL_D.html#olap-4",
    "href": "04_VL_D.html#olap-4",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nPivotierung\n\n\nAuswertung erfolgt meist auf zweidimensionalen Ausschnitten aus dem Cube, beispielsweise Produktkategorie pro Jahr\nGrafisch entspricht eine solche Ansicht einer Seite des Würfels\nDurch Pivotierung wird der Würfel um eine Achse gedreht\nIm Beispiel: Geografie pro Jahr statt Produktkategorie über Geografie\n\n\n\n\n\n\nOLAP Pivotierung/Rotation Quelle: Wikipedia"
  },
  {
    "objectID": "04_VL_D.html#olap-5",
    "href": "04_VL_D.html#olap-5",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nRoll-Up, Drill-Down & Drill-Through\n\nBeim Roll-Up werden die Werte einer Hierarchieebene auf die Werte einer übergeordneten Hierarchieebene aggregiert\nBeim Drill-Down wiederum wird ein aggregierter Wert in die einzelnen Bestandteile aufgeschlüsselt, bspw. die Betrachtung von einzelnen Monaten im Jahr 2004\nIn einigen Fällen wird beim Drilling die physikalische Datenquelle gewechselt, da beispielsweise nur eine begrenzte Granularitätsstufe im aktuellen Cube verfügbar ist\nDas geschieht im Normalfall ohne Wissen der Anwendenden, die die Abfrage stellen"
  },
  {
    "objectID": "04_VL_D.html#olap-6",
    "href": "04_VL_D.html#olap-6",
    "title": "Business Intelligence & Data Science",
    "section": "OLAP",
    "text": "OLAP\nPhysikalische Umsetzung und Anbindungsschnittstellen\n\nDie Datenhaltung erfolgt in OLAP Komponenten weitgehend unabhängig von der Anwendungsansicht meist in Client-Server-Architekturen und die Datenhaltung erfolgt serverseitig\nOLAP-Anwendungen erfordern zudem eine Programmoberfläche und oft erfolgt die Einbindung in Tabellenkalkulationsprogramm wie Excel\nEin bekanntes Beispiel ist SAP Analysis für Excel\nAndere gängige Praxis sind webbasierte Schnittstellen wie Cubeware Cockpit"
  },
  {
    "objectID": "04_VL_D.html#hausaufgabe",
    "href": "04_VL_D.html#hausaufgabe",
    "title": "Business Intelligence & Data Science",
    "section": "Hausaufgabe",
    "text": "Hausaufgabe\nWiederholung lineare Regression\n\nKleine Wiederholung der Basics in linearer Regression, entweder mit eigenen Unterlagen oder diesem Video hier (Link zum Video):"
  },
  {
    "objectID": "04_VL_D.html#quellen",
    "href": "04_VL_D.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nBaars, Henning, und Hans-Georg Kemper. 2021. Business Intelligence & Analytics: Grundlagen und praktische Anwendungen: Ansätze der IT-basierten Entscheidungsunterstützung. 4., überarbeitete und erweiterte Auflage. Lehrbuch. Wiesbaden [Heidelberg]: Springer Vieweg.\n\n\nD’Onofrio, Sara, und Andreas Meier, Hrsg. 2021. Big Data Analytics: Grundlagen, Fallbeispiele und Nutzungspotenziale. Edition HMD. Wiesbaden: Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-32236-6.\n\n\nDittmar, Carsten, und Peter Schulz. 2023. „Architekturen und Technologien für den Data Lake“. In Künstliche Intelligenz und Data Science in Theorie und Praxis, herausgegeben von Andreas Gillhuber, Göran Kauermann, und Wolfgang Hauner, 157–66. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-66278-6_12.\n\n\nQuix, Christoph. 2021. „Big-Data-Technologien“. In Data Science, herausgegeben von Detlev Frick, Andreas Gadatsch, Jens Kaufmann, Birgit Lankes, Christoph Quix, Andreas Schmidt, und Uwe Schmitz, 133–48. Wiesbaden: Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-33403-1_8."
  },
  {
    "objectID": "04_VL.html#der-plan-für-heute",
    "href": "04_VL.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung 4\n\nBig Data:\n\nWas ist Big Data?\nDefinitionen und Eigenschaften\nTechnologien und Werkzeuge\nAnwendungsfälle\n\nData Lake:\n\nWas ist der Data Lake?\nBraucht man das?\nWas sind die Unterschiede zum Data Warehouse?\n\nWeitere Ursachenforschung für den Wachstumsschwund bei Tofispy"
  },
  {
    "objectID": "04_VL.html#einordnung-in-den-gesamtkontext",
    "href": "04_VL.html#einordnung-in-den-gesamtkontext",
    "title": "Business Intelligence & Data Science",
    "section": "Einordnung in den Gesamtkontext",
    "text": "Einordnung in den Gesamtkontext\nDer Data Lake & Big Data\n\n\n\nBIA Gesamtansatz. Eigene Darstellung in Anlehnung an Baars und Kemper (2021)."
  },
  {
    "objectID": "04_VL.html#big-data-1",
    "href": "04_VL.html#big-data-1",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\nWas ist Big Data?\n\n\n\nQuelle: Siemens Stiftung 2019"
  },
  {
    "objectID": "04_VL.html#big-data-2",
    "href": "04_VL.html#big-data-2",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\nDie 3Vs\n\n\n\n\nVolume:\n\nDie erhebliche Menge an Daten, die erfasst oder generiert wird und die vorgehalten sowie gespeichert werden muss\nD’Onofrio und Meier (2021) sprechen im Petabyte bis Zettabyte-Bereich von Big Data\n\nVariety:\n\nMeint die unterschiedlichen Arten von Daten in Big Data-Szenarien\nNeben strukturierte Daten in Tabellenform auch um unstrukturierte Daten wie Texte, Bilder, Videos, Audios sowie semi-strukturierte Daten\n\nVelocity:\n\nDie Rate, mit der Daten erzeugt, gesammelt und verarbeitet werden.\nIm Extremfall müssen Daten in Echtzeit analysiert werden, um sofortige Erkenntnisse zu gewinnen.\n\n\n\n\nExkurs Bits und Bytes:\n1 Bit = Kleinste Informationseinheit, kann 0 oder 1 sein 1 Byte = 8 Bit, entspricht einem Zeichen, bspw. einem Buchstaben und kann 256 verschiedene Kombinationen annehmen, nämlich 2^8. Warum 2^8? Weil 2 Zustände (0 oder 1) und 8 Stellen (Bit) = 2^8 = 256 1 Byte ist auch die kleinste Speichereinheit in Computern 1 Kilobyte (KB) = 1024 Byte 1 Megabyte (MB) = 1024 KB 1 Gigabyte (GB) = 1024 MB 1 Terabyte (TB) = 1024 GB 1 Petabyte (PB) = 1024 TB 1 Exabyte (EB) = 1024 PB 1 Zettabyte (ZB) = 1024 EB 1 Yottabyte (YB) = 1024 ZB\n\n\n\n\nQuelle: Siemens Stiftung 2019"
  },
  {
    "objectID": "04_VL.html#big-data-3",
    "href": "04_VL.html#big-data-3",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\n…und noch mehr Vs\n\nWeitere Vs in der Literatur zielen auf die Verwendung der Daten ab und weniger auf die technischen Eigenschaften der 3 Vs\n\n\nValue (Wert)\n\nDer Wert, den Unternehmen aus den gesammelten und analysierten Daten ziehen können\nDas Hauptziel von Big Data ist es, nutzbare Erkenntnisse zu gewinnen und daraus einen Mehrwert zu generieren"
  },
  {
    "objectID": "04_VL.html#big-data-4",
    "href": "04_VL.html#big-data-4",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\n…und noch mehr Vs\n\nVeracity (Genauigkeit)\n\nDie Qualität der Daten, besonders Präzision und Zuverlässigkeit, da große Daten oft Rauschen und ungenaue Informationen enthalten\nBaars und Kemper (2021) betonen, dass Genauigkeit auf die durch Big Data erzielten Ergebnisse abzielt und weniger auf die Daten\n\nValidity (Validität)\n\nDie Gültigkeit der Datenanalyse, inwiefern die durchgeführten Analysen tatsächlich die gewünschten Erkenntnisse liefern\n\nOutput Velocity (Analyse-Geschwindigkeit)\n\nDie Bereitstellung von relevanten Ergebnissen mit geringer Latenz, idealerweise nahezu in Echtzeit"
  },
  {
    "objectID": "04_VL.html#big-data-5",
    "href": "04_VL.html#big-data-5",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\nTechnologien und Werkzeuge\n\nBig Data Technologien müssen hohe Datenvolumina im mehrstelligen Petabyte-Bereich handhaben, um hierauf Analysen durchzuführen\nDie Ergebnisse dieser Analysen gilt es hiernach im IT-Entscheidungsunterstützungssystem zu integrieren\nDies ist mit den bisher vorgestellten relationalen CDWH-Lösungen nicht ohne Weiteres möglich\nDie aus den 3 Vs resultierenden Anforderungen erfordern hohe Rechen- und Speicherkapazitäten, die oft durch parallele Infrastruktur realisiert werden."
  },
  {
    "objectID": "04_VL.html#big-data-6",
    "href": "04_VL.html#big-data-6",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\nTechnologien und Werkzeuge\n\nAllgemein werden zwei Formen der Parallelisierung unterschieden:\n\nVertikale Skalierung:\n\nAuch Scale Up genannt\nNutzung von leistungsstärkeren Rechnern\n\nHorizontale Skalierung:\n\nAuch Scale Out genannt\nNutzung von vielen günstigen Standard-Servern oder Cloud Infrastruktur bei modernen Cloud Hyper Scalern, um Lasten zu verteilen\n\n\nVertikale Skalierung stößt schnell an technische Grenzen, horizontale Skalierung meist an Budgetgrenzen"
  },
  {
    "objectID": "04_VL.html#big-data-7",
    "href": "04_VL.html#big-data-7",
    "title": "Business Intelligence & Data Science",
    "section": "Big Data",
    "text": "Big Data\nTechnologien und Werkzeuge\n\nVertikale Skalierung stößt schnell an technische Grenzen, horizontale Skalierung meist an Budgetgrenzen\nDie Datenhaltung in Big Data-Szenarien erfolgt in der Regel in NoSQL-Datenbanken (Not Only SQL)\nDies dient primär der Parallelisierung der Datenhaltung und -verarbeitung\nGleichzeitig ermöglichen diese Datenbanken die Speicherung von unstrukturierten Daten"
  },
  {
    "objectID": "04_VL.html#nosql-datenbanken",
    "href": "04_VL.html#nosql-datenbanken",
    "title": "Business Intelligence & Data Science",
    "section": "NoSQL Datenbanken",
    "text": "NoSQL Datenbanken\nKey-Value Stores\n\n\n\nDatenbanken, die paarweise einen (einmaligen) Schlüssel mit einem zugehörigen Wert ablegen\nDiese Werte können beliebige Datenstrukturen sein, wie z.B. Texte, Bilder, Videos, Audios, JSON-Objekte, XML-Dateien, etc.\nKey-Value Stores sind optimiert für schnelle Lese- und Schreibzugriffe auf Basis von Keys\n\n\n\n\n\nKey Value Store, Quelle: Data Engineering Wiki"
  },
  {
    "objectID": "04_VL.html#nosql-datenbanken-1",
    "href": "04_VL.html#nosql-datenbanken-1",
    "title": "Business Intelligence & Data Science",
    "section": "NoSQL Datenbanken",
    "text": "NoSQL Datenbanken\nDocument Stores\n\n\n\nEnthalten poly-strukturierte Dokumente beliebiger Länge, die auf Basis von Dokumenteninhalten recherchiert werden können.\nHäufig in Form von JSON oder XML Files abgelegt, da diese die Möglichkeit bieten, unstrukturierte Attribute zu hinterlegen und flexible Schemata zu definiere.\nDas Beispiel rechts entspricht dem JavaScript Object Notation JSON\n\n\n\n\n\nDocument Data Base, Quelle: Data Engineering Wiki"
  },
  {
    "objectID": "04_VL.html#nosql-datenbanken-2",
    "href": "04_VL.html#nosql-datenbanken-2",
    "title": "Business Intelligence & Data Science",
    "section": "NoSQL Datenbanken",
    "text": "NoSQL Datenbanken\nWide Column Stores und Graph-Databases\n\n\n\n\nWide Column Stores:\n\nDatenbanken, die Daten mit einer jeweils variablen (dynamischen) Anzahl von Spalten und Subspalten verwalten können\nEinzelne Zeilen können eine unterschiedlichen Anzahl von Spalten beinhalten, die darüber hinaus sehr groß sein kann\n\nGraph-Databases:\n\nSind auf Ablage, Verarbeitung und Suche von vernetzten Datenstrukturen ausgerichtet.\nBasieren auf graph-typischen Strukturen mit Knoten und Kanten (Nodes and Edges), mit jeweils eigenen Attributen (Properties)\n\n\n\n\n\n\n\n\nGraph Database, Quelle: Data Engineering Wiki"
  },
  {
    "objectID": "04_VL.html#data-lake-1",
    "href": "04_VL.html#data-lake-1",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake",
    "text": "Data Lake\nDas Konzept des Data Lake\n\nDas Data Warehouse galt lange Zeit als das zentrale Architekturkonzept für dispositive Reporting- und Analysezwecke\nBig Data Technologien haben den Data Lake in den Fokus gerückt, der meist als eine ergänzende Komponente zu DWH dient\nEin Data Lake erhebt den Anspruch, alle Quelldaten in roher Form als Rohdaten zu persistieren und zur Verfügung zu stellen\nBeim Data Lake steht der effiziente Umgang mit großen und polystrukturierten Datenmengen im Vordergrund, die es schnell zu verarbeiten gilt\nDies ermöglicht komplexe Analysen für neue Machine Learning Anwendungen, die verschiedene Datenquellen benötigen"
  },
  {
    "objectID": "04_VL.html#data-lake-vs.-data-warehouse",
    "href": "04_VL.html#data-lake-vs.-data-warehouse",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake vs. Data Warehouse",
    "text": "Data Lake vs. Data Warehouse\nIllustration\n\n\n\nData Lake versus Data Warehouse, Quelle: Twitter"
  },
  {
    "objectID": "04_VL.html#data-lake-vs.-data-warehouse-1",
    "href": "04_VL.html#data-lake-vs.-data-warehouse-1",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake vs. Data Warehouse",
    "text": "Data Lake vs. Data Warehouse\nCharakteristika\n\n\n\nTabelle 1: Wichtige Charakteristika von Data Lake und Data Warehouse im Vergleich. In Anlehnung an Dittmar und Schulz (2023), S. 159\n\n\n\n\n\n\nData Warehouse\nData Lake\n\n\n\n\nOptimiert für wiederholbare Prozesse\nOriginär eine Erweiterung der DWH Staging Area\n\n\nUnterstützt eine Vielzahl von unternehmensinternen Informationsbedarfen\nOptimiert Daten für Analytics-Lösungen\n\n\nFokus auf vergangenheitsbezogene Auswertungen\nFokus auf unbekannte explorative Datenanalyse und zukunftsorienterte Methoden\n\n\nSchema-on-Write mit harmonisiertem Datenmodell\nSchema-on-Read mit Echtzeit Rohdaten Bewirtschaftung"
  },
  {
    "objectID": "04_VL.html#data-lake-vs.-data-warehouse-2",
    "href": "04_VL.html#data-lake-vs.-data-warehouse-2",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake vs. Data Warehouse",
    "text": "Data Lake vs. Data Warehouse\nAnwendungsbereiche\n\n\n\nAnwendungsfälle Data Lake und Data Warehouse, Quelle: Twitter"
  },
  {
    "objectID": "04_VL.html#data-lake-2",
    "href": "04_VL.html#data-lake-2",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake",
    "text": "Data Lake\nArchitekturprinzipien\n\n\nDatenanbindung:\n\nEs werden ausschließlich primäre Datenquellen angebunden\nDie feinste Granularität der Datenquelle wird verwendet\n\nDatenhaltung\n\nEine Löschung der Rohdaten erfolgt nur aus regulatorischen Gründen (Datenschutz)\nEin Zonenkonzept ist zu empfehlen\nEine Anonymisierung persönlicher Daten erfolgt während der Beladung\n\nDatenplattform\n\nInfrastructure as Code wird angestrebt\nEine Portierbarkeit zur Vermeidung von Vendor-Lock-In wird angestrebt\nEin zentrales Identity Management System steuert Zugriffe granular\nEin Datenkatalog mit fachlicher und operativer Perspektive wird empfohlen"
  },
  {
    "objectID": "04_VL.html#data-lake-3",
    "href": "04_VL.html#data-lake-3",
    "title": "Business Intelligence & Data Science",
    "section": "Data Lake",
    "text": "Data Lake\nZonen\n\nÄhnlich wie im ETL Prozess umfasst auch der Data Lake verschiedene Zonen, in denen die Daten verschiedene Prozesse durchlaufen:\n\n\n\nTransient Zone: Eingangsbereich, in den alle Daten in Rohform extrahiert werden, Daten werden nicht dauerhaft vorgehalten und es wird ggf. anonymisiert\nRaw Data Zone: Alle Daten werden in ihrer möglichst rohen Form dauerhaft vorgehalten\nCurated Zone: Hier werden aufbereitete Daten hinterlegt, die bereits Filterprozesse durchlaufen haben und von Mängeln befreit wurden\nDiscovery Sandbox: Hier werden Daten für direkten Zugriff durch Analysten bereitgestellt\nConsumption Zone: Hier stehen vollständig transformierte, angereicherte und aggregierte Daten für die Endnutzung zur Verfügung"
  },
  {
    "objectID": "04_VL.html#business-case",
    "href": "04_VL.html#business-case",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nTofispy User Events\n\nNachdem wir die User Events in unserem Cloud Data Warehouse Big Query gespeichert haben, können wir nun weiter auf Ursachenforschung gehen\nDer Datensatz in Superset heißt nun “user_events_clean” und ist nach wie vor im Controlling Mart gespeichert.\nWie wäre es mit einem Line Chart?\nHierzu erneut in Superset:\n\nOben auf “+Chart” klicken\nDas Dataset “user_events_clean” auswählen\nAnschließend “Time Series Line Chart” auswählen\n\nFrage: Was ist der Hauptgrund für das schwindende Wachstum?"
  },
  {
    "objectID": "04_VL.html#business-case-1",
    "href": "04_VL.html#business-case-1",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nLine Plot Starthilfe"
  },
  {
    "objectID": "04_VL.html#business-case-2",
    "href": "04_VL.html#business-case-2",
    "title": "Business Intelligence & Data Science",
    "section": "Business Case",
    "text": "Business Case\nHausaufgabe\n\nIn Superset befindet sich ein weiterer Datensatz “cancellation_questionnaire”, der die Antworten einer Stichprobe von Usern erhält, die bei Kündigung einen Fragebogen bekommen\nDer Datensatz enthält zwei Spalten:\n\nQuestion: Die gestellte Frage mit vier Ausprägungen\nResponse: Die Antworten der User\n\nZiel: Eine passende Visualisierung, die aufzeigt, wieso User hauptsächlich kündigen\nSupport durch Google und preset.io Dokumentation"
  },
  {
    "objectID": "04_VL.html#quellen",
    "href": "04_VL.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nBaars, Henning, und Hans-Georg Kemper. 2021. Business Intelligence & Analytics: Grundlagen und praktische Anwendungen: Ansätze der IT-basierten Entscheidungsunterstützung. 4., überarbeitete und erweiterte Auflage. Lehrbuch. Wiesbaden [Heidelberg]: Springer Vieweg.\n\n\nD’Onofrio, Sara, und Andreas Meier, Hrsg. 2021. Big Data Analytics: Grundlagen, Fallbeispiele und Nutzungspotenziale. Edition HMD. Wiesbaden: Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-32236-6.\n\n\nDittmar, Carsten, und Peter Schulz. 2023. „Architekturen und Technologien für den Data Lake“. In Künstliche Intelligenz und Data Science in Theorie und Praxis, herausgegeben von Andreas Gillhuber, Göran Kauermann, und Wolfgang Hauner, 157–66. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-66278-6_12.\n\n\nQuix, Christoph. 2021. „Big-Data-Technologien“. In Data Science, herausgegeben von Detlev Frick, Andreas Gadatsch, Jens Kaufmann, Birgit Lankes, Christoph Quix, Andreas Schmidt, und Uwe Schmitz, 133–48. Wiesbaden: Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-33403-1_8."
  },
  {
    "objectID": "99_VL.html#der-plan-für-heute",
    "href": "99_VL.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung\n\n\nBusiness Intelligence & Data Science, SoSe 2024"
  },
  {
    "objectID": "07_VL.html#der-plan-für-heute",
    "href": "07_VL.html#der-plan-für-heute",
    "title": "Business Intelligence & Data Science",
    "section": "Der Plan für heute…",
    "text": "Der Plan für heute…\nVorlesung\n\nQuiz und kurzes Recap logistische Regression\nModellgestützte Analysen\n\nWeitere Modellevaluation\nMultiple Logistische Regression\nMultinominale Logistische Regression"
  },
  {
    "objectID": "07_VL.html#ablauf",
    "href": "07_VL.html#ablauf",
    "title": "Business Intelligence & Data Science",
    "section": "Ablauf",
    "text": "Ablauf\nAktualisierter Ablaufplan\n\n\n\n\n\nBlock\nGruppe D\nThema\n\n\n\n\n1\n21.03.2024:09:00 – 10:30\nOrganisation, Einleitung\n\n\n2\n21.03.2024:10:45 – 12:15\nDatenbereitstellung: Data Warehousing\n\n\n3\n28.03.2024:13:00 – 14:30\nDatentransformation\n\n\n4\n28.03.2024:14:45 – 16:15\nBig Data und Data Lake\n\n\n5\n02.04.2024:13:00 – 14:30\nInformationsgenerierung: Berichtsorientierte Analysen\n\n\n6\n02.04.2024:14:45 – 16:15\nAdvanced und Predictive Analytics: Grundlagen\n\n\n7\n15.04.2024:13:00 – 14:30\nAdvanced und Predictive Analytics: Klassifikation\n\n\n8\n15.04.2024:14:45 – 16:15\nAdvanced und Predictive Analytics: Klassifikation\n\n\n9\n02.05.2024:13:00 – 14:30\nRestinhalte Klassifikation; Assoziationsanalyse; Probeklausur (30 Minuten)\n\n\n10\n02.05.2024:14:45 – 16:15\nBesprechung Probeklausur; Evaluation; Fragen\n\n\n-\n13.05.2024:13:00 – 14:30\nKlausur (60 Minuten)"
  },
  {
    "objectID": "07_VL.html#modellevaluation",
    "href": "07_VL.html#modellevaluation",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision\n\n\nAndere Metriken beheben die Schwächen der Accuracy\nPrecision oder Präzision gibt an, wie viele der vom Klassifikator als positiv identifizierten Fälle tatsächlich positiv sind und entspricht dem Anteil der tatsächlich positiven Fälle an der Menge aller als positiv klassifizierten Fälle\n\n\n\\[\\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\\]\n\n\nWelcher Anteil der positiven Identifikationen war tatsächlich korrekt? Oder: Wenn das Modell einen Datenpunkt positiv klassifiziert, wie wahrscheinlich ist es, dass diese Klassifikation richtig ist?\nEine hohe Precision bedeutet also, dass der Klassifikator nur wenige irrelevante Fälle als relevant einstuft und ein Klassifikator mit einer Precision von 1,0 liefert keine FP."
  },
  {
    "objectID": "07_VL.html#modellevaluation-1",
    "href": "07_VL.html#modellevaluation-1",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nRecall\n\n\nRecall hingegen gibt an, wie viele der tatsächlich positiven Fälle vom Klassifikator als positiv bzw. relevant erkannt wurden:\n\n\n\\[\\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}}\\]\n\n\nEin hoher Recall-Wert bedeutet, dass der Klassifikator viele relevante Fälle erkennt und beantwortet die Frage, welcher Anteil der positiven Ergebnisse richtig identifiziert wurde.\nWie wahrscheinlich ist es, dass das Modell einen positiven Datenpunkt erkennt?\nAuch häufig als Sensitivität oder True Positive Rate bezeichnet\n\n\n\n\nKasten: Sample aller Beobachtungen\nLinker Kasten: Positive Beobachtungen\nRechte Kasten: Negative Beobachtungen\nKreis: Vorhersage des Modells, also ALLE als positiv vorhergesagte Datenpunkte\nKreis besteht aus zwei Hälften: TP und FN"
  },
  {
    "objectID": "07_VL.html#modellevaluation-2",
    "href": "07_VL.html#modellevaluation-2",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision und Recall\n\n\n\nIllustration Precision, Recall, Accuracy, Quelle: Maleki u. a. (2020)\n\n\n\n\nKasten: Sample aller Beobachtungen\nLinker Kasten: Positive Beobachtungen\nRechte Kasten: Negative Beobachtungen\nKreis: Vorhersage des Modells, also ALLE als positiv vorhergesagte Datenpunkte\nKreis besteht aus zwei Hälften: TP und FN"
  },
  {
    "objectID": "07_VL.html#modellevaluation-3",
    "href": "07_VL.html#modellevaluation-3",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision und Recall für den Spam-Filter\n\n\n\n\nUm Precision und Recall bei unausgewogenen Datensätzen zu illustrieren, betrachten wir erneut den Spam-Filter\nFür Precision erhalten wir\n\n\n\\[\\text{Precision} = \\frac{\\text{1}}{\\text{1 + 0}} = 1\\]\n\n\nFür Recall ergibt sich\n\n\n\\[\\text{Recall} = \\frac{\\text{1}}{\\text{1 + 99}} = 0,01.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTatsächlich Positiv\nTatsächlich Negativ\nSumme Vorhersage\n\n\n\n\nVorhergesagt Positiv\n1\n0\n1\n\n\nVorhergesagt Negativ\n99\n900\n999\n\n\nSumme Tatsächlich\n100\n900\n1000\n\n\n\n\n\n\nWenn das Modell eine Mail als Spam klassifiziert, ist diese Prognose zu 100% korrekt\nDas Modell erkennt aber nur 1% der tatsächlichen Spam-Mails."
  },
  {
    "objectID": "07_VL.html#modellevaluation-4",
    "href": "07_VL.html#modellevaluation-4",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nPrecision und Recall für den Spam-Filter\n\nWelches Maß ist nun das Richtige?\n\n\nWenn ein balancierter Datensatz vorliegt, kann Accuracy bedenkenlos verwendet werden, um das Modell zu evaluieren\nWenn wir sicher sein wollen, dass eine positive Vorhersage korrekt ist, dann ist Precision die angemessene Evaluationsmetrik. Dies ist oft der Fall, wenn FP mit höheren Kosten verbunden sind, als FN.\nWenn es wichtig ist, so viele positive Fälle wie möglich zu identifizieren, sollte ein Modell mit hohem Recall verwendet werden. In diesem Fall sind die Kosten von FN besonders hoch, sodass es besser ist, einige Fälle fälschlicherweise negativ zu klassifizieren, als dass uns tatsächlich positive Fälle durch die Lappen gehen"
  },
  {
    "objectID": "07_VL.html#modellevaluation-5",
    "href": "07_VL.html#modellevaluation-5",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nF1-Score\n\nOft sind Kosten und Nutzen von FP und FN Klassifikationen nicht eindeutig\nDer F1 Score kompensiert die Nachteile der Accuracy bei unbalancierten Datensätzen zu kompensieren und legt gleichzeitig einen ausgewogenen Fokus auf Precision und Recall:\n\n\n\\[\nF1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\nF1 ist das harmonische Mittel von Precision und Recall und liegt zwischen 0 und 1"
  },
  {
    "objectID": "07_VL.html#modellevaluation-6",
    "href": "07_VL.html#modellevaluation-6",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nF1-Score\n\nIm Spam Beispiel:\n\n\n\\[\nF1 = 2 \\cdot \\frac{\\text{1} \\cdot \\text{0,01}}{\\text{1} + \\text{0,01}}= 0,0198\n\\]\n\n\nDieser Wert ist deutlich niedriger, als uns die 90,1% Accuracy zunächst suggerieren.\nTrotzdem ist das Beispielmodell nahezu nutzlos, wie anhand des niedrigen F1-Scores erkennbar wird."
  },
  {
    "objectID": "07_VL.html#modellevaluation-7",
    "href": "07_VL.html#modellevaluation-7",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nAbschließende Bemerkungen\n\nBei einem balancierten Datensatz und gleicher Gewichtung von FP und FN ist die Accuracy die einfachste und intuitivste Metrik\nBei unbalancierten Datensätzen sollten Precision, Recall und F1-Score verwendet werden\nWenn FP höhere Kosten haben, dann sollte Precision im Fokus stehen\nWenn FN höhere haben, dann sollte Recall im Fokus stehen\nDer F1-Score ist besonders nützlich, wenn Precision und Recall gleiche Gewichtung haben"
  },
  {
    "objectID": "07_VL.html#modellevaluation-8",
    "href": "07_VL.html#modellevaluation-8",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKosten von FP und FN\n\n\nDie meisten Klassifikationsmethoden basieren auf der Annahme, dass FP und FN gleich problematisch sind\nIn praktischen Szenarios ist dies jedoch selten der Fall, Beispiel:\nBetrugserkennung: Eine Haftpflichtversicherung prüft Schadensmeldungen mit ML. Das Modell soll den Schaden automatisch abwickelnd und entweder als “Zahlung” oder “Keine Zahlung” klassifizieren, je nachdem ob ein Betrugsversuch vorliegt\nFragen:\n\nWas ist die positive (interessante) Klasse aus Sicht der Versicherung?\nWelche Fälle sind dann FN und FP?\nWelche Fehlklassifikation ist teurer? Welches Maß sollte für das Modell maximiert werden?\n\n\n\n\n\nAntworten:\n\nDie interessante Klasse ist die Zahlung des Schadens\nFN: Die Forderung ist berechtigt, aber die Versicherung zahlt nicht\nFP: Die Forderung ist unberechtigt, aber die Versicherung zahlt\nFP ist teurer, da die Versicherung zahlen muss, deshalb sollte Precision im Vordergrund stehen"
  },
  {
    "objectID": "07_VL.html#modellevaluation-9",
    "href": "07_VL.html#modellevaluation-9",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nKosten von FP und FN\n\nIdentifikation einer ansteckenden Krankheit: Ein medizinisches Modell identifiziert eine ansteckende Krankheit mit der Eingabe weniger Symptome. Anschließend wird die Person isoliert und umfänglich getestet\n\n\nFragen:\n\nWas ist die interessante Klasse aus Sicht einer Gesundheitsbehörde?\nWelche Fälle sind dann FN und FP?\nWelche Fehlklassifikation ist teurer? Welches Maß sollte für das Modell maximiert werden?\n\n\n\nAntworten:\n\nDie interessante Klasse ist die Identifikation der Krankheit\nFN: Die Person ist krank, wird aber nicht isoliert\nFP: Die Person ist nicht krank, wird aber isoliert\nFN ist teurer, da die Krankheit weiterverbreitet wird, deshalb sollte das Modell einen hohen Recall haben"
  },
  {
    "objectID": "07_VL.html#modellevaluation-10",
    "href": "07_VL.html#modellevaluation-10",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nTauziehen zwischen Precision und Recall\n\nPrecision und Recall sind in binären Klassifikationsmodellen oft invers proportional, eine Erhöhung von Precision führt zu einer Verringerung von Recall (und umgekehrt)\nDieses Tauziehen kann durch die Anpassung des Cut-Off Points \\(C\\) beeinflusst werden\nZur Illustration noch einmal zurück zur interaktiven Visualisierung:\n\nhttps://bi-and-ds-logistic-regression-qkwupfgvpq-ey.a.run.app/\n\nFragen:\n\nWas passiert mit Precision und Recall, wenn \\(C\\) ansteigt (sinkt)?\nFür welche Werte von \\(C\\) erhalten wir maximale Precision und Recall?"
  },
  {
    "objectID": "07_VL.html#modellevaluation-11",
    "href": "07_VL.html#modellevaluation-11",
    "title": "Business Intelligence & Data Science",
    "section": "Modellevaluation",
    "text": "Modellevaluation\nTauziehen zwischen Precision und Recall\n\n\n\nPrecision und Recall in Abhängigkeit von \\(C\\). Quelle: Shi u. a. (2009)"
  },
  {
    "objectID": "07_VL.html#logistische-regression",
    "href": "07_VL.html#logistische-regression",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrere unabhängige Variablen\n\n\n\nBisher haben wir nur eine unabhängige Variable, Energy, betrachtet\nDa die Aufteilung jedoch nicht perfekt war, ist es sinnvoll, weitere Features zu verwenden, bspw. Danceability\nZur Identifikation geeigneter Variablen lassen sich erneut Box-Plots oder Violin-Plots verwenden\nZur Erinnerung: Auch Danceability scheint geeignet, um zwischen EDM und Klassik zu unterscheiden"
  },
  {
    "objectID": "07_VL.html#logistische-regression-1",
    "href": "07_VL.html#logistische-regression-1",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrere unabhängige Variablen\n\nBei der Hinzunahme mehrerer Variablen ist neben der Korrelation mit der abhängigen Variable auch die Korrelation zwischen den unabhängigen Variablen zu beachten\nBei hoher positiver Korrelation zwischen den Variablen ist es möglich, dass die Hinzunahme dieser Variablen zum Modell keinen zusätzlichen Informationsgewinn liefert\nAuch besteht das Risiko der Multikollinearität, die zu instabilen Koeffizienten und einer schlechten Modellperformance führen kann\nDie Korrelation zwischen Danceability und Energy beträgt 0,8, ist also hoch, aber noch nicht bedenklich"
  },
  {
    "objectID": "07_VL.html#logistische-regression-2",
    "href": "07_VL.html#logistische-regression-2",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrere unabhängige Variablen\n\n\n\nBei zwei unabhängigen Variablen lässt sich der Zusammenhang zwischen den Variablen und der abhängigen Variable nach wie vor grafisch darstellen, diesmal als Scatter Plot"
  },
  {
    "objectID": "07_VL.html#logistische-regression-3",
    "href": "07_VL.html#logistische-regression-3",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrere unabhängige Variablen\n\nAllgemein hat das logistische Modell mit mehreren unabhängigen Variablen die Form:\n\n\n\\[\nP(y = 1|X) = \\frac{ e^{(\\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\ldots + \\beta_P \\cdot X_P)}}{1 + e^{(\\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\ldots + \\beta_P \\cdot X_P)}}\n\\]\n\n\nMit den beiden Variablen Energy und Danceability erhalten wir folgendes Modell:\n\n\n\\[\nP(EDM = 1|\\text{Energy, Danceability}) = \\\\ \\frac{ e^{(\\beta_0 + \\beta_1 \\cdot \\text{Energy} + \\beta_2 \\cdot \\text{Danceability})}}{1 + e^{(\\beta_0 + \\beta_1 \\cdot \\text{Energy} + \\beta_2 \\cdot \\text{Danceability})}}\n\\]"
  },
  {
    "objectID": "07_VL.html#logistische-regression-4",
    "href": "07_VL.html#logistische-regression-4",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrere unabhängige Variablen\n\n\nDie Koeffizienten für die unabhängigen Variablen sind:\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n-38.1001\n\n\nenergy\n27.2445\n\n\ndanceability\n41.1560\n\n\n\n\n\n\n\n\n\nAuch wenn die Interpretation der Koeffizienten bei Klassifikation weniger wichtig ist, zwei Anmerkungen zur Interpretation:\n\nDa der Zusammenhang zwischen den unabhängigen Variablen und der abhängigen Variable nicht linear ist, können wir die Koeffizienten nicht direkt interpretieren wie bei der linearen Regression\nDie Vorzeichen der Koeffizienten geben jedoch an, ob die unabhängige Variable positive oder negative Auswirkungen auf die abhängige Variable hat"
  },
  {
    "objectID": "07_VL.html#logistische-regression-5",
    "href": "07_VL.html#logistische-regression-5",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMultiple Logistische Regression\n\n\n\nFür das Modell mit zwei Variablen erhalten wir die Konfusionsmatrix auf der rechten Seite.\nAccuracy, Precision, Recall und F1-Score sind:\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\naccuracy\n0.9933\n\n\nrecall\n0.9930\n\n\nprecision\n0.9930\n\n\nf_meas\n0.9930"
  },
  {
    "objectID": "07_VL.html#logistische-regression-6",
    "href": "07_VL.html#logistische-regression-6",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMultiple Logistische Regression\n\n\n\n\nMit dem Scatter Plot lassen sich die Vorhersagen des Modells visualisieren und evaluieren\nDie gestrichelte Linie ist die Decision Boundary, also die Linie, auf der die Wahrscheinlichkeit für beide Klassen gleich 0,5 ist"
  },
  {
    "objectID": "07_VL.html#logistische-regression-7",
    "href": "07_VL.html#logistische-regression-7",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMultiple Logistische Regression\n\n\n\nUm die Performance des Modells systematisch zu optimieren, können wir erneut den Cut-Off Point \\(C\\) anpassen\nStatt Trial & Error ist es sinnvoll, Precision und Recall für verschiedene Werte von \\(C\\) zu berechnen und zu visualisieren"
  },
  {
    "objectID": "07_VL.html#logistische-regression-8",
    "href": "07_VL.html#logistische-regression-8",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nZusammenfassung binäre Klassifikation\n\nDie binäre Klassifikation erfordert zunächst die Umwandlung der abhängigen Variable in eine Dummy-Variable, kodiert mit 0 und 1\nZur Vereinfachung der Interpretation wird die interessante Klasse als 1 kodiert\nDas Modell berechnet für jeden Datenpunkt eine Wahrscheinlichkeit, dass die Beobachtung zur interessanten Klasse gehört\nAnhand eines Cut-Off Points \\(C\\) wird entschieden, ob die Beobachtung zur interessanten Klasse gehört\nDie Wahl der Evaluationsmetrik hängt von den Kosten von FP und FN und der Klassenungleichgewichtung ab\nKonfusionsmatrix, Accuracy, Precision, Recall und F1-Score sind gängige Metriken zur Modellbewertung"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression",
    "href": "07_VL.html#multinominale-logistische-regression",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nErweiterung auf mehrere Klassen\n\nAuch bei mehreren unabhängigen Variablen bleibt die logistische Regression ein binäres Modell\nZur Erweiterung auf \\(K&gt;2\\) Klassen wird die multinomiale logistische Regression verwendet, bei der zunächst eine Referenz- oder Baseline-Klasse festgelegt wird\nZur Schätzung der Koeffizienten werden anschließend alle \\(K-1\\) Klassen paarweise separat gegen die Referenzklasse regressiert\nAnschließend lassen sich die Klassenzugehörigkeiten ähnlich wie im binären Modell vorhersagen"
  },
  {
    "objectID": "07_VL.html#logistische-regression-9",
    "href": "07_VL.html#logistische-regression-9",
    "title": "Business Intelligence & Data Science",
    "section": "Logistische Regression",
    "text": "Logistische Regression\nMehrklassen-Klassifikation\n\n\nFür die Klassen \\(k = 1, ..., K-1\\) geschieht dies mit\n\n\n\\[\np(y_i = k|X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + ... + \\beta_{kp}x_p}}{1+\\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + ... + \\beta_{lp}x_p} }\n\\]\n\nund für die Referenzklasse \\(K\\) mit\n\n\\[\np(y_i = k|X = x) = \\frac{1}{1+\\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + ... + \\beta_{lp}x_p} }\n\\]\n\n\nDabei sind \\(\\beta_{k0}, ..., \\beta_{kp}\\) die Koeffizienten für die Klasse \\(k\\) und \\(x_1, ..., x_p\\) die unabhängigen Variablen"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression-1",
    "href": "07_VL.html#multinominale-logistische-regression-1",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nNeue Daten von Tofispy\n\nTofispy hat uns neue Daten zur Verfügung gestellt:\n\nNeben Klassik und EDM ist jetzt auch Hip-Hop enthalten\nNach wie vor haben wir die beiden Variablen Energy und Danceability zur Verfügung\n\nErneut teilen wir die Daten in Trainings- und Testdaten auf\nDiesmal verwenden wir 70% der Daten für das Training und 30% für das Testen\nDie Bewertung des Modells erfolgt dann anhand der Testdaten"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression-2",
    "href": "07_VL.html#multinominale-logistische-regression-2",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nErgebnisse\n\nFür das multinominale Modell erhalten wir jeweils Koeffizienten für \\(K-1\\) Klassen, bis auf die Referenzklasse \\(K\\)\nFür das Beispiel mit den drei Genres Klassik, EDM und Hip-Hop erhalten wir folgende Koeffizienten\n\n\n\n\n\n\n\n\nClass\nVariable\nCoefficient\n\n\n\n\nHip-Hop\n(Intercept)\n5.1263\n\n\nHip-Hop\nenergy\n-9.7410\n\n\nHip-Hop\ndanceability\n3.2574\n\n\nKlassik\n(Intercept)\n23.7448\n\n\nKlassik\nenergy\n-30.5953\n\n\nKlassik\ndanceability\n-15.4205"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression-3",
    "href": "07_VL.html#multinominale-logistische-regression-3",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nWahrscheinlichkeit und Klassenzugehörigkeit\n\nAuch das Modell der multinomialen logistischen Regression gibt für jede Klasse eine Wahrscheinlichkeit aus\nAnders als im binären Modell wird jedoch nicht nur die Wahrscheinlichkeit für eine Klasse ausgegeben, sondern für alle Klassen\nDie Klassenzugehörigkeit wird dann anhand der höchsten Wahrscheinlichkeit bestimmt, ein Cut-Off Point ist nicht notwendig"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression-4",
    "href": "07_VL.html#multinominale-logistische-regression-4",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nPrognose aus dem multinominalen Modell\n\nDie Prognose für die jeweilligen Klassen erfolgt erneut durch einsetzen der Koeffizienten in die logistische Funktion\nFür Hip-Hop erhalten wir beispielsweise\n\n\n\n\\[\n\\operatorname{\\widehat{P}(Hip-Hop)|\\text{Energy, Dance}} = \\\\\n\\frac{e^{5.13  -9.74 \\cdot \\text{Energy} + 3.26 \\cdot \\text{Dance}}}{1 + e^{5.13  -9.74 \\cdot \\text{Energy} + 3.26 \\cdot \\text{Dance}} +\ne^{23.74  -30.6 \\cdot \\text{Energy}  -15.42 \\cdot \\text{Dance}}}  \n\\]\n\n\n\nUnd für die Referenzklasse EDM:\n\n\n\n\\[\n\\operatorname{\\widehat{P}(Hip-Hop)|\\text{Energy, Dance}} = \\\\\n\\frac{1}{1 + e^{5.13  -9.74 \\cdot \\text{Energy} + 3.26 \\cdot \\text{Dance}} +\ne^{23.74  -30.6 \\cdot \\text{Energy}  -15.42 \\cdot \\text{Dance}}}  \n\\]"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression-5",
    "href": "07_VL.html#multinominale-logistische-regression-5",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nPrognose aus dem multinominalen Modell\n\nDa die händische Berechnung viel zu aufwendig ist, hier ein Beispiel für 10 ausgewählte Songs:\n\n\n\n\n\n\n\n\npred_class\ncategory\np_EDM\np_Hip-Hop\np_Klassik\ntrack.name\ntrack.artist\nenergy\ndanceability\n\n\n\n\nEDM\nEDM\n0.8015\n0.1985\n0.0000\nSamaria\nEuggy\n0.9080\n0.713\n\n\nEDM\nEDM\n0.8665\n0.1335\n0.0000\nMake It Work\nBklava\n0.9540\n0.705\n\n\nHip-Hop\nHip-Hop\n0.1855\n0.8145\n0.0000\nOKAY!\nOG Keemo\n0.6850\n0.929\n\n\nEDM\nEDM\n0.6489\n0.3511\n0.0000\nBlack Swan\nAxel N.\n0.8070\n0.651\n\n\nKlassik\nKlassik\n0.0000\n0.0000\n1.0000\nPréludes - Book 2, L.123: 11. Les tierces alternées\nClaude Debussy\n0.0633\n0.279\n\n\nHip-Hop\nHip-Hop\n0.1179\n0.8796\n0.0025\nTipsy\nDizzy\n0.5520\n0.694\n\n\nHip-Hop\nHip-Hop\n0.4729\n0.5271\n0.0000\nOvernight Celebrity\nTwista\n0.7920\n0.828\n\n\nHip-Hop\nHip-Hop\n0.2642\n0.7358\n0.0000\nThe Humpty Dance\nDigital Underground\n0.6930\n0.813\n\n\nKlassik\nKlassik\n0.0000\n0.0000\n1.0000\nPapa, Can You Hear Me?\nElliott Jack Sansom\n0.0128\n0.452\n\n\nKlassik\nKlassik\n0.0000\n0.0005\n0.9994\nPeaceful Here Now\nColin Stetson\n0.3170\n0.241"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression-6",
    "href": "07_VL.html#multinominale-logistische-regression-6",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nEvaluationsmetriken\n\nDie Evaluationsmetriken für die multinominale logistische Regression sind analog zum binären Modell, Accuracy, Precision, Recall und F1-Score können auch hier verwendet werden\nAuch die Konfusionsmatrix ist ein geeignetes Mittel zur Modellbewertung, umfasst nun aber alle Klassen und nicht nur vier Felder\nAccuracy entspricht dann nach wie vor der Summe aller richtig klassifizierten Fälle geteilt durch die Gesamtanzahl der Fälle\nAndere Metriken können entweder für jede Klasse einzeln berechnet oder gemittelt werden, um ein Gesamtbild der Modellperformance zu erhalten"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression-7",
    "href": "07_VL.html#multinominale-logistische-regression-7",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nEvaluationsmetriken\n\nBei der Aggregation wird häufig Macro-Averaging verwendet, bei dem der gleichgewichtete Durchschnitt über alle Klassen berechnet wird\nZum Beispiel für Precision ergibt sich bei Macro-Averaging:\n\n\n\n\\[\nPrecision_{\\text{macro}} = \\frac{1}{K} \\sum_{k=1}^{K} Precision_k\n\\]\n\n\n\nWobei \\(K\\) die Anzahl der Klassen ist\nAnalog für Recall möglich"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression-8",
    "href": "07_VL.html#multinominale-logistische-regression-8",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nEvaluationsmetriken\n\n\n\nDie Konfusionsmatrix für das Modell mit drei Klassen ist auf der rechten Seite dargestellt\nIm Durchschnitt über alle Klassen erhalten wir die folgenden Metriken:\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\naccuracy\n0.8478\n\n\nrecall\n0.8454\n\n\nprecision\n0.8479\n\n\nf_meas\n0.8464"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression-9",
    "href": "07_VL.html#multinominale-logistische-regression-9",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nEvaluationsmetriken\n\nDie Performance des Modells weicht zwischen den Klassen oft ab, häufig auch in einer Klassenungleichgewichtung begründet\nAus diesem Grund ist es manchmal sinnvoll, die Metriken für jede Klasse einzeln zu betrachten\nFür Precision:\n\n\n\n\\[\nPrecision_{\\text{k}} = \\frac{TP_{\\text{k}}}{TP_{\\text{k}} + FP_{\\text{k}}} = \\frac{TP_{\\text{k}}}{\\text{# vorhergesagte Klasse k}}\n\\]\n\n\n\nund für Recall:\n\n\n\n\\[\nRecall_{\\text{k}} = \\frac{TP_{\\text{k}}}{TP_{\\text{k}} + FN_{\\text{k}}} = \\frac{TP_{\\text{k}}}{\\text{# tatsächliche Klasse k}}\n\\]"
  },
  {
    "objectID": "07_VL.html#multinominale-logistische-regression-10",
    "href": "07_VL.html#multinominale-logistische-regression-10",
    "title": "Business Intelligence & Data Science",
    "section": "Multinominale Logistische Regression",
    "text": "Multinominale Logistische Regression\nVergleich Aggregation vs. Einzelmetriken\n\n\n\nMacro-Averaging\n\nEinfach interpretierbar, Reduktion auf eine Zahl\nSinnvoll wenn alle Klassen gleich wichtig sind oder Datensatz ausgewogen\nKann schlechte Performance in kleinen Klassen verbergen\n\n\n\n\nBerechnung pro Klasse\n\nWenn nur bestimmte Klassen interessant sind kann man gezielt evaluieren\nAuch bei unbalancierten Datensätzen sinnvoll, um die Performance kleiner Klassen hervorzuheben\nKann zu unübersichtlichen Ergebnissen führen, wenn viele Klassen vorhanden sind\n\n\n\n\n\nDa wir vornehmlich balancierte Klassen betrachten, ist Macro-Averaging in unsere Zwecke ausreichend"
  },
  {
    "objectID": "07_VL.html#quellen",
    "href": "07_VL.html#quellen",
    "title": "Business Intelligence & Data Science",
    "section": "Quellen",
    "text": "Quellen\n\n\nBusiness Intelligence & Data Science, SoSe 2024\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, und Robert Tibshirani. 2021. An Introduction to Statistical Learning: with Applications in R. Second edition. Springer texts in statistics. New York NY: Springer. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nMaleki, Farhad, Katie Ovens, Keyhan Najafian, Behzad Forghani, Caroline Md, und Reza Forghani. 2020. „Overview of Machine Learning Part 1“. Neuroimaging Clinics of North America 30 (November): e17–32. https://doi.org/10.1016/j.nic.2020.08.007.\n\n\nShi, Feng, Juanzi Li, Jie Tang, Guotong Xie, und Hanyu Li. 2009. „Actively Learning Ontology Matching via User Interaction“. In, 5823:585–600. https://doi.org/10.1007/978-3-642-04930-9_37."
  }
]