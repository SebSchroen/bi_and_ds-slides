---
title: "Business Intelligence & Data Science"
subtitle: "Vorlesung 1"
---

```{r}

library(kableExtra)
library(readr)
library(magrittr)
library(dplyr)
```




## Der Plan für heute...
### Vorlesung 1
:::{.incremental}
  * Vorstellung & kurzes Kennenlern-Quiz
  * Organisatorisches
  * Einleitung:
    * Was sind Business Intelligence & Data Science?
    * Was sind operative und dispositive Daten?
    * Wie werden aus operativen Daten entscheidungsrelevante Informationen?
:::

::: {.notes}
* Wir starten mit einer kurzen Vorstellung, in der ich mich kurz vorstelle und anschließend gibt es ein kurzes Quiz, das einerseits Quizzes als Tool einführt, mit dem wir künftig die Doppelblöcke eröffnet und mir andererseits die Gelegenheit gibt, den Wissenstand abzuklopfn
* Dann folgt ein Stresstest für unser BI-Tool Apache Superset, das wir künftig für die Visualisierung von Daten nutzen werden, indem wir uns alle gleichzeitig einloggen und testen, ob das System der Last standhält
* Hiernach folgen ein paar organisatorische Hinweise, bevor wir in die inhaltliche Einführung starten:
  * Was ist das eigentlich, Business Intelligence und Data Science?
  * Wie werden Informationen aus operativen Daten gewonnen? 
  
:::

## Vorstellung
### Kennenlern-Quiz

::: {.incremental}
* Das Kennenlern-Quiz umfasst Fragen zu Vorkenntnissen rund um BI-Software und R, um ein Gefühl für den Wissensstand zu bekommen.
* Außerdem gibt es wichtige Fachfragen rund um BI und Data Science.
* Die Gewinnerin/Der Gewinner erhält einen großartigen Preis
* Als Plattform dient Quizziz.com, es ist keine Registrierung notwendig, Verwendung über Smartphone und Laptop möglich
* Künftig starten wir einen Doppelblock mit einem Quiz zum vorangeganenen Block mit einem kleinen Preis für den Tagessieg
:::

<!-- ## Apache Superset -->
<!-- ### Stresstest -->
<!-- ::: {style="font-size: 0.92em"} -->
<!-- :::{.incremental} -->

<!-- * Apache Superset ist ein Open Source BI-Tool, das über die Vorlesung hinweg neben R als zentrales Tool dient. -->
<!-- * Das Tool ist in einem sogenannten Kubernetes Cluster in der Google Cloud gehosted und kann über folgenden Link erreicht werden: -->

<!--     * [http://34.95.70.95/login/](http://34.95.70.95/login/) -->

<!-- * Die Verbindung ist (aktuell noch) nicht https verschlüsselt, daher wird ein Warnhinweis erscheinen, der sich je nach Browser unterschiedlich umgehen lässt. -->
<!-- * Die Anmeldung erfolgt mit den Zugangsdaten, die auf StudIP veröffentlicht sind. -->
<!-- * Die Superset Instanz bleibt bis zur letzten Vorlesung online, bis dahin erstellte Dashboards und Grafiken werden als Export auf StudIP bereitgestellt. -->
<!-- ::: -->
<!-- ::: -->
# Motivation

## Warum BI und Data Science?

### Entscheidende Skills für die berufliche Zukunft

![Quelle: [McKinsey](https://kure.app/insights/mckinsey-company-defines-key-skills-needed-for-future-workforce)](img/job_skills.png){width="525" height="525"}

::: {.notes}

* Die Grafik zeigt die sogenannten Delta Skills nach McKinsey, das sind jene Fähigkeiten, die einen Unterschied machen werden, um in der Arbeitswelt der Zukunft erfolgreich zu sein.
* Die Kategorien sind: Kognitiv, Interpersonell, Selbstführung und Digital
* Unser FOkus ist die Kategorie Digital, die sich wiederum in die Bereiche Digital Fluency, Software Nutzung und Entwicklung sowie Verständnis digitaler Systeme aufteilt.
* Was ist digital Fluency? Es ist die Fähigkeit, digitale Tools und Technologien zu nutzen und hier Digital Learning und Collaboration besonders wichtig.
* Software Nutzung und Entwicklung umfasst insbesondere Programming und Datenanalyse, insbesondere den zweiten Punkt werden wir sehr ausführlich im Methodenteil zu Data Science behandeln.
* Ziel ist hier nicht, in wenigen Blöcken eine vollwertige Ausbildung in diesem Bereich zu ersetzen, sondern vielmehr das Big Picture zu vermitteln, um im Unternehmen als guter Sparringspartner für Data Science Themen und Projekte zu fungieren.
* Programming wird eher nebenbei mit einfließen
* Ein entscheidender Punkt ist unten rechts die Data Literacy. Das umfasst die Fähigkeit, Daten zu lesen, zu interpretieren und zu kommunizieren. Das ist ein zentraler Punkt, den wir in dieser Vorlesung adressieren werden. Meines Erachtens der größte Knackpunkt im Unternehmensumfeld, da hier die größte Lücke zwischen den Fachbereichen und der IT besteht.

:::

## Warum BI und Data Science?

### Technologische Kompetenzen


![[Link zur Quelle](https://www.stifterverband.org/medien/future-skills-2021)](img/IT_Skills.png){width="900" height="500"}

::: {.notes}

* Die Grafik zeigt die Ergebnisse einer Studie des Stifterverbands, die sich mit den zukünftigen Anforderungen an die Arbeitswelt befasst.
* Es handelt sich hier um eine Umfrage und die Grafik gibt den Anteil der teilnehmenden an, welche die gegeben Aspekte jetzt und in 5 Jahren als wichtig erachten.
* Für uns wichtig sind IT-Architektur und Data Analytics & KI
* Die Vorlesung wird einige Architektur-Komponenten umfassen, nämlich den Aufbau einer BI-Architektur und eine Einbettung ins Unternehmen allgemein
* Data Analytics und KI umfasst die Methoden, die wir in der zweiten Hälfte der Vorlesung behandeln werden, wobei der Fokus weniger auf KI liegt und mehr auf den Data Science Methoden, die vielen KI-Technologien zu Grunde liegen


:::

# Organisation 
## Kursaufbau
### Zeitplan Gruppe C

```{r, message= FALSE, echo = FALSE, warning = FALSE}
read_csv("/home/seb/LeibnizFH/bi_and_ds-lecture_notes/data/ablauf.csv") %>% 
  select(-Slides) %>% 
  select(-`Gruppe D`) %>% 
  kable( escape = FALSE)  %>% 
  kable_styling(font_size = 24)
```

## Kursaufbau
### Zeitplan Gruppe D

```{r, message= FALSE, echo = FALSE, warning = FALSE}
read_csv("/home/seb/LeibnizFH/bi_and_ds-lecture_notes/data/ablauf.csv") %>% 
  select(-Slides) %>% 
  select(-`Gruppe C`) %>% 
  kable( escape = FALSE)  %>% 
  kable_styling(font_size = 24)
```

## Kursmaterialen
### Kurs-Website

* Alle Kursinhalte sind auf der Kurs-Website verfügbar:

  * [https://sebschroen.github.io/bi_and_ds-lecture_notes/](https://sebschroen.github.io/bi_and_ds-lecture_notes/)

::: {.incremental}
* Aktuell passwortgeschützt aufgrund Nutzung Copyright-geschützter Materialien, daher nur Nutzung in diesem Personenkreis
* 3 gefundene und gemeldete Typos (E-Mail, StudIP)  = 1 Packung Bahlsen-Kekse nach Wahl
* Tipp: Bookmark des Links nach Passworteingabe erübrigt künftige Eingabe des Passworts
:::

::: {.notes}
* Passwort anschreiben
* Einfach zusammen durchklicken und kurz Zeit zur Orientierung geben, Trick zum Bookmark zeigen

:::

## Kursmaterialen
### Folien
::: {.incremental}
* Folien sind auf der Startseite der Kurs-Website verlinkt
* Darstellung ist für den Browser (Chrome, Safari und Firefox) optimiert, um interaktive Elemente darzustellen
* Im Burger-Menü oben rechts lässt sich zu jeder Zeit auch eine PDF Version zum Ausdruck in Papierform oder für Notizen erstellen:
  * Tools -> PDF Export Mode -> Strg + P (Cmd + P) -> Druck als PDF
* Die Folien werden rechtzeitig vor der Vorlesung als PDF auf StudIP hochgeladen
* Interaktive Elemente werden separat verlinkt.
:::

::: {.notes}
* Die Folien sind im Ablaufplan auf der Startseite in aktueller Fassung verlinkt und lassen sich auf mehreren Wegen darstellen.
1. Konzipiert sind die Folien für die Darstellung im Browser, um interaktive Elemente darzustellen.
2. Im Menü oben rechts lässt sich zu jeder Zeit auch eine PDF Version zum Ausdrucken in Papierform erstellen oder zur Darstellung auf Endgeräten, auf denen Sie Notizen machen möchten.
3. Am Ende jeder Vorlesung wird die finale Fassung der Folien auf StudIP im PDF Format hochgeladen.
4. Die HTML Fassung bleibt verfügbar.
:::

## Kursmaterialen
### PDF-Skript

* Neben der HTML-Version ist auch ein PDF-Skript verfügbar und kann auf der Startseite heruntergeladen werden
* Die PDF-Version entspricht inhaltlich immer der Website
* Die Darstellung ist für HTML optimiert und kann für Artefakte beim PDF Rendering sorgen, erleichtert aber das Ausdrucken

::: {.notes}
* Die PDF-Version zum Skript ist auf der Startseite der Website verlinkt und entspricht inhaltlich immer der Website.
* Die Darstellung ist für html optimiert und kann für Artefakte beim PDF Rendering sorgen, erleichtert aber das Ausdrucken
:::

## Kursmaterialien
### Ergänzende Literatur

::: {style="font-size: 0.92em"}
::: {.incremental}
* Alle klausurrelevanten Inhalte lassen sich auf der Kurs-Website finden und nachlesen, zusätzliche Literatur ist nicht notwendig
* Der Aufbau des Kurses richtet sich nach dem Lehrbuch [Business Intelligence & Analytics - Grundlagen und praktische Anwendungen, 4. Auflage](https://link.springer.com/book/10.1007/978-3-8348-2344-1) von Henning Baars und Hans-Georg Kemper
* Das Buch ist über die Bibliothek der Leibniz FH als E-Book verfügbar
* Block 1 - 5 entstammen größtenteils in @baars_business_2021 
* Methodische Aspekte rund um Predictive Analytics entstammen größtenteils dem **frei verfügbaren** [Introduction to Statistical Learning, 2. Auflage](https://www.statlearning.com/) von Gareth James, Daniela Witten, Trevor Hastie und Robert Tibshirani
* Die Quellen zu jeder Vorlesung sind jeweils auf der letzten Folien angegeben.
:::
:::
# Einleitung

## Business Intelligence
### Begriffsabgrenzung


::: {.callout-tip title="Definition"}
Business Intelligence (BI) ist eine Reihe von  Architekturen und Technologien, die Rohdaten in sinnvolle und nutzbare, entscheidungsrelevante Informationen umwandeln. 
Es ermöglicht Anwendenden, informierte Entscheidungen auf der Grundlage von Daten zu treffen, die ein Unternehmen gegenüber seinen Wettbewerbern in Vorteil bringen können (siehe [Forrester.com](https://www.forrester.com/report/Topic-Overview-Business-Intelligence/RES39218)).
:::

::: {.incremental}
* Abgeleitet vom *Intelligence*-Begriff in der militärischen Informationsverarbeitung Großbritanniens im 2. Weltkrieg: 
  * Die richtigen Informationen zur richtigen Zeit an die richtigen Personen.
* Frühe kommerzielle Ansätze in den 60er Jahren im Zuge der Entwicklung relationaler Datenbanken.
:::
## Business Intelligence
### Begriffsabgrenzung

::: {.incremental}
* Zunächst Fokus auf Management Support Systeme (MSS) und daher eher auf oberste Ebenen zugeschnitten
* Der Begriff Business Intelligence (BI) wurde in den 1990ern geprägt
* Heute wird BI laut Gartner Group charakterisiert durch:
    * Breite Verfügbarkeit von BI Tools auf allen Ebenen des Unternehmens
    * Geschäftsentscheidung auf Basis aktueller Informationen und Daten und nicht auf Intuition
    * Umfangreiche Analyse- und Reportingmöglichkeiten mit Self-Service Tools für Fachbereiche
:::

## Business Intelligence
### Betriebliche Dimensionen


![Begriffsdimensionen von BI nach @gluchowski_historische_2016.](https://sebschroen.github.io/bi_and_ds-lecture_notes/d90cdc8d7878e4a3c323e2982bd6be988cb4dd33/img/BI_Dimensionen.png){width="65%"}

::: {.notes}
1. Eine Personengruppe innerhalb der Organisation ist mit der Realisierung von BI-Prozessen vertraut.
2. Die Generierung geschäftsrelevanter Informationen, Erkenntnisse und Wissen erfordert die Überführung fragmentierter Unternehmens- und Wettbewerbsdaten in handlungsgerichtetes Wissen. Hierbei liegt der Fokus auf dem Geschäftsprozess von der Datenerfassung hin zur Wissenskommunnikation.
3. Bezeichnet das Ergebnis eines Erkenntnissprozesses, beispielsweise ziel- und zweckorientiertes Wissen in Form von Berichten, Analysen und Prognosen für das Management.
3. Eine Sammlung von informationstechnischen Werkzeugen, Architekturen, Systemen und Technologien zur Aufbereitung und Bereitstellung reschäftsrelevanter Daten zum Zweck der Informationsgewinnung.

Im Mittelpunkt dieser Vorlesung steht der BI-Prozess, beginnend mit der Datenerfassung und -bereitstellung im unternehmenseigenen Data Warehouse, über die  Informationsentdeckung bzw. -generierung mit modellgestützten Analysemethoden, bis hin zur Kommunikation. 
Jeder Bestandteil des BI-Prozesses wird dabei um wichtige technische Aspekte ergänzt, bspw. Data Warehouse Architekturen, ausgewählten Analysemethoden und Einblicken in moderne BI-Dashboard-Tools.
Ziel ist eine ganzheitliche Sicht auf den BI-Prozess im Unternehmenskontext.
:::

## Data Science
### Begriffsabgrenzung

::: {.callout-tip title="Definition"}
Data Science beschäftigt sich mit einer zweckorientierten Datenanalyse und der systematischen Generierung von Entscheidungshilfen und -grundlagen, um Wettbewerbsvorteile erzielen zu können. Der Schwerpunkt liegt dabei nicht auf den Daten selbst, sondern auf der Art und weise, wie diese verarbeitet und analysiert werden [siehe @gesellschaft_fur_informatik_data_2019].
:::

::: {.fragment .fade-in}

* Data Science ist ein vergleichsweise neues wissenschaftliches Feld, eine Kombination aus Statistik und Informatik, insbesondere Software Engineering
* Da es sich um ein junges Feld handelt sind Definitionen und die damit verbundenen Rollen im stetigen Wandel
:::
::: {.notes}
Dieser Wandel geht so weit, dass die Rolle "Data Scientist" heute seltener auf Job-Portalen zu finden ist, als noch vor einigen Jahren. Stattdessen werden vermehrt spezialisierte Rollen wie "Data Engineer", "Data Analyst" oder "Machine Learning Engineer" ausgeschrieben.
:::

## Data Science
### Schwerpunkte

* Aufgrund der potentiellen Breite des Felds erfolgt oft eine genauere Aufteilung in vier Kernbereiche:

::: {.incremental}
  1. **Data Engineering**: Methoden und Prozesse für die Speicherung, Haltung und Replikation von Daten
  2. **Data Analytics**: Datenanalyse mit statistischen Methoden
  3. **Predictive Modelling**: Die Verwendung von statistischen Methoden zur Vorhersage
  4. **Machine Learning**: Algorithmen, die aus Daten lernen, Muster erkennen und hierauf aufbauend neue Situationen oder zukünftige Entwicklungen vorhersagen
:::

## Data Science
### Reifegrade von Data Analytics

![Quelle: [Milind Desai on Medium.com](https://medium.com/@milind.bapuji.desai/understanding-the-analytics-maturity-model-84982836b107)](img/data_analytics_maturity.webp)

::: {.notes}
1. **Descriptive Analytics**: Beschreibende Analyse des Ist-Zustandes und der Vergangenheit. Im Mittelpunkt steht die Frage: Was ist passiert?
2. **Diagnostic Analytics**: Analysiert die Zusammenhänge, die zum Ist-Zustand geführt haben und führt oft mehrere deskriptive Charakteristika zusammen: Warum ist der Status Quo eingetreten?
3. **Predictive Modelling**: Ausgehend von einem detaillierten Verständnis des Status Quo wird eine Prognose für die Zukunft erstellt: Was wird passieren?
4. **Prescriptive Analytics**: Dient der Identifizierung möglicher Handlungsoptionen auf Basis der Prognosen, entweder um eine schlechte Prognose abzuwenden oder die Realisierung einer guten Prognose zu unterstützen. Mit anderen Worten: Was muss getan werden, um die Zukunft zu unseren Gunsten zu beeinflussen
:::


## Data Science
### Data Science, Data Analytics, Data Mining?

::: {.incremental}
* Die Unterscheidung zwischen den Begriffen Data Analytics, Data Mining und Data Science ist nicht immer trennscharf
* Data Mining ist meist definiert als der Prozess der Informationsextraktion aus Daten und ist ebenso wie Data Analytics  eine Teilmenge von Data Science
* In dieser Vorlesung dient Data Science als methodischer Baukasten, um den BI-Prozess mit modellgestützten Methoden anzureichern und Zusammenhänge sichtbar zu machen
* Hierbei steht der Zweck der Modelle, nämlich die Entscheidungsunterstützung, im Vordergrund
:::

## Business Intelligence und Data Science
### Zusammenführung der Begriffe und inhaltlicher Aufbau der Vorlesung


![BIA Gesamtansatz. Eigene Darstellung in Anlehnung an @baars_business_2021.](img/bia_ordnungsrahmen.drawio.png){width="65%"}

::: {.notes}
Die Abbildung illustriert den BIA-Ansatz als dreiteiligen Ordnungsrahmen, bestehend aus Datenbereitstellung, Informationsgenerierung und Informationsbereitstellung. Die Datenerfassung aus operativen und externen Systemen ist diesem Ordnungsrahmen hier vorgelagert. Das hieraus entstehende Modell entspricht der prozessualen BI-Dimension.

Ganz unten startenw ir mit zahlreichen operative und externen Quellsysteme, beispielsweise ERP-Systeme (häufig SAP), Produktdatenmanagement Systeme (PDM) oder Manufacturing Execution Systeme (MES). 

Hinzu kommen häufig offene Daten wie Wetter- oder Konjunkturdaten und insbesondere im industriellen Kontext verstärkt Sensordaten aus internetfähigen Maschinen, sogenannte Internet of Things (IoT) Devices. 
Die strukturierte und systematische Integration dieser Daten mittels ETL Methoden (Extract, Transfer, Load), ist die erste Herausforderung jedes integrierten BIA-Systems.

Die sogenannte Datenbereitstellung dient der konsistenten und strukturierten Speicherung und Persistierung aller relevanten Daten aus den oben genannten Quellsystemen. 
Hier gilt es verschiedene Konzepte näher zu beleuchten, insbesondere gängige Data Warehouse Konzepte, die meist aus sogenannten Data Marts und Core Data Warehouses bestehen und der themenbezogenen und integrierten Datenhaltung dienen. 
Je nach Anwendung wird das Datenmaterial meist voraggregiert. Zur Integration großvolumiger und schnell einlaufender Daten hat sich ergänzend das Konzept eines Data Lakes etabliert, in dem anders als im Data Warehouse Rohdaten ohne Aggregation abgelegt und verfügbar gemacht werden.

Die Informationsgenerierung als zweite Schicht dient der Umwandlung der Rohdaten in entscheidungsfreundliche Formate, bspw. berichtsorientierte oder modellgestützte Analysen. Hier werden aus Daten erste Informationen generiert, auf Basis derer weitere Erkenntnisse über den Status Quo entstehen und mögliche Prognosen für die Zukunft erstellt werden können. Das Bindeglied zwischen Datenbereitstellung und Informationsgenerierung sind Systeme zur Datenabfrage und Exploration.

Die Darstellung, Kanalisierung und Verbreitung von Informationen folgt in der dritten Schicht, der Informationsbereitstellung. Neben modernen Self-Service BI-Tools umfasst dies auch zielgruppenadäquate Präsentationen oder statische Berichte.
:::

## Dispositive und operative Daten


### Operative versus dispositive Aufgaben

::: {.incremental}
* Anders als die anfänglichen MSS unterstützen moderne BI-Systeme sowohl operative, als auch dispositive Aufgaben 
* Dispositive Aufgaben sind Leitungs- und Lenkungstätigkeiten im betrieblichen Ablauf
* Operative Aufgaben umfassen die Leistungserstellung oder -verwertung
* An beide Aufgabenfelder gelten unterschiedliche Anforderungen, die in Daten und Systemen abgebildet werden müssen

:::
## Dispositive und operative Daten
### Operative versus dispositive Daten
::: {style="font-size: 0.90em"}
::: {.incremental}
:::: {.columns}

::: {.column width="50%"}
Dispositive Daten

* Unterstützen Leitungs- und Lenkungstätigkeiten im betrieblichen Ablauf
* Häufig verdichtet, transformiert und themenbezogen aufbereitet und mit Historie angereichert
:::

::: {.column width="50%"}
Operative Daten

* Dienen der Abwicklung von Geschäftsprozessen und werden im Rahmen von Transaktionen^[Atomare und logisch untrennbare Datenbankvorgänge] erzeugt
* Sehr granular und mit hoher Änderungsrate
* Beispiele sind Bestellungen, Aufträge und Lagerbestände oder Stammdaten
:::
::::
:::
:::
## Dispositive und operative Daten
### Operative versus dispositive Daten  

::: {style="font-size: 0.70em"}
|  | Operative Daten | Entscheidungsorientierte Daten |
|---|---|---|
| Ziel | Abwicklung der Geschäftsprozesse | Informationen für Entscheidungen |
| Ausrichtung | Detailliert und granular | Meist verdichtet und transformiert mit  Metadaten |
| Zeitbezug | Aktualität steht im Vordergrund, Zeitpunkt der Transaktion, keine Historisierung | Aktualität variiert mit der Aufgabe, Historienbetrachtung ist möglich |
|Modellierung  |Keine Altbestände | Sachgebiets- und themenbezogen orientiert und anwendungstauglich  |
|Zustand  |Häufig redundant und inkonsistent zwischen Systemen  |Konsistent modelliert, Redundanz bewusst  |
|Update  |Laufend, Real-time  | Ergänzend als Fortschreibung  |
|Queries  |Strukturiert, standardisiert und meistens statisch  |Ad-hoc und dynamisch für wechselnde Fragestellungen sowie Standardberichte  |

: Charakteristika operativer und entscheidungsorientierter Daten im Vergleich. In Anlehnung an  @baars_business_2021 {tbl-colwidths="[8,46,46]"}
:::
## Dispositive und operative Daten
### Überführung von Daten in Information

:::: {.columns}

::: {.column width="45%"}
* Hauptziel dieser Vorlesung ist die Überführung von operationalen Daten in entscheidungsrelevante Informationen
* Dies hat zwei Hauptaspekte:
  * Block 1-4: Technische Infrastruktur und Architektur
  * Block 5-9: Methodische Konzepte wie modellorientierte Analysen

:::

::: {.column width="55%"}

```{r, message= FALSE, echo = FALSE, warning = FALSE}
read_csv("/home/seb/LeibnizFH/bi_and_ds-lecture_notes/data/ablauf.csv") %>% 
  select(-Slides) %>% 
  select(-`Gruppe D`, -`Gruppe C`) %>%
  filter(!Block %in% c("10", "-")) %>% 
  kable( escape = FALSE)  %>% 
  kable_styling(font_size = 24)
```
:::

::::




<!-- # Der Data Science Prozess -->

<!-- ## CRISP-DM  -->
<!-- ### Geschäftsmodell -->

<!-- 1. Verständnis des Geschäftsmodells -->
<!--     * Verständnis des Geschäftsmodells und der Unternehmensziele -->
<!--     * Berücksichtigung von Chancen, Risiken und zeitlichen Aspekten -->
<!--     * Definition des erwartbaren Nutzens und messbarer Erfolgskriterien -->

<!-- ## CRISP-DM -->
<!-- ### Anwendungs-/Datendomäne und Datenvorbereitung -->

<!-- 2. Verständnis der Anwendungs- und Datendomäne -->
<!--     * Analyse der Unternehmensprozesse und Datenquellen -->
<!--     * Zusammenführung, Beschreibung und Exploration der Zieldaten -->
<!--     * Eruierung der Datenqualität -->
<!-- 3. Datenvorbereitung -->
<!--     * Zusammenführung und Beschreibung polystrukturierter Daten -->
<!--     * Berechnung von Kennzahlen und Durchführung von Datentransformationen -->

<!-- ## CRISP-DM -->
<!-- ### Modellierung, Evaluation und Bereitstellung -->

<!-- 4. Modellierung -->
<!--     * Modellauswahl und -erstellung -->
<!--     * Iterativer Prozess zur Weiterentwicklung von Modellen -->
<!--     * Möglicher Einsatz vortrainierter Modelle für die Verfeinerung -->
<!-- 5. Evaluation -->
<!--     * Bewertung der erstellten Modelle anhand definierter Erfolgskriterien -->
<!-- 6. Einsatz -->
<!--     * Umsetzung der Ergebnisse in die Praxis und Integration in die Unternehmensprozesse -->

## Slide Title {visibility="hidden"}

@gluchowski_analytische_2016

@sharma_analytics_2023
@gluchowski_analytische_2016

    
## Quellen    
