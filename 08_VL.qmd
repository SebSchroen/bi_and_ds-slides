---
title: "Business Intelligence & Data Science"
subtitle: "Vorlesung 8"
format:
  revealjs:
    incremental: true  
---
  
  
```{r}
library(kableExtra)
library(tidyverse)
library(patchwork)
library(tidymodels)
library(plotly)
library(ggcorrplot)
data <- read_csv("data/songs_with_features.csv") %>% 
  filter(category %in% c("Klassik", "Dance/Electronic")) %>% 
  select(track.name, track.artist, category, energy, danceability) %>% 
  mutate(category = case_when(
    category == "Klassik" ~ "Klassik",
    category == "Dance/Electronic" ~ "EDM"
  ))

set.seed(1)

split <- initial_split(data, prop = 3*0.029156, strata = category)
training <- 
  training(split) %>% 
  select(category, energy, danceability, track.name, track.artist) %>% 
  mutate(edm = case_when(
    category == "EDM" ~ 1,
    category == "Klassik" ~ 0
  )) %>% 
  mutate(edm_factor = as.factor(edm))

explicit <- readRDS("data/category_songs_raw.rds") %>% 
  distinct(track.id, track.explicit) 

data_multi <- read_csv("data/songs_with_features.csv") %>% 
  filter(category %in% c("Klassik", "Dance/Electronic", "Hip-Hop")) %>% 
  mutate(category = case_when(
    category == "Klassik" ~ "Klassik",
    category == "Dance/Electronic" ~ "EDM",
    category == "Hip-Hop" ~ "Hip-Hop"
  )) %>% 
  left_join(explicit, by = c("track.id" = "track.id")) %>%
  select(track.name, track.artist, category, energy, danceability, speechiness, track.explicit) %>% 
  mutate(track.explicit = as.factor(track.explicit)) %>% 
  mutate(category = as.factor(category))


split_multi <- initial_split(data_multi, prop = 0.7, strata = category)


training_multi <- training(split_multi)
testing_multi <- testing(split_multi)


```




## Der Plan für heute...
### Vorlesung 8

* Modellgestützte Analysen
  * Nearest Neighbor Klassifikation
  * Entscheidungsbäume

## Nearest Neighbor Klassifikation
### Grundidee

* Nearest Neighbor Klassifikation oder meist K-Nearest Neighbor (KNN) basiert auf der Idee, dass ähnliche Datenpunkte zu ähnlichen Klassen gehören
* KNN macht keine Annahmen über die Verteilung der Daten und die For der Entscheidungsgrenze
* KNN ist ein sogenanntes "lazy learning" Verfahren, da es keine explizite Modellbildung durchführt
* Die Klassifikation erfolgt durch die Mehrheitsentscheidung der $K$ nächsten Nachbarn in den Testdaten


## Nearest Neighbor Klassifikation
### Grundidee

::: {style="font-size: 0.90em"}

* Auch bei KNN wird die Wahrscheinlichkeit ermittelt, mit der Element $x_0$ zur Klasse $j$ gehört

::: {.fragment .fade-in}

$$
P(Y=j|X= x_0) = \frac{1}{K} \sum_{i \in \aleph_0} I(y_i=j)
$$
:::

* Dabei repräsentiert $\aleph_0$ die Menge der $K$ nächsten Nachbarn von $x_0$ und $I(y_i=j)$ ist eine Indikatorfunktion, die 1 zurückgibt, wenn $y_i=j$ und 0 sonst
* Die Zuordnung erfolgt dann durch die Klasse mit der höchsten Wahrscheinlichkeit im Sinne einer Mehrheitsentscheidung
* Beispiel: Wenn $K = 3$, wird die Klasse bestimmt, zu der die Mehrheit der drei nächsten Nachbarn gehört und die Wahrscheinlichkeit entspricht dem Anteil der Nachbarn in dieser Klasse

::: 

## Nearest Neighbor Klassifikation
### Bestimmung der Nachbarn

* Die Bestimmung der nächsten Nachbarn erfolgt durch sogenannte Ähnlichkeits- oder Distanzmaße
* Es gibt eine Vielzahl von Distanzmaßen, die sich in der Berechnung und Interpretation unterscheiden
* Die einfachste Form ist die euklidische Distanz, in allgemeiner Form:

::: {.fragment .fade-in}
$$
d(p,q) = \sqrt{(p_1-q_1)^2 + (p_2-q_2)^2  + ... + (p_n-q_n)^2}
$$
:::

## Nearest Neighbor Klassifikation
### Bestimmung der Nachbarn

* In zwei Dimensionen vereinfacht sich diese Gleichung zu:

::: {style="font-size: 0.75em"}

::: {.fragment .fade-in}

$$
d(p,q) = \sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}
$$

:::

:::

::: {.fragment .fade-in}

```{r, fig.width = 8, fig.height=4, fig.align='center'}

data_for_plot <- training_multi %>%
  group_by(track.artist) %>% 
  summarise(energy = round(mean(energy),1), danceability = round(mean(danceability),1)) %>% 
  filter(track.artist %in% c("Demi Lovato", "Wolfgang Amadeus Mozart"))





  ggplot(data = data_for_plot, aes(x = energy, y = danceability, label = track.artist))+ 
  geom_text(nudge_y = 0.05, nudge_x = 0.03) +
#  xlim(0,0.7) + ylim(-0.1,1) + 
  geom_text(label = c("q", "p"), size = 5, nudge_y = 0.01, nudge_x = c(0.01, -0.01)) + 
    labs(x = "Energy", y = "Danceability") +
    geom_point() +
    theme_minimal() +
    scale_y_continuous(limits= c(0, 1), breaks = c(-0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1),
                       labels = c(-0.1, 0, 0.1, 0.2,  expression("p"["2"]), 0.4, 0.5, 0.6, 0.7, 0.8, expression("q"["2"]), 1)) +
        scale_x_continuous(limits = c(0,0.7), breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1),
                       labels = c(0,  expression("p"["1"]), 0.2, 0.3, 0.4,  expression("q"["1"]), 0.6, 0.7, 0.8, 0.9, 1)) 


```

:::


## Nearest Neighbor Klassifikation
### Bestimmung der Nachbarn

::: {.nonincremental}


* In zwei Dimensionen vereinfacht sich diese Gleichung zu:

:::

::: {style="font-size: 0.75em"}

$$
d(p,q) = \sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}
$$

:::

```{r, fig.width = 8, fig.height=4, fig.align='center'}

data_for_plot <- training_multi %>%
  group_by(track.artist) %>% 
  summarise(energy = round(mean(energy),1), danceability = round(mean(danceability),1)) %>% 
  filter(track.artist %in% c("Demi Lovato", "Wolfgang Amadeus Mozart"))





  ggplot(data = data_for_plot, aes(x = energy, y = danceability, label = track.artist))+ 
  geom_text(nudge_y = 0.05, nudge_x = 0.03) +

  geom_text(label = c("q", "p"), size = 5, nudge_y = 0.01, nudge_x = c(0.01, -0.01)) + 
    geom_point() +
        labs(x = "Energy", y = "Danceability") +
    theme_minimal() +
    annotate("segment", x = data_for_plot$energy[1], xend = data_for_plot$energy[2], 
             y = data_for_plot$danceability[1], yend = data_for_plot$danceability[2],
             colour = "steelblue") +
      annotate("segment", x = data_for_plot$energy[1], xend = data_for_plot$energy[1], 
             y = data_for_plot$danceability[1], yend = data_for_plot$danceability[2],
             colour = "steelblue", linetype = "dashed") +
        annotate("segment", x = data_for_plot$energy[1], xend = data_for_plot$energy[2], 
             y = data_for_plot$danceability[2], yend = data_for_plot$danceability[2],
             colour = "steelblue", linetype = "dashed") +
     annotate("text", x = 0.55, y = 0.6, label = expression("q"["2"]-"p"["2"])) +
     annotate("text", x = 0.3, y = 0.25, label = expression("q"["1"]-"p"["1"])) +
    annotate("text", x = 0.3, y = 0.70, label = "d(p,q)") + 
    scale_y_continuous(limits= c(0, 1), breaks = c(-0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1),
                       labels = c(-0.1, 0, 0.1, 0.2,  expression("p"["2"]), 0.4, 0.5, 0.6, 0.7, 0.8, expression("q"["2"]), 1)) +
        scale_x_continuous(limits = c(0,0.7), breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1),
                       labels = c(0,  expression("p"["1"]), 0.2, 0.3, 0.4,  expression("q"["1"]), 0.6, 0.7, 0.8, 0.9, 1)) 
  


```


## Nearest Neighbor Klassifikation
### Bestimmung der Nachbarn

::: {.nonincremental}


* In zwei Dimensionen vereinfacht sich diese Gleichung zu:

:::

::: {style="font-size: 0.75em"}

$$
d(p,q) = \sqrt{(0,5-0,1)^2 + (0,3-0,9)^2} = 0,7221
$$

:::

```{r, fig.width = 8, fig.height=4, fig.align='center'}

data_for_plot <- training_multi %>%
  group_by(track.artist) %>% 
  summarise(energy = round(mean(energy),1), danceability = round(mean(danceability),1)) %>% 
  filter(track.artist %in% c("Demi Lovato", "Wolfgang Amadeus Mozart"))





  ggplot(data = data_for_plot, aes(x = energy, y = danceability, label = track.artist))+ 
  geom_text(nudge_y = 0.05, nudge_x = 0.03) +

  geom_text(label = c("q", "p"), size = 5, nudge_y = 0.01, nudge_x = c(0.01, -0.01)) + 
    geom_point() +
        labs(x = "Energy", y = "Danceability") +
    theme_minimal() +
    annotate("segment", x = data_for_plot$energy[1], xend = data_for_plot$energy[2], 
             y = data_for_plot$danceability[1], yend = data_for_plot$danceability[2],
             colour = "steelblue") +
      annotate("segment", x = data_for_plot$energy[1], xend = data_for_plot$energy[1], 
             y = data_for_plot$danceability[1], yend = data_for_plot$danceability[2],
             colour = "steelblue", linetype = "dashed") +
        annotate("segment", x = data_for_plot$energy[1], xend = data_for_plot$energy[2], 
             y = data_for_plot$danceability[2], yend = data_for_plot$danceability[2],
             colour = "steelblue", linetype = "dashed") +
     annotate("text", x = 0.55, y = 0.6, label = expression("q"["2"]-"p"["2"])) +
     annotate("text", x = 0.3, y = 0.25, label = expression("q"["1"]-"p"["1"])) +
    annotate("text", x = 0.3, y = 0.70, label = "d(p,q)") + 
    scale_y_continuous(limits= c(0, 1), breaks = c(-0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1),
                       labels = c(-0.1, 0, 0.1, 0.2,  expression("p"["2"]), 0.4, 0.5, 0.6, 0.7, 0.8, expression("q"["2"]), 1)) +
        scale_x_continuous(limits = c(0,0.7), breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1),
                       labels = c(0,  expression("p"["1"]), 0.2, 0.3, 0.4,  expression("q"["1"]), 0.6, 0.7, 0.8, 0.9, 1)) 
  


```



## Nearest Neighbor Klassifikation
### Schritte und Intuition

::: {.nonincremental}

1. Bestimme die Trainingsdaten durch einen Split, bspw. 70% Trainingsdaten, lege $K$ fest

:::

```{r, fig.width = 8, fig.height=4, fig.align='center'}
set.seed(2)
training_knn_visual <- training_multi %>%
  group_by(track.artist, category) %>% 
  summarise(energy = round(mean(energy),1), danceability = round(mean(danceability),1)) %>% 
  ungroup() %>% 
  mutate(length_name = nchar(track.artist)) %>% 
  filter(length_name <= 8) %>% 
  sample_n(10)



ggplot(data = training_knn_visual, aes(x = energy, y = danceability, label = track.artist, color = category))+ 
  geom_text(size = 4, nudge_y = rnorm(10, mean = 0, sd = 0.02), nudge_x = rnorm(10, mean = 0, sd = 0.02)) +
  geom_point() +
  labs(x = "Energy", y = "Danceability",color = "Genre") +
  theme_minimal()

```




## Nearest Neighbor Klassifikation
### Schritte und Intuition

::: {.nonincremental}

2. Für den neuen Datenpunkt `Doja Cat`, berechne die euklidische Distanz zu allen anderen Punkten im Trainingsdatensatz

:::

```{r, fig.width = 8, fig.height=4, fig.align='center'}
set.seed(2)
training_knn_visual <- training_multi %>%
  group_by(track.artist, category) %>% 
  summarise(energy = round(mean(energy),1), danceability = round(mean(danceability),1)) %>% 
  ungroup() %>% 
  mutate(length_name = nchar(track.artist)) %>% 
  filter(length_name <= 8) %>% 
  sample_n(10)

testing_knn_visual  <- training_multi %>%
  group_by(track.artist, category) %>% 
  summarise(energy = round(mean(energy),1), danceability = round(mean(danceability),1)) %>% 
  filter(track.artist == "Doja Cat")

ggplot(data = training_knn_visual, aes(x = energy, y = danceability, label = track.artist, color = category))+ 
  geom_text(size = 4, nudge_y = rnorm(10, mean = 0, sd = 0.02), nudge_x = rnorm(10, mean = 0, sd = 0.02)) +
  geom_point() +
  labs(x = "Energy", y = "Danceability",color = "Genre") +
  theme_minimal() + 
  geom_point(data = testing_knn_visual, aes(x = energy, y = danceability), color = "orange", size = 2) +
  geom_text(data = testing_knn_visual, aes(x = energy, y = danceability, label = track.artist), color = "orange", nudge_y = 0.02) 

```

## Nearest Neighbor Klassifikation
### Schritte und Intuition

::: {.nonincremental}

2. Für den neuen Datenpunkt `Doja Cat`, berechne die euklidische Distanz zu allen anderen Punkten im Trainingsdatensatz

:::

```{r, fig.width = 8, fig.height=4, fig.align='center'}
set.seed(2)
training_knn_visual <- training_multi %>%
  group_by(track.artist, category) %>% 
  summarise(energy = round(mean(energy),1), danceability = round(mean(danceability),1)) %>% 
  ungroup() %>% 
  mutate(length_name = nchar(track.artist)) %>% 
  filter(length_name <= 8) %>% 
  sample_n(10)

testing_knn_visual  <- training_multi %>%
  group_by(track.artist, category) %>% 
  summarise(energy = round(mean(energy),1), danceability = round(mean(danceability),1)) %>% 
  filter(track.artist == "Doja Cat")

ggplot(data = training_knn_visual, aes(x = energy, y = danceability, label = track.artist, color = category))+ 
  geom_text(size = 4, nudge_y = rnorm(10, mean = 0, sd = 0.02), nudge_x = rnorm(10, mean = 0, sd = 0.02)) +
  geom_point() +
  labs(x = "Energy", y = "Danceability",color = "Genre") +
  theme_minimal() + 
  geom_point(data = testing_knn_visual, aes(x = energy, y = danceability), color = "orange", size = 2) +
  geom_text(data = testing_knn_visual, aes(x = energy, y = danceability, label = track.artist), color = "orange", nudge_y = 0.02) + 
  annotate("segment", x = training_knn_visual$energy, xend = testing_knn_visual$energy, 
           y = training_knn_visual$danceability, yend = testing_knn_visual$danceability,
           colour = "steelblue", linetype = "dashed") 

```


## Nearest Neighbor Klassifikation
### Schritte und Intuition

::: {.nonincremental}

3. Identifiziere die $K$ nächsten Nachbarn und ordne den neuen Punkt der Klasse zu, in der die Mehrheit der Nachbarn liegt

:::

::: {style="font-size: 0.45em"}

```{r}
# compute euclidean distance in two dimensions:
euclidean_distance <- function(x1, x2){
  sqrt(sum((x1 - x2)^2))
}


training_knn_visual %>% 
  select(category, track.artist, energy, danceability) %>%
  mutate(`Distanz zu Doja Cat` = map2_dbl(energy, danceability, ~euclidean_distance(c(.x, .y), c(testing_knn_visual$energy, testing_knn_visual$danceability)))) %>% 
  arrange(`Distanz zu Doja Cat`) %>% 
  mutate(Rang = 1:n()) %>% 
  kable(digits = 2)


```

:::

* Wie ist die Vorhersage bei und die zugehörige Wahrscheinlichkeit bei:
  * K = 1
  * K = 5
  * K = 10



## Nearest Neighbor Klassifikation
### Schritte und Intuition



* Für die ausgewählten $K$ erhalten wir das folgende Ergebnis:

::: {.fragment .fade-in}

```{r}
train <- training_knn_visual %>% ungroup() %>%  select(energy, danceability, category)
test <- testing_knn_visual %>%  ungroup() %>% select(energy, danceability, category)

library(class)
results <- tibble(Vorhersage = character(), Wahrscheinlichkeit = numeric(), K = numeric())

for (k in 1:10) {
  knn <- knn(train=train[,-3], test=test[,-3], cl=train$category, k = k, prob = TRUE)
  results <- results %>% add_row(Vorhersage = knn[1], Wahrscheinlichkeit = attr(knn, "prob")[1], K = k)
}

results %>% 
  filter(K %in% c(1,5,10)) %>%
  mutate(Wahrscheinlichkeit = "", 
         Vorhersage = "") %>% 
  kable(digits = 2)


```

:::

* Anschließend wird der Prozess für alle anderen neuen Beobachtungen wiederholt 

## Nearest Neighbor Klassifikation
### Modellevaluation

:::: {.columns}

::: {.column width=35%}

* Die Modellevaluation erfolgt anschließend analog zur logistischen Regression mit den bisher dargestellten Metriken Accuracy, Precision, Recall und F1-Score
* $K$ ist ein wichtiger Hyperparameter, der sorgfältig gewählt werden sollte

:::

::: {.column width=65%}

::: {.fragment .fade-in}



```{r, fig.height = 5, fig.width = 7}


set.seed(1)

multi_metric <- metric_set(yardstick::accuracy, recall, precision, f_meas)


library(foreach)
K <- c(1,  3, 5, 10,15,20, 25, 30, 40, 50, 60, 80, 100)


metrics_multiple_k <- foreach(i = K, .combine = "rbind") %do% {
  knn_multi <- 
    nearest_neighbor(neighbors = i) %>% 
    set_engine("kknn") %>% 
    set_mode("classification") %>% 
    fit(category ~ danceability + energy, data = training_multi)
  
  knn_multi %>% 
    augment(testing_multi) %>% 
    multi_metric(truth = category, estimate = .pred_class) %>% 
    mutate(K = i)
  
}



ggplot(data =metrics_multiple_k, aes(x= K, y = .estimate, color = .metric)) + geom_line() +
   geom_point() +
  theme_minimal() +
  labs(x = "K", y = "Wert", color = "Metrik") +
    scale_x_continuous(breaks = seq(0, 100, by = 10)) 

```
:::

:::
  
::::



## Nearest Neighbor Klassifikation
### Modellevaluation


:::: {.columns}

::: {.column width=50%}

* Offenbar maximiert $K = 60$ sämtliche Metriken
* Mit $K$ = 60 erhalten wir die Konfusionsmatrix rechts sowie die Metriken unten:

::: {style="font-size: 0.75em"}

::: {.fragment .fade-in}

```{r}
knn_multi <- 
  nearest_neighbor(neighbors = 60) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") %>% 
  fit(category ~ danceability + energy, data = training_multi)

knn_multi %>%
  augment(testing_multi) %>% 
  multi_metric(truth = category, estimate = .pred_class) %>% 
  kable(digits = 2)


```

:::

:::

:::

::: {.column width=50%}

::: {.fragment .fade-in}

```{r, fig.height = 5, fig.width = 5}
knn_multi %>%
  augment(testing_multi) %>% 
  conf_mat(truth = category, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

```

:::

:::


::::

## Entscheidungsbäume
### Grundidee

* Entscheidungsbäume sind ein weiteres Verfahren des überwachten Lernens, das sich sowohl für Klassifikations- als auch für Regressionsprobleme eignet
* Die Klassifikation mit Entscheidungsbäumen umfasst grob zwei Schritte:

1. Aufteilung der erklärenden Variablen $X_1$, $X_2$, ..., $X_p$ in $K$ immer kleinere, nicht überlappende Regionen/Untergruppen (Blätter) $R_1$, $R_2$, ..., $R_J$ anhand von Entscheidungsregeln (Knoten)
2. Klassifikation der Testdaten anhand der Entscheidungsregeln. Alle Beobachtungen, die in Region $R_k$ fallen, werden der Klasse $k$ zugeordnet


## Entscheidungsbäume
### Schritte und Intuition

::: {style="font-size: 0.90em"}
* Start ist der gesamte Datensatz, auch Root-Node oder Wurzelknoten
* Gesucht wird die beste Variable und der beste Schwellenwert, um den Datensatz in zwei Gruppen zu teilen, auch Entscheidungsregel oder Split
* Zur Ermittlung der besten Entscheidungsregel wird ein Maß für die Homogenität der Regionen verwendet, bspw. der Gini-Index 

::: {.fragment .fade-in}
$$
G = \sum_{k=1}^{K} \hat{p}_{mk}(1-\hat{p}_{mk})
$$
:::

* wobei $\hat{p}_{mk}$ der Anteil der Beobachtungen der Klasse $k$ in Region $m$ ist. 
* Wenn die Region $m$ homogen ist, ist der Gini-Index 0, eine hälftige Aufteilung ergibt bei zwei Klassen den Wert 0.5 


:::

## Entscheidungsbäume
### Schritte und Intuition

* Anschließend wird mit dem Gini-Index der Informationsgewinn $IG$ berechnet, der sich aus der Differenz des Gini-Index vor und nach dem Split ergibt:

::: {.fragment .fade-in}

$$
\text{IG} = G_{\text{init}} - G_{\text{split}}
$$
:::

* wobei $G_{\text{init}}$ der Gini-Index vor dem Split und $G_{\text{split}}$ der Gini-Index nach dem Split ist
* $G_{\text{split}}$ ist die gewichtete Summe der Gini-Indizes der Regionen, die durch den Split entstehen

## Entscheidungsbäume
### Schritte und Intuition

:::: {.columns}

::: {.column width=50%}

::: {style="font-size: 0.78em"}

* Wir starten mit dem gesamten Trainings-Datensatz für die drei Genres Klassik, EDM und Hip-Hop
* Zunächst berechnen wir den Gini-Index für die Root-Node:

::: {.fragment .fade-in}

```{r}
gini_tmp <- training_multi %>% 
  group_by(category) %>% 
  summarise(p = n()/3780) %>% 
  mutate(`1-p` = 1-p) %>%
  mutate(Gini = p*(1-p)) 

gini_tmp %>%
  kable(digits = 3)

```

:::

::: {.fragment .fade-in}

$$
G_{\text{init}} = `r round(sum(gini_tmp$Gini[1]), 3)` + `r round(sum(gini_tmp$Gini[2]), 3)` + `r round(sum(gini_tmp$Gini[3]), 3)` = `r round(sum(gini_tmp$Gini), 3)`
$$

:::

* Anschließend wird der beste Split für die Root-Node ermittelt, indem der Informationsgewinn für alle möglichen Splits berechnet wird

:::
:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}

Gini = function(x, group, thres) {
  
  df = data.frame(x, group, right = ifelse(x >= thres, "right", "left"))
  A = as.matrix(table(df$right, df$group))
  B = prop.table(A,1) # row-wise proportions (splits)
  a = colSums(A) # classes totals
  b = rowSums(A) # split totals
  n = sum(A) # total samples
  gini.init = signif(sum(a/n * (1 - a/n)), 3) # initial Gini index
  x = B[1,] # right split
  y = B[2,] # left split
  gini.x = sum(x * (1 - x)) # right split Gini index
  gini.y = sum(y * (1 - y)) # left split Gini index
  gini.split = signif(gini.x * b[1]/n + gini.y * b[2]/n, 3) # Gini index after split
  gini.gain = signif(gini.init - gini.split, 3) # Gini index information gain
  gini.gain.perc = signif(gini.gain / gini.init * 100, 3)
  DF = data.frame(gini.init, gini.split, gini.gain, gini.gain.perc)
  row.names(DF) = ""
  return(DF)
}


training_multi %>% 
  ggplot(aes(x = energy, y = danceability, color = category)) +
  geom_point() +
  labs(x = "Energy", y = "Danceability", color = "Genre") +
  theme_minimal() +
    theme(legend.position = "bottom") 




```

:::

::::



## Entscheidungsbäume
### Schritte und Intuition

:::: {.columns}

::: {.column width=50%}

::: {style="font-size: 0.8em"}

* Für den Beispiel-Split rechts erhalten wir links von der gestrichelten Linie

::: {.fragment .fade-in}

```{r}
left_split <- training_multi %>% 
  filter(energy < 0.5) %>%
  group_by(category) %>% 
  summarise(n = n()) %>% 
  mutate(N = sum(n),
         p = n/N) %>% 
  mutate(`1-p` = 1-p) %>%
  mutate(Gini = round(p*(1-p),3))  %>% 
  select(category, n, p, `1-p`, Gini)  %>% 
  rbind(tibble(category = "Total",p =1, `1-p` = 1, n = sum(.$n), Gini = sum(.$Gini))) 

left_split %>% 
  kable(digits = 3)
  
```

:::

* und rechts davon:

::: {.fragment .fade-in}

```{r}
right_split <- training_multi %>% 
  filter(energy > 0.5) %>%
  group_by(category) %>% 
  summarise(n = n()) %>% 
  mutate(N = sum(n),
         p = n/N) %>% 
  mutate(`1-p` = 1-p) %>%
  mutate(Gini = round(p*(1-p),3))  %>% 
  select(category, n, p, `1-p`, Gini)  %>% 
  rbind(tibble(category = "Total",p =1, `1-p` = 1, n = sum(.$n), Gini = sum(.$Gini))) 

right_split %>% 
  kable(digits = 3)
  
```

:::

:::
:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}


training_multi %>% 
  ggplot(aes(x = energy, y = danceability, color = category)) +
  geom_point() +
  labs(x = "Energy", y = "Danceability", color = "Genre") +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  theme_minimal() +
    theme(legend.position = "bottom") 








```

:::

::::




## Entscheidungsbäume
### Schritte und Intuition

:::: {.columns}

::: {.column width=50%}

::: {style="font-size: 0.8em"}


```{r}

N <- nrow(training_multi)
N_left <- left_split %>% filter(category == "Total") %>% pull(n)
N_right <- right_split %>% filter(category == "Total") %>% pull(n)

Gini_left <- left_split %>% filter(category == "Total") %>% pull(Gini) %>% round(., 3)
Gini_right <- right_split %>% filter(category == "Total") %>% pull(Gini) %>%  round(.,3)
Gini_init <- sum(gini_tmp$Gini) %>% round(., 3)


```

* Der Information Gain beträgt für den Split bei 0.5:

::: {.fragment .fade-in}

$$
IG = G_{\text{init}} - G_{\text{split}} = \\`r Gini_init` - \left( \frac{`r N_left`}{`r N` } \cdot `r Gini_left` + \frac{`r N_right`}{`r N` } \cdot `r Gini_right` \right) = \\`r round(Gini_init - (N_left/N * Gini_left + N_right/N * Gini_right), 4)`
$$

:::

* Für alle Variablen und Splits wird nun der Split mit dem höchsten Informationsgewinn ermittelt



:::
:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}


training_multi %>% 
  ggplot(aes(x = energy, y = danceability, color = category)) +
  geom_point() +
  labs(x = "Energy", y = "Danceability", color = "Genre") +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  theme_minimal() +
    theme(legend.position = "bottom") 





```

:::

::::


## Entscheidungsbäume
### Schritte und Intuition

:::: {.columns}

::: {.column width=50%}

* Der Split bei 0.5 ist allerdings nicht ideal
* Die Grafik rechts illustriert den Informationsgewinn für alle möglichen Splits der Variablen Energy und Danceability
* Der maximale Informationsgewinn wird für den Split bei ca. 0.37 für die Variable Energy erzielt

:::

::: {.column width=50%}



```{r, fig.width = 5, fig.height=5, fig.align='center'}


results_energy <- tibble(gini.init = numeric(), gini.split = numeric(), gini.gain = numeric(), gini.gain.perc = numeric())


results_danceability <- tibble(gini.init = numeric(), gini.split = numeric(), gini.gain = numeric(), gini.gain.perc = numeric())

for (i in seq(0.1, 0.9, by = 0.01)) {
  results_energy <- results_energy %>% add_row(Gini(training_multi$energy, training_multi$category, i))
  
  results_danceability <- results_danceability %>% add_row(Gini(training_multi$danceability, training_multi$category, i))
}

results_energy$threshold <- seq(0.1, 0.9, by = 0.01)
results_energy$variable <- "Energy"
results_danceability$threshold <- seq(0.1, 0.9, by = 0.01)
results_danceability$variable <- "Danceability"

results <- rbind(results_energy, results_danceability)

ggplot(data = results, aes(x = threshold, y = gini.gain, color = variable)) +
  geom_line() +
  geom_point() +
  labs(x = "Threshold", y = "Information Gain", color = "Variable") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_x_continuous(breaks = seq(0.1, 0.9, by = 0.1)) +
  scale_y_continuous(breaks = seq(0, 0.5, by = 0.1))
```


:::

::::

## Entscheidungsbäume
### Schritte und Intuition

:::: {.columns}

::: {.column width=50%}

::: {style="font-size: 0.85em"}
* Anschließend wird dieser Vorgang für die neu entstandenen Regionen wiederholt, bis kein zusätzlicher Informationsgewinn mehr erzielt wird

:::
:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}

library(rpart)
library(parttree)



ti_tree =
  decision_tree(cost_complexity = 0.005, tree_depth =  1) |>
  set_engine("rpart") |>
  set_mode("classification") |>
  fit(category ~ danceability + energy , data = training_multi )



ggplot(training_multi, aes(x = energy, y = danceability)) +
  geom_parttree(data = ti_tree, alpha = 0.1, aes(fill = category)) + # <-- key layer
  geom_point(aes(col = category)) +
  labs(
    x = "Energy", y = "Danceability", color = "Genre", fill = "Genre"
    ) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

:::

::::



## Entscheidungsbäume
### Schritte und Intuition

:::: {.columns}

::: {.column width=50%}

::: {style="font-size: 0.85em"}

::: {.nonincremental}
* Anschließend wird dieser Vorgang für die neu entstandenen Regionen wiederholt, bis kein zusätzlicher Informationsgewinn mehr erzielt wird

:::

:::
:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}

library(rpart)
library(parttree)



ti_tree =
  decision_tree(cost_complexity = 0.01, tree_depth =  2) |>
  set_engine("rpart") |>
  set_mode("classification") |>
  fit(category ~ danceability + energy , data = training_multi )



ggplot(training_multi, aes(x = energy, y = danceability)) +
  geom_parttree(data = ti_tree, alpha = 0.1, aes(fill = category)) + # <-- key layer
  geom_point(aes(col = category)) +
  labs(
    x = "Energy", y = "Danceability", color = "Genre", fill = "Genre"
    ) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

:::

::::




## Entscheidungsbäume
### Schritte und Intuition

:::: {.columns}

::: {.column width=50%}

::: {style="font-size: 0.85em"}

::: {.nonincremental}

* Anschließend wird dieser Vorgang für die neu entstandenen Regionen wiederholt, bis kein zusätzlicher Informationsgewinn mehr erzielt wird

:::

* Die Regionen werden dann als Blätter bezeichnet und die Klassifikation erfolgt anhand der Mehrheit der Beobachtungen in der Region
* Die Grafik rechts zeigt den finalen Entscheidungsbaum für die Genres Klassik, EDM und Hip-Hop mit den Variablen Energy und Danceability
* Die Hintergrundfarbe repräsentiert die Klassifikation, die Farbe der Punkte die Klasse der Trainingsdaten



:::
:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}

library(rpart)
library(parttree)



ti_tree =
  decision_tree(cost_complexity = 0.005) |>
  set_engine("rpart") |>
  set_mode("classification") |>
  fit(category ~ danceability + energy , data = training_multi )



ggplot(training_multi, aes(x = energy, y = danceability)) +
  geom_parttree(data = ti_tree, alpha = 0.1, aes(fill = category)) + # <-- key layer
  geom_point(aes(col = category)) +
  labs(
    x = "Energy", y = "Danceability", color = "Genre", fill = "Genre"
    ) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

:::

::::





## Entscheidungsbäume
### Darstellung der Entscheidungsregeln


:::: {.columns}

::: {.column width=50%}

* Eine noch intuitivere Darstellung des Entscheidungsbaums ist die Visualisierung der Entscheidungsregeln
* Hierbei wird ein Pfad durch den Baum verfolgt und die Entscheidungsregeln für die Klassifikation dargestellt
* Die Grafik rechts zeigt die Entscheidungsregeln für die Genres Klassik, EDM und Hip-Hop illustriert denselben Sachverhalt wie die Partitionsgrafik

:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}
library(rpart.plot)
rpart.plot(ti_tree$fit)
```

:::

::::

## Entscheidungsbäume
### Darstellung der Entscheidungsregeln


:::: {.columns}

::: {.column width=50%}

1. Root-Node
  * Der Split umfasst die Anteile der Klassen in der Root-Node
  * Der Datensat umfasst 30% EDM Songs, 36% Hip-Hop Songs und 33% Klassik Songs
  * Da die Mehrheit der Songs aus dem Genre Hip-Hop stammen, ist die Root-Node Hip-Hop grau

:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}
library(rpart.plot)
rpart.plot(ti_tree$fit)
```

:::

::::



## Entscheidungsbäume
### Darstellung der Entscheidungsregeln


:::: {.columns}

::: {.column width=50%}



::: {style="font-size: 0.85em"}

2. Entscheidungsregel 1:
    * Unter den Nodes wird die Entscheidungsregel dargestellt
    * In unserem Fall wird der Datensatz anhand der Variable Energy $\geq$ 0.37 in zwei Regionen geteilt
    * Der Pfad links entspricht der Region, in der die Bedingung erfüllt ist, rechts der Region, in der die Bedingung nicht erfüllt ist
    * Beide Wege führen zu einer weiteren Entscheidungsregel

:::

:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}
library(rpart.plot)
rpart.plot(ti_tree$fit)
```

:::

::::


## Entscheidungsbäume
### Darstellung der Entscheidungsregeln


:::: {.columns}

::: {.column width=50%}



::: {style="font-size: 0.85em"}

2. Entscheidungsregel 2 (links):
    * Hier befinden sich 67% der Daten
    * Diese setzen sich zusammen aus:
      * 45% EDM, 53% Hip-Hop und 2% Klassik
    * Die Entscheidungsregel ist nun Energy $\geq$ 0.82
    * Wenn die Bedingung erfüllt ist, wird der Song dem Genre EDM zugeordnet
    * Wenn die Bedingung nicht erfüllt ist, wird der Song dem Genre Hip-Hop zugeordnet


:::

:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}
library(rpart.plot)
rpart.plot(ti_tree$fit)
```

:::

::::


## Entscheidungsbäume
### Darstellung der Entscheidungsregeln


:::: {.columns}

::: {.column width=50%}



::: {style="font-size: 0.85em"}

3. Entscheidungsregel 3 (rechts):
    * Hier befinden sich 33% der Daten
    * Diese setzen sich zusammen aus:
      * 0% EDM, 2% Hip-Hop und 98% Klassik
    * Die Entscheidungsregel ist nun Danceability $\geq$ 0.67
    * Wenn die Bedingung erfüllt ist, wird der Song dem Genre Hip-Hop zugeordnet
    * Wenn die Bedingung nicht erfüllt ist, wird der Song dem Genre Klassik zugeordnet


:::

:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}
library(rpart.plot)
rpart.plot(ti_tree$fit)
```

:::

::::


## Entscheidungsbäume
### Darstellung der Entscheidungsregeln


:::: {.columns}

::: {.column width=50%}



::: {style="font-size: 0.85em"}

* Die untersten Nodes sind die sogenannten Blätter des Entscheidungsbaums
* Jedes Blatt entspricht einer Region, in der die Klassifikation erfolgt
* Die Angabe unterhalb der Klassifikation ergibt die Wahrscheinlichkeit, dass ein Song in das jeweilige Genre fällt
* Die Wahrscheinlichkeit entspricht dabei dem Anteil der Songs in der Region, die den jeweiligen Genres zugeordnet sind
* Die Variable Danceability spielt für die Klassifikation offenbar keine Rolle

:::

:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}
library(rpart.plot)
rpart.plot(ti_tree$fit)
```

:::

::::

## Entscheidungsbäume
### Modellevaluation

:::: {.columns}

::: {.column width=50%}



* Die Modellevaluation erfolgt analog zur logistischen Regression und Nearest Neighbor Klassifikation
* Die Konfusionsmatrix rechts führt zu diesen Metriken auf Basis der Testdaten:


::: {style="font-size: 0.80em"}

::: {.fragment .fade-in}

```{r}

ti_tree %>%
  augment(testing_multi) %>% 
  multi_metric(truth = category, estimate = .pred_class) %>% 
  kable(digits = 2)

```

:::

:::

:::

::: {.column width=50%}

::: {.fragment .fade-in}

```{r, fig.width = 5, fig.height=5, fig.align='center'}

ti_tree %>%
  augment(testing_multi) %>% 
  conf_mat(truth = category, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

```

:::

:::

::::

## Entscheidungsbäume
### Overfitting


::: {style="font-size: 0.85em"}

* Ziel beim Machine Learning ist eine Generalisierung auf bisher unbekannte Daten
* Das bisherige Modell hat nach 2 Nodes aufgehört, aber warum?
* Dies erfolgt aufgrund sogenannter Stopping Rules oder Hyperparameter
* Komplexitätskosten: 
  * Ein zusätzlicher Split verbessert die Klassifikation auf den Trainingsdaten, führt aber zu erhöhter Komplexität des Entscheidungsbaums
  * Die Komplexitätskosten begrenzen weitere Splits und sorgen dafür, dass nur dann weiter aufgeteilt wird, wenn ein Minimum an zusätzlicher Verbesserung des Modells gewährleistet wird 
* Maximale Tiefe:  
  * Begrenzung der Verzweigung verhindert eine granulare Einteilung der Daten  
* Minimum an Beobachtungen pro Blatt:
  * Verhindert, dass nur wenige Beobachtungen in den finalen Blättern sind
  
:::

## Entscheidungsbäume
### Overfitting

:::: {.columns}

::: {.column width=35%}

::: {style="font-size: 0.80em"}

* Generell gilt: Je komplexer der Entscheidungsbaum, desto besser ist die Anpassung auf den Trainingsdaten
* Aber: Je perfekter die Anpassung auf den Trainingsdaten, desto schlechter die Generalisierung auf unbekannte Daten
* Die Grafik rechts zeigt ein extremes Beispiel, bei dem der Entscheidungsbaum mit 5 Nodes ausgebaut wurde

:::

:::

::: {.column width=65%}

::: {.fragment .fade-in}

```{r, fig.width = 7, fig.height=6, fig.align='center'}

library(rpart)

ti_tree_bad =
  decision_tree(cost_complexit =0.0001, tree_depth = 5) |>
  set_engine("rpart") |>
  set_mode("classification") |>
  fit(category ~ danceability + energy , data = training_multi )




# ggplot(training_multi, aes(x = energy, y = danceability)) +
#   geom_parttree(data = ti_tree_bad, alpha = 0.1, aes(fill = category)) + # <-- key layer
#   geom_point(aes(col = category)) +
#   labs(
#     x = "Energy", y = "Danceability", color = "Genre", fill = "Genre"
#     ) +
#   theme_minimal() +
#   theme(legend.position = "bottom")
# 
# 
# ti_tree_bad %>%
#   augment(testing_multi) %>% 
#   multi_metric(truth = category, estimate = .pred_class) %>% 
#   kable(digits = 2)


rpart.plot(ti_tree_bad$fit)

```

:::

:::

::::

## Entscheidungsbäume
### Overfitting

:::: {.columns}

::: {.column width=50%}

::: {style="font-size: 0.85em"}
* Im Rahmen des Hyperparameter-Tunings wird die Komplexität des Entscheidungsbaums optimiert
* Die Grafik rechts zeigt die Anpassung des Entscheidungsbaums an die Trainingsdaten und die Generalisierung auf die Testdaten für verschiedene Tiefen des Baums
* Als Metrik dient der F1-Score
* Offenbar steigt der F1-Score auf den Trainingsdaten mit der Tiefe des Baums, während es auf den Testdaten ein Optimum bei 6 Nodes gibt

:::
:::

::: {.column width=50%}

```{r, fig.width = 5, fig.height=5, fig.align='center'}

# Test und Training F1 score für verschiedene Tiefen:

ti_tree_results <- tibble(depth = numeric(), F1_train = numeric(), F1_test = numeric())

for (i in 1:20) {
  ti_tree =
    decision_tree(cost_complexity = 0.00001, tree_depth = i) |>
    set_engine("rpart") |>
    set_mode("classification") |>
    fit(category ~ danceability + energy , data = training_multi )
  
  f1_train <- ti_tree %>%
    augment(training_multi) %>% 
    f_meas(truth = category, estimate = .pred_class)
    
  f1_test <- ti_tree %>%
    augment(testing_multi) %>% 
    f_meas(truth = category, estimate = .pred_class)
  

  ti_tree_results <- ti_tree_results %>% add_row(depth = i, F1_train = f1_train$.estimate, F1_test = f1_test$.estimate)
  
  }

ggplot(data = ti_tree_results, aes(x = depth, y = F1_train, color = "Trainingsdaten")) +
  geom_line() +
  geom_point() +
  geom_line(data = ti_tree_results, aes(x = depth, y = F1_test, color = "Testdaten")) +
  geom_point(data = ti_tree_results, aes(x = depth, y = F1_test, color = "Testdaten")) +
  labs(x = "Tiefe des Enscheidungsbaums", y = "F1-Score", color = "Datensatz") +
  theme_minimal() +
  theme(legend.position = "bottom") + 
  scale_x_continuous(breaks = seq(0, 20, by = 2)) 



```

:::
::::

## Entscheidungsbäume
### Erweiterungen

* Entscheidungsbäume sind sehr intuitiv und einfach zu interpretieren
* Entscheidungsbäume erfordern nur wenig Datenbereinigung und können mit numerischen und kategorialen Variablen umgehen
* Allerdings sind Entscheidungsbäume anfällig für Overfitting
* Entscheidungsbäume sind Grundlage für viele weitere Modelle:
  * Random Forests kombinieren mehrere Entscheidungsbäume, um die Vorhersagegenauigkeit zu verbessern und das Overfitting zu reduzieren
  * Boosted Trees kombinieren sequentielle Entscheidungsbäume, die auf den Fehlern des vorherigen Baums aufbauen

## Modellvergleich
### ...and the Winner is?

* Die Modelle mit 2 erklärenden Variablen dienen primär der Illustration der Modellintuition und sind daher nicht unbedingt praxistauglich
* Welches Modell performt am besten auf den Testdaten?

::: {style="font-size: 0.70em"}


::: {.fragment .fade-in}

```{r}

# logistic model k nearest neighbor und decision tree im vergleich auf den Testdaten:

folds <- rsample::vfold_cv(training_multi, v = 5)

logistic <- 
  multinom_reg() |>
  set_engine("nnet") |>
  set_mode("classification") |>
  fit(category ~ danceability + energy, data = training_multi)

knn <- 
  nearest_neighbor(neighbors = 60) |>
  set_engine("kknn") |>
  set_mode("classification") |>
  fit(category ~ danceability + energy, data = training_multi)

tree <- 
  decision_tree(cost_complexity = 0.00001, tree_depth = 6) |>
  set_engine("rpart") |>
  set_mode("classification") |>
  fit(category ~ danceability + energy, data = training_multi)


logistic_eval <- logistic %>%
  augment(testing_multi) %>% 
  multi_metric(truth = category, estimate = .pred_class) %>% 
  mutate(Model = "Logistische Regression")

knn_eval <- knn %>%
  augment(testing_multi) %>% 
  multi_metric(truth = category, estimate = .pred_class) %>% 
  mutate(Model = "60-Nearest Neighbor")

tree_eval <- tree %>%
  augment(testing_multi) %>% 
  multi_metric(truth = category, estimate = .pred_class) %>% 
  mutate(Model = "Entscheidungsbaum 6 Nodes")


rbind(logistic_eval, knn_eval, tree_eval) %>% 
  select(-.estimator) %>% 
  pivot_wider(names_from = Model, values_from = .estimate) %>% 
  kable(digits = 4)


```

:::
:::

* Wenn auch knapp hat das Nearest Neighbor Modell mit $K = 60$ die Nase vorn 


